{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNzEJC1yEFqLR0LXwl8pkAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerezamo/NLP_brouillon/blob/master/Camembert_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# BERT classification model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "58a4c9c9-20b8-4982-b6ff-b33e6cab8e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "15f3209a-f65b-4461-ba6c-eae88d9b5498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "e2b73099-364e-4740-fe75-1c5a61b324c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "d6df388f-ce3e-4c5b-be85-c1df4f7bb76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Import medium_df_desq in \"files\"\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0FNej5emFx8",
        "colab_type": "text"
      },
      "source": [
        "**CHOOSE ONE OF THE OPTIONS BELOW**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRDpDKe9_j-",
        "colab_type": "text"
      },
      "source": [
        "1. Unbalanced sample \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Gl1QBWAlTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b5fe9123-dc32-4643-9d83-87db5e4d2d2b"
      },
      "source": [
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 5,000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mesdames et Messieurs les parlementaires, Mesd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mes amis, vous êtes une foule immense sur la p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ce n'est pas nouveau : la droite accorde, en p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Madame la Ministre, Mon Général, Monsieur le D...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mesdames, Messieurs,Chers amis,Si la professio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  Mesdames et Messieurs les parlementaires, Mesd...     0\n",
              "1  Mes amis, vous êtes une foule immense sur la p...     0\n",
              "2  Ce n'est pas nouveau : la droite accorde, en p...     0\n",
              "3  Madame la Ministre, Mon Général, Monsieur le D...     0\n",
              "4  Mesdames, Messieurs,Chers amis,Si la professio...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrqgCPYFApFQ",
        "colab_type": "text"
      },
      "source": [
        "2. Balanced sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOIKxWaXHPVB",
        "colab_type": "code",
        "outputId": "81da67e7-3136-4a74-e894-25623a0a54ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monsieur le Président,Madame,C'est un honneur ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Messieurs les élus,Monsieur le président du co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Madame l'Ambassadeur,Monsieur le Consul généra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Madame la ministre, chère Ericka BAREIGTS, Mad...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Je voulais d'abord remercier Angela MERKEL et ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  Monsieur le Président,Madame,C'est un honneur ...     0\n",
              "1  Messieurs les élus,Monsieur le président du co...     0\n",
              "2  Madame l'Ambassadeur,Monsieur le Consul généra...     1\n",
              "3  Madame la ministre, chère Ericka BAREIGTS, Mad...     0\n",
              "4  Je voulais d'abord remercier Angela MERKEL et ...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNHgnNbAsoq",
        "colab_type": "text"
      },
      "source": [
        "3. Spliting texts in order to feed the model with all parts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JmisLKH2L8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "39ad5e0e-c4d2-4728-b60c-90f639798d61"
      },
      "source": [
        "from itertools import repeat\n",
        "n=2500\n",
        "chunks, label_split=[],[]\n",
        "j=0\n",
        "for text in df.Texte :\n",
        "    txt=[text[i:i+n] for i in range(0, len(text), n)]\n",
        "    chunks.append(txt)\n",
        "    label_split.extend(repeat(df.sexe[j], len(txt)))\n",
        "    j+=1\n",
        "\n",
        "chunks = [item for sublist in chunks for item in sublist]\n",
        "df=pd.DataFrame([chunks,label_split]).transpose()\n",
        "df.columns=['Texte','sexe']\n",
        "len(df)\n",
        "\n",
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=0.5).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Put as integer \n",
        "df['sexe'] = df['sexe'].astype(int)\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "# In this case we will cut the sample \n"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 6,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>t celui du changement. François l'a dit, Marti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>r les territoires et leur mise en synergie par...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in du mois se limite à hauteur d'un loyer clas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>urité, - après rapport du ministre de l'intéri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Monsieur le Vice-Président,Mesdames et Messieu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  t celui du changement. François l'a dit, Marti...     1\n",
              "1  r les territoires et leur mise en synergie par...     1\n",
              "2  in du mois se limite à hauteur d'un loyer clas...     1\n",
              "3  urité, - après rapport du ministre de l'intéri...     0\n",
              "4  Monsieur le Vice-Président,Mesdames et Messieu...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDafeOtBg9T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "model, dev = train_test_split(df, test_size=0.02)\n",
        "\n",
        "df= model\n",
        "df_dev= dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXJZbQHl9Is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fb54245-75fa-4c5b-f318-e159a38bde50"
      },
      "source": [
        "len(df_dev)"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbgs39TYNqRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08yLt5edOhL3",
        "colab_type": "code",
        "outputId": "3b92e5e3-b57f-434e-d6d2-78a0208f1691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  t celui du changement. François l'a dit, Martin et Henri également...Ceux qui me questionnent me disent : vous faites du 7 juin une sanction contre Nicolas Sarkozy.Nous faisons une sanction contre le libéralisme de Nicolas Sarkozy.Il proposait que les Français s'endettent plus avec des crédits hypothécaires, et nous traitait d'archaïques.Il n'a eu de cesse de nous dire que toutes les protections empêchaient l'initiative. Et nous disait qu'il allait arracher la croissance avec les dents. C'est l'inverse qu'il a fait car il a apporté à ceux qui avaient déjà tout, mais ne crée pas une société qui crée de la croissance et du développement.Nous voulons effectivement sanctionner cette crise financière.C'est dès le début de l'année dernière que nous étions déjà en récession, que le pouvoir d'achat baissait et que le chômage commençait à augmenter. On peut dire que Nicolas Sarkozy avait réussi à anticiper la crise par la politique libérale qu'il a menée dans notre pays. C'est cela qu'il faut changer.La situation est grave. Nous sentons monter dans notre pays une colère. Une colère contre un homme qui a fait tant de promesse et n'a rien respecté. Rappelez-vous comment il a réussi à faire croire aux classes populaires qu'il serait le président du pouvoir d'achat. Comment il a dit que les personnes pourraient gagner plus en faisant des heures supplémentaires.Et la France des propriétaires, où il n'a donné des avantages qu'à ceux qui étaient déjà propriétaires.La France des droits de l'homme. Quand on voit M. Besson nous donner des leçons sur l'immigration. Nous ne sommes pas d'accord pour que des personnes soient poursuivies pour délit de solidarité.Alors, la colère monte aussi parce que l'impression qu'ont les Français aujourd'hui, c'est que le président de la République a décidé de continuer sur sa ligne. Nous étions 1 million en janvier dans les rues, 3 millions en mars, et au 1er mai, 5 fois plus que les autres années, et une unité syndicale qu'il n'y avait pas eue depuis 1947.Malgré tout ça, Nicolas Sarkozy persiste et signe.C'est vrai qu'il ne reste pas sans parler, il s'agite beaucoup, il parle surtout beaucoup. Ceux qui se souviendront du discours de Toulon, celui qu'il a fait quand la crise a commencé, où il disait qu'on allait dénoncer les patrons voyous !Puis depuis, nous en avons su beaucoup plus. Il nous a dit, à quatre reprises au moins, et je crois même que les banquiers ont fini par le croire. Il a dit : je vais faire voter une loi (il aime beaucoup f\n",
            "Tokenized:  ['▁t', '▁celui', '▁du', '▁changement', '.', '▁François', '▁l', \"'\", 'a', '▁dit', ',', '▁Martin', '▁et', '▁Henri', '▁également', '...', 'C', 'eux', '▁qui', '▁me', '▁question', 'nent', '▁me', '▁disent', '▁:', '▁vous', '▁faites', '▁du', '▁7', '▁juin', '▁une', '▁sanction', '▁contre', '▁Nicolas', '▁Sarkozy', '.', 'Nous', '▁faisons', '▁une', '▁sanction', '▁contre', '▁le', '▁libéralisme', '▁de', '▁Nicolas', '▁Sarkozy', '.', 'Il', '▁proposait', '▁que', '▁les', '▁Français', '▁s', \"'\", 'end', 'ette', 'nt', '▁plus', '▁avec', '▁des', '▁crédits', '▁hypothécaire', 's', ',', '▁et', '▁nous', '▁trait', 'ait', '▁d', \"'\", 'arch', 'aï', 'ques', '.', 'Il', '▁n', \"'\", 'a', '▁eu', '▁de', '▁cesse', '▁de', '▁nous', '▁dire', '▁que', '▁toutes', '▁les', '▁protection', 's', '▁emp', 'êch', 'aient', '▁l', \"'\", 'initiative', '.', '▁Et', '▁nous', '▁disait', '▁qu', \"'\", 'il', '▁allait', '▁arracher', '▁la', '▁croissance', '▁avec', '▁les', '▁dents', '.', '▁C', \"'\", 'est', '▁l', \"'\", 'inverse', '▁qu', \"'\", 'il', '▁a', '▁fait', '▁car', '▁il', '▁a', '▁apporté', '▁à', '▁ceux', '▁qui', '▁avaient', '▁déjà', '▁tout', ',', '▁mais', '▁ne', '▁crée', '▁pas', '▁une', '▁société', '▁qui', '▁crée', '▁de', '▁la', '▁croissance', '▁et', '▁du', '▁développement', '.', 'Nous', '▁voulons', '▁effectivement', '▁sanctionner', '▁cette', '▁crise', '▁financière', '.', 'C', \"'\", 'est', '▁dès', '▁le', '▁début', '▁de', '▁l', \"'\", 'année', '▁dernière', '▁que', '▁nous', '▁étions', '▁déjà', '▁en', '▁récession', ',', '▁que', '▁le', '▁pouvoir', '▁d', \"'\", 'achat', '▁baissa', 'it', '▁et', '▁que', '▁le', '▁chômage', '▁commençait', '▁à', '▁augmenter', '.', '▁On', '▁peut', '▁dire', '▁que', '▁Nicolas', '▁Sarkozy', '▁avait', '▁réussi', '▁à', '▁anticiper', '▁la', '▁crise', '▁par', '▁la', '▁politique', '▁libérale', '▁qu', \"'\", 'il', '▁a', '▁menée', '▁dans', '▁notre', '▁pays', '.', '▁C', \"'\", 'est', '▁cela', '▁qu', \"'\", 'il', '▁faut', '▁changer', '.', 'La', '▁situation', '▁est', '▁grave', '.', '▁Nous', '▁sent', 'ons', '▁monter', '▁dans', '▁notre', '▁pays', '▁une', '▁colère', '.', '▁Une', '▁colère', '▁contre', '▁un', '▁homme', '▁qui', '▁a', '▁fait', '▁tant', '▁de', '▁promesse', '▁et', '▁n', \"'\", 'a', '▁rien', '▁respecté', '.', '▁Rappelez', '-', 'vous', '▁comment', '▁il', '▁a', '▁réussi', '▁à', '▁faire', '▁croire', '▁aux', '▁classes', '▁populaires', '▁qu', \"'\", 'il', '▁serait', '▁le', '▁président', '▁du', '▁pouvoir', '▁d', \"'\", 'achat', '.', '▁Comment', '▁il', '▁a', '▁dit', '▁que', '▁les', '▁personnes', '▁pourraient', '▁gagner', '▁plus', '▁en', '▁faisant', '▁des', '▁heures', '▁supplémentaires', '.', 'Et', '▁la', '▁France', '▁des', '▁propriétaires', ',', '▁où', '▁il', '▁n', \"'\", 'a', '▁donné', '▁des', '▁avantages', '▁qu', \"'\", 'à', '▁ceux', '▁qui', '▁étaient', '▁déjà', '▁propriétaires', '.', 'La', '▁France', '▁des', '▁droits', '▁de', '▁l', \"'\", 'homme', '.', '▁Quand', '▁on', '▁voit', '▁M', '.', '▁Besson', '▁nous', '▁donner', '▁des', '▁leçons', '▁sur', '▁l', \"'\", 'immigration', '.', '▁Nous', '▁ne', '▁sommes', '▁pas', '▁d', \"'\", 'accord', '▁pour', '▁que', '▁des', '▁personnes', '▁soient', '▁poursuivie', 's', '▁pour', '▁délit', '▁de', '▁solidarité', '.', 'Alors', ',', '▁la', '▁colère', '▁monte', '▁aussi', '▁parce', '▁que', '▁l', \"'\", 'impression', '▁qu', \"'\", 'ont', '▁les', '▁Français', '▁aujourd', \"'\", 'hui', ',', '▁c', \"'\", 'est', '▁que', '▁le', '▁président', '▁de', '▁la', '▁République', '▁a', '▁décidé', '▁de', '▁continuer', '▁sur', '▁sa', '▁ligne', '.', '▁Nous', '▁étions', '▁1', '▁million', '▁en', '▁janvier', '▁dans', '▁les', '▁rues', ',', '▁3', '▁millions', '▁en', '▁mars', ',', '▁et', '▁au', '▁1', 'er', '▁mai', ',', '▁5', '▁fois', '▁plus', '▁que', '▁les', '▁autres', '▁années', ',', '▁et', '▁une', '▁unité', '▁syndicale', '▁qu', \"'\", 'il', '▁n', \"'\", 'y', '▁avait', '▁pas', '▁eu', 'e', '▁depuis', '▁1947', '.', 'Mal', 'gré', '▁tout', '▁ça', ',', '▁Nicolas', '▁Sarkozy', '▁persiste', '▁et', '▁signe', '.', 'C', \"'\", 'est', '▁vrai', '▁qu', \"'\", 'il', '▁ne', '▁reste', '▁pas', '▁sans', '▁parler', ',', '▁il', '▁s', \"'\", 'agit', 'e', '▁beaucoup', ',', '▁il', '▁parle', '▁surtout', '▁beaucoup', '.', '▁Ceux', '▁qui', '▁se', '▁sou', 'viendront', '▁du', '▁discours', '▁de', '▁Toulon', ',', '▁celui', '▁qu', \"'\", 'il', '▁a', '▁fait', '▁quand', '▁la', '▁crise', '▁a', '▁commencé', ',', '▁où', '▁il', '▁disait', '▁qu', \"'\", 'on', '▁allait', '▁dénoncer', '▁les', '▁patrons', '▁voy', 'ous', '▁!', 'Pu', 'is', '▁depuis', ',', '▁nous', '▁en', '▁avons', '▁su', '▁beaucoup', '▁plus', '.', '▁Il', '▁nous', '▁a', '▁dit', ',', '▁à', '▁quatre', '▁reprises', '▁au', '▁moins', ',', '▁et', '▁je', '▁crois', '▁même', '▁que', '▁les', '▁banquier', 's', '▁ont', '▁fini', '▁par', '▁le', '▁croire', '.', '▁Il', '▁a', '▁dit', '▁:', '▁je', '▁vais', '▁faire', '▁voter', '▁une', '▁loi', '▁(', 'il', '▁aime', '▁beaucoup', '▁f']\n",
            "Token IDs:  [271, 330, 25, 1306, 9, 1543, 17, 11, 55, 227, 7, 2928, 14, 4934, 200, 57, 228, 914, 31, 103, 397, 4443, 103, 3313, 43, 39, 1126, 25, 333, 522, 28, 12510, 192, 2289, 5868, 9, 3975, 5475, 28, 12510, 192, 16, 24937, 8, 2289, 5868, 9, 1799, 19013, 27, 19, 1455, 52, 11, 904, 1606, 113, 40, 42, 20, 7168, 30562, 10, 7, 14, 63, 4680, 199, 18, 11, 13595, 1790, 2145, 9, 1799, 49, 11, 55, 331, 8, 2923, 8, 63, 248, 27, 208, 19, 872, 10, 6994, 15029, 488, 17, 11, 4406, 9, 139, 63, 3515, 46, 11, 62, 1937, 23898, 13, 1618, 42, 19, 4111, 9, 84, 11, 41, 17, 11, 5837, 46, 11, 62, 33, 82, 173, 51, 33, 5569, 15, 320, 31, 917, 235, 66, 7, 65, 45, 2750, 34, 28, 426, 31, 2750, 8, 13, 1618, 14, 25, 499, 9, 3975, 6385, 2579, 29725, 78, 1662, 2903, 9, 228, 11, 41, 564, 16, 479, 8, 17, 11, 520, 576, 27, 63, 4707, 235, 22, 26969, 7, 27, 16, 351, 18, 11, 1550, 24879, 312, 14, 27, 16, 4103, 12119, 15, 3138, 9, 201, 104, 248, 27, 2289, 5868, 171, 1522, 15, 21016, 13, 1662, 37, 13, 462, 11754, 46, 11, 62, 33, 5894, 29, 127, 256, 9, 84, 11, 41, 207, 46, 11, 62, 213, 999, 9, 1003, 595, 30, 2252, 9, 170, 2517, 273, 2570, 29, 127, 256, 28, 3597, 9, 180, 3597, 192, 23, 421, 31, 33, 82, 376, 8, 8052, 14, 49, 11, 55, 254, 10817, 9, 28057, 26, 315, 404, 51, 33, 1522, 15, 85, 1757, 68, 3539, 4980, 46, 11, 62, 448, 16, 668, 25, 351, 18, 11, 1550, 9, 841, 51, 33, 227, 27, 19, 242, 2424, 1481, 40, 22, 1208, 20, 511, 3465, 9, 4407, 13, 184, 20, 3984, 7, 147, 51, 49, 11, 55, 804, 20, 2854, 46, 11, 169, 320, 31, 530, 235, 3984, 9, 1003, 184, 20, 873, 8, 17, 11, 698, 9, 877, 91, 1037, 188, 9, 29601, 63, 509, 20, 9959, 32, 17, 11, 7529, 9, 170, 45, 464, 34, 18, 11, 1311, 24, 27, 20, 242, 1053, 24189, 10, 24, 10554, 8, 3834, 9, 20900, 7, 13, 3597, 4621, 99, 398, 27, 17, 11, 1464, 46, 11, 263, 19, 1455, 405, 11, 265, 7, 60, 11, 41, 27, 16, 668, 8, 13, 1547, 33, 1258, 8, 1760, 32, 77, 284, 9, 170, 4707, 124, 5429, 22, 695, 29, 19, 3602, 7, 135, 713, 22, 697, 7, 14, 36, 124, 108, 667, 7, 205, 151, 40, 27, 19, 214, 318, 7, 14, 28, 6167, 18088, 46, 11, 62, 49, 11, 105, 171, 34, 331, 35, 176, 18582, 9, 18488, 7702, 66, 136, 7, 2289, 5868, 13384, 14, 1700, 9, 228, 11, 41, 600, 46, 11, 62, 45, 353, 34, 112, 639, 7, 51, 52, 11, 567, 35, 217, 7, 51, 921, 381, 217, 9, 4499, 31, 48, 3438, 14553, 25, 2081, 8, 13317, 7, 330, 46, 11, 62, 33, 82, 206, 13, 1662, 33, 1592, 7, 147, 51, 3515, 46, 11, 88, 1937, 11344, 19, 13577, 14639, 7215, 83, 18496, 418, 176, 7, 63, 22, 296, 1548, 217, 40, 9, 69, 63, 33, 227, 7, 15, 700, 4315, 36, 175, 7, 14, 50, 1115, 93, 27, 19, 17978, 10, 96, 1856, 37, 16, 1757, 9, 69, 33, 227, 43, 50, 676, 85, 6170, 28, 589, 38, 62, 1473, 217, 906]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = df.Texte.values\n",
        "labels = df.sexe.values\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "# Apply function to our corpus\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                   )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        " \n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "outputId": "3bf85386-6d70-41c0-d040-a10c1ec9c7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Original text and transformed tensor print \n",
        "print('Original: ', texts[0])\n",
        "print('IDs:', input_ids[0])"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   de notre Défense qui sont mieux adaptés aux besoins de notre époque, un fonds souverain à la française qui se tient désormais aux côtés de nos entreprises pour les aider à se développer et pour les protéger.Grâce à un plan d'investissement sans précédent nous allons pouvoir accomplir la révolution numérique, donner à tous l'accès au haut débit, numériser nos livres pour que notre langue, notre culture puissent continuer à rayonner, mais aussi créer 20 000 places d'internats d'excellence pour rétablir une réelle égalité des chances, et doter notre enseignement supérieur et notre recherche de moyens considérables pour réussir le pari de l'intelligence.Grâce à la loi Hadopi qui sera mise en oeuvre en 2010, nos créateurs et nos artistes vont être protégés.Grâce au Grenelle de l'Environnement nous allons pouvoir relever le défi de la protection de notre environnement. C'est un domaine où il est bien difficile de faire évoluer les mentalités et les comportements. Mais je ne suis pas un homme qui renonce à la première difficulté, et la fiscalité écologique qui permet de taxer la pollution et d'exonérer le travail est un enjeu majeur. Dès le 20 janvier, le gouvernement présentera un nouveau dispositif afin que les consommateurs soient incités à consommer mieux et les producteurs à produire propre.Beaucoup de réformes ont été accomplies. Je sais qu'elles ont bouleversé des habitudes et qu'avant de produire leurs effets elles ont pu provoquer des inquiétudes. Mais qui peut croire que dans ce monde qui bouge l'immobilisme soit une alternative ? Il nous reste encore bien du travail. Je le conduirai avec le Premier Ministre et le Gouvernement dans le dialogue et avec un esprit de justice. En 2010, il va nous falloir : faire reculer le chômage et l'exclusion, réduire nos dépenses courantes pour nous permettre d'accroître nos dépenses d'avenir, simplifier notre organisation territoriale trop lourde, trop compliquée, trop onéreuse, consolider notre système de retraites dont j'ai le devoir d'assurer la pérennité financière, relever le défi de la dépendance qui sera dans les décennies à venir l'un des problèmes les plus douloureux auxquels nos familles seront confrontées. En 2010, nous réformerons notre Justice pour qu'elle protège davantage les libertés et qu'elle soit plus attentive aux victimes.Mes chers compatriotes, même si les épreuves ne sont pas terminées, 2010 sera une année de renouveau. Les efforts que nous faisons depuis deux ans et demi vont porter leurs fruit\n",
            "IDs: tensor([    5,     8,   127,  7682,    31,    56,   334,  5159,    68,   979,\n",
            "            8,   127,  2246,     7,    23,  1751,  8146,    15,    13,   781,\n",
            "           31,    48,  1866,  1085,    68,  2641,     8,   166,   699,    24,\n",
            "           19,   987,    15,    48,  1523,    14,    24,    19,  1948,     9,\n",
            "          546, 23558,   291,    15,    23,   379,    18,    11,  3407,   112,\n",
            "         2501,    63,  2545,   351, 11182,    13,  3177,  1495,     7,   509,\n",
            "           15,   117,    17,    11,  1288,    36,   540,  4779,     7, 23492,\n",
            "         1864,   166,  1320,    24,    27,   127,  1209,     7,   127,  1030,\n",
            "         4601,  1760,    15,  4356,   944,     7,    65,    99,   739,   325,\n",
            "          624,  2637,    18,    11, 26470,    10,    18,    11, 10640,    24,\n",
            "         8362,    28,  3351, 15882,    20,  4295,     7,    14, 21224,   127,\n",
            "         8010,  2062,    14,   127,   332,     8,  1149,  6063,    10,    24,\n",
            "         3333,    16, 10242,     8,    17,    11,  6031,     9,   546, 23558,\n",
            "          291,    15,    13,   589, 17074, 18545,    31,   210,   375,    22,\n",
            "         2893,    22,  5257,   166,  8339,    14,   166,  2099,   774,    98,\n",
            "        11535,     9,   546, 23558,   291,    36, 13139,   144,     8,    17,\n",
            "           11, 16481,    63,  2545,   351,  5536,    16,  2280,     8,    13,\n",
            "          872,     8,   127,  1898,     9,    84,    11,    41,    23,   813,\n",
            "          147,    51,    30,    72,   863,     8,    85,  5120,    19, 13055,\n",
            "           10,    14,    19,  6266,     9,   159,    50,    45,   146,    34,\n",
            "           23,   421,    31, 19007,    15,    13,   272,  2691,     7,    14,\n",
            "           13, 13508,  5001,    31,   288,     8,  5478,    81,    13,  6504,\n",
            "           14,    18,    11,   850,    88,  6078,    16,   225,    30,    23,\n",
            "        12124,  4329,     9,  2335,    16,   325,   695,     7,    16,   754,\n",
            "        15759,    23,   281,  2507,   289,    27,    19,  3697,  1053, 28687,\n",
            "           10,    15,  7567,   334,    14,    19,  5069,    15,  2983,   627,\n",
            "            9, 22054,  7829,     8,  9795,    96,   101, 23508,    10,     9,\n",
            "          100,   555,    46,    11,   734,    96, 19028,    20,  6255,    14,\n",
            "           46,    11,  1949,     8,  2983,   187,  1804,   582,    96,   456,\n",
            "         6545,    20, 21749,     9,   159,    31,   104,  1757,    27,    29,\n",
            "           44,   164,    31,  6337,    17,    11,  1225, 10710,   884,   191,\n",
            "           28,  5608,   106,    69,    63,   353,   143,    72,    25,   225,\n",
            "            9,   100,    16, 23390,   233,    42,    16,  2616,  6375,    14,\n",
            "           16,  6518,    29,    16,  3036,    14,    42,    23,  2132,     8,\n",
            "         1746,     9,   107,  5257,    51,   198,    63,  7190,    43,    85,\n",
            "        19645,    16,  4103,    14,    17,    11,  8660,     7,  2020,   166,\n",
            "         3857, 17873,    24,    63,  1027,    18,    11, 22653,   166,  3857,\n",
            "           18,    11,  2128,     7, 15584,   127,  2413,  9321,   237,  7542,\n",
            "            7,   237, 11716,     7,   237, 31410,     7, 18403,   127,   439,\n",
            "            8, 16358,   174,    76,    11,    73,    16,  2308,    18,    11,\n",
            "         2688,    13, 21501,  2903,     7,  5536,    16,  2280,     8,    13,\n",
            "         7113,    31,   210,    29,    19,  6768,    15,   894,    17,    11,\n",
            "           59,    20,  1014,    19,    40, 12695,  6233,   166,  1641,   519,\n",
            "        18049,    10,     9,   107,  5257,    63,  3145,  1628,   127,  8371,\n",
            "           24,    46,    11,   144,  6962,  1921,    19,  8922,    14,    46,\n",
            "           11,   144,   191,    40, 21654,    68,  2525,     9,  6684,    10,\n",
            "         6175, 16681,    10,     7,    93,    86,    19,  6787,    45,    56,\n",
            "           34,  7726,    10,     7,  1793,   210,    28,   433,     8, 16601,\n",
            "            9,    74,  2943,    27,    63,  5475,   176,   116,   134,    14,\n",
            "         1644,   774,  1499,   187,  3735,     6,     1,     1,     1,     1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OPWOx2-FG7g",
        "colab_type": "code",
        "outputId": "d08b14b9-ad6e-45fb-ced3-54be91f7cac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 1,  ..., 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFc6MW5df566",
        "colab_type": "text"
      },
      "source": [
        "#### Train and validation dataset construction \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y3MT9B-gAHU",
        "colab_type": "code",
        "outputId": "baac414d-9f7a-40c5-afcb-2d5265e1bc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine all above\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Let's create a 80-20 train / validation dataset \n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('We have {} training samples'.format(train_size))\n",
        "print('We have {} validation samples'.format(val_size))"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 5096 training samples\n",
            "We have 1274 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF9rj3AWgynY",
        "colab_type": "text"
      },
      "source": [
        "In order to save on memory we use the convenient DataLoader of pytorch.utils "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr-fhDIQgAFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# We set the size of the batch lower than what is usually set (16 of 32)\n",
        "batch_size = 4\n",
        "\n",
        "# We create data loaders for the train and validation dataset. \n",
        "train_dataloader = DataLoader(\n",
        "            train_set,  # The training samples.\n",
        "            sampler = RandomSampler(train_set), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "            val_set, # The validation samples.\n",
        "            sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poTTEJX1hoUK",
        "colab_type": "text"
      },
      "source": [
        "### CamemBERT Sequence Classification model tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VeJI0lDwf",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MPOVB7iRcl",
        "colab_type": "text"
      },
      "source": [
        "We will finally build up our model. We will use the  CamemBERT model for sequence classification which includes a special top layer designed for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHhHzjKgAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing from transformers\n",
        "from transformers import CamembertForSequenceClassification, CamembertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMdM-QqgAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "# Ici je ne suis pas sure pour le 'cased' ou pas (je crois que oui)\n",
        "gender_model1 = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKynoykf_9y",
        "colab_type": "code",
        "outputId": "67b47169-91cf-4176-98c2-363f47361081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We run the model on the colab GPU \n",
        "gender_model1.cuda()"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWyHWg5xlBck",
        "colab_type": "text"
      },
      "source": [
        "Optimizers and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go-uY70Mloqv",
        "colab_type": "text"
      },
      "source": [
        "We will choose the AdamW optimizer and set for this first model the learning rate and the epsilon to default. At the batch is little we might want to increase the learning rate a bit from what is usually used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyP0__vf_7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#Implements Adam algorithm with weight decay fix.\n",
        "optimizer = AdamW(gender_model1.parameters(),\n",
        "                  lr = 5e-5, # Adaptative (yes i think)\n",
        "                  eps = 1e-8 # prevent division by 0 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYr-iTzVmXZu",
        "colab_type": "text"
      },
      "source": [
        "We fiw the number of epochs to 4\n",
        "We also configure the learning rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ1ymlmqf_4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# set number of epochs\n",
        "epochs = 2\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgwmHirxnEie",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the training and validation loop \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6rIz4UnLwq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDt2ZElwcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZt12wxqh7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQxSQsQxm6mv",
        "colab_type": "code",
        "outputId": "9260bb6f-7d75-47d9-feae-6045f7ba6b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# https://github.com/huggingface/transformers \n",
        "# https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L404  \n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
        "\n",
        "import random\n",
        "# Let's put a seed to make this result reproducible \n",
        "seed=2020\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# We want to evaluate the training phase \n",
        "training_stats = []\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "  print('Training starts')\n",
        "\n",
        "  \n",
        "\n",
        "  ################################### TRAINING ################################\n",
        "\n",
        "  # Set the train loss for the epoch to 0 \n",
        "  total_train_loss = 0\n",
        "\n",
        "  #Put the model in training \n",
        "  gender_model1.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Cpy the 3 batch to GPU \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Clear gradients \n",
        "    gender_model1.zero_grad() \n",
        "    \n",
        "    #return loss and logits\n",
        "    loss, logits = gender_model1(b_input_ids, \n",
        "                         token_type_ids=None, \n",
        "                         attention_mask=b_input_mask, \n",
        "                         labels=b_labels) \n",
        "    \n",
        "    # Accumulate training loss for all batches \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Backward to calculate gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Prevent exploding gradients problem \n",
        "    torch.nn.utils.clip_grad_norm_(gender_model1.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters \n",
        "    optimizer.step()\n",
        "\n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "  #Calculate the average training loss over all batches  \n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print('')\n",
        "  print('And now, validation STARTS')\n",
        "\n",
        "  ###################### VALIDATION #############################\n",
        "\n",
        "  # Put model in evaluation mode \n",
        "  gender_model1.eval()\n",
        "\n",
        "  # Set statistics to 0\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  nb_eval_steps = 0\n",
        "\n",
        "  # Confusion matrix ?\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "     \n",
        "     # We don't care about gradients for eval\n",
        "    with torch.no_grad(): \n",
        "      (loss, logits) = gender_model1(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "      # Move logits and labels to CPU \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Confusion matrix ?\n",
        "    val_batch_preds = np.argmax(logits, axis=1)\n",
        "    val_batch_labels = label_ids\n",
        "    predictions.extend(val_batch_preds)\n",
        "    true_labels.extend(val_batch_labels)\n",
        "\n",
        "    # Accumulation accuracy for all batch\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    #Final accuracy on all batch\n",
        "  avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #Final loss over all batch\n",
        "  avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "  # confusion matrix ? \n",
        "  pred_tags = [i for i in predictions]\n",
        "  valid_tags = [i for i in true_labels]\n",
        "\n",
        "  # f1 score \n",
        "  F1_score_val = f1_score(valid_tags,pred_tags)\n",
        "\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': ep + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Valid F1_score' : F1_score_val\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Done !\")"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 2 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.60\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.52\n",
            "===========Starting Epoch 2 / 2 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.46\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.79\n",
            "  Validation Loss: 0.64\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqfBkNDKVxW",
        "colab_type": "code",
        "outputId": "18d0a200-2fc7-4ec1-dba2-a696c8cdddca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Confusion matrix sur le dernier epoch (à insérer pour avoir les trois ? )\n",
        "confusion_matrix(valid_tags, pred_tags)"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[530, 107],\n",
              "       [156, 481]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKUBFeBAJTOK",
        "colab_type": "code",
        "outputId": "cad17c10-b8ec-4ed6-9874-8fbc23adb7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_mat = confusion_matrix(valid_tags, pred_tags)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGmCAYAAACUbzs0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVhU1f8H8DfosAquA4oKYioQCAKp\noaaVZGgooGJaIWRh5fIVzL5J2vK1UlNULMsF09CoXALHJdBK29TcBdFRcwsVkcGFkXUGmN8fxB3H\nGRnQ0aHffb+eZ55Hzj3n3EOP4YfP55x7LTQajQZEREREImJp7gUQERERPWwMgIiIiEh0GAARERGR\n6DAAIiIiItFhAERERESiwwCIiIiIRKepuRdwO3XhOXMvgUh0bF2eMPcSiESrUnX5od7PlP/OStp0\nNtlc5sAMEBEREYlOo8oAERER0QNUXWXuFTQaDICIiIjEQlNt7hU0GiyBERERkegwA0RERCQW1cwA\n1WIAREREJBIalsAELIERERGR6DADREREJBYsgQkYABEREYkFS2AClsCIiIhIdJgBIiIiEgs+CFHA\nAIiIiEgsWAITsARGREREosMMEBERkVjwFJiAARAREZFI8EGIWiyBERERkegwA0RERCQWLIEJGAAR\nERGJBUtgApbAiIiISHSYASIiIhILPghRwAwQERGRWGiqTfdpgH379sHDw8Pg5+zZszp9Dx8+jDFj\nxsDPzw99+/bFRx99hLKyMr05VSoV5s+fj379+sHX1xejRo3C3r17670mZoCIiIjooYiOjoa3t7dO\nm7Ozs/BnuVyOmJgYdOnSBdOnT0d+fj5WrVqFS5cuYdmyZTrjpk+fjh07dmDs2LFwc3NDeno6YmNj\nsXbtWvj7+xtdCwMgIiIisTDzKbBevXohODj4rtcXLlyIFi1aYO3atbC3twcAdOjQATNnzsTevXsR\nFBQEAMjOzsa2bduQkJCAmJgYAEB4eDhCQ0ORmJiI1NRUo2thCYyIiEgszFQCu11xcTEqKysNtu/Z\nswfh4eFC8AMAYWFhsLOzQ0ZGhtCWmZkJiUSCyMhIoc3a2hojR47EoUOHUFBQYHQdDICIiIjooXjr\nrbcQGBgIPz8/jBs3DqdOnRKunTp1CpWVlfDx8dEZY2VlBS8vL8jlcqFNLpfD3d1dJ1ACAF9fX2g0\nGp2+d8MSGBERkViYsASmVCqhVCr12h0dHeHo6KjTJpFI8Oyzz6J///5o2bIlTp06hVWrVuGFF17A\nxo0b4e7uDoVCAQCQSqV6c0qlUhw9elT4WqFQ6Owdur0fgHplgBgAERERiYRGY7pj8CkpKViyZIle\n+6RJkzB58mSdtoCAAAQEBAhfDxw4EE8//TRGjBiBJUuWYMGCBSgvLwdQk/G5k7W1tXAdAMrLyyGR\nSAz2A4CKigqj62cARERERA0WHR2NiIgIvfY7sz934+npiaCgIPz5558AABsbGwA1x9vvVFFRIVyv\n7atWqw32A7SBUF0YABEREYmFCV+FYajU1VDt2rUTAqDa8lVtKex2CoUCTk5OwtdSqdRgmat27O19\n74aboImIiMSiutp0HxO4ePEiWrZsCQDo1q0bmjZtipycHJ0+KpUKcrkcXl5eQpunpyfOnz+PkpIS\nnb5ZWVnCdWMYABEREYmFmY7BX79+Xa/t4MGD2LdvH/r16wcAcHBwQFBQEGQymU5gI5PJUFpaipCQ\nEKEtJCQEarUaGzZsENpUKhXS0tIQEBBgcIP0nVgCIyIiogcqLi4Otra28Pf3R8uWLfHXX39h3bp1\naNmypc6G6fj4eIwePRpRUVGIjIxEfn4+Vq9ejf79+6NPnz5CPz8/P4SEhCAxMREKhQKurq5IT09H\nXl4e5syZU681WWg0Go3Jv9N7pC48Z+4lEImOrcsT5l4CkWhVqi4/1PuVH/jeZHPZ9BxR775r1qzB\nli1bkJubi+LiYrRq1Qr9+vXD5MmT4eLiotP34MGDSExMxIkTJ9CsWTMMGTIEU6dOhZ2dnU6/iooK\nJCUlYcuWLSgqKoKHhwemTp2qEyjVhQEQkcgxACIyn4ceAO3fYLxTPdn0ijTeqRHjHiAiIiISHe4B\nIiIiEgszvwy1MWEAREREJBYmfA7Qvx1LYERERCQ6zAARERGJBUtgAgZAREREYsEASMASGBEREYkO\nM0BEREQiodFUmXsJjQYDICIiIrFgCUzAEhgRERGJDjNAREREYsHnAAkYABEREYkFS2AClsCIiIhI\ndJgBIiIiEguWwAQMgIiIiMSCJTABS2BEREQkOswAERERiQVLYAIGQERERGLBEpiAJTAiIiISHWaA\niIiIxIIZIAEDICIiIrHgHiABS2BEREQkOswAERERiQVLYAIGQERERGLBEpiAJTAiIiISHWaAiIiI\nxIIlMAEDICIiIrFgCUzAEhgRERGJDjNAREREYsESmIABEBERkVgwABKwBEZERESiwwCIiIhILDQa\n033uQ3JyMjw8PBAWFqbTHhUVBQ8PD71PfHy83hwqlQrz589Hv3794Ovri1GjRmHv3r31XgNLYERE\nRGLRCEpgCoUCS5cuhZ2dncHrLi4uiIuL02lr3769Xr/p06djx44dGDt2LNzc3JCeno7Y2FisXbsW\n/v7+RtfBAIiIiIgemgULFsDHxwcajQZKpVLvuqOjo15m6E7Z2dnYtm0bEhISEBMTAwAIDw9HaGgo\nEhMTkZqaanQdLIERERGJRXW16T73IDs7G5s3b0ZCQkKd/SorK1FSUnLX65mZmZBIJIiMjBTarK2t\nMXLkSBw6dAgFBQVG18IAiIiISCw01ab7NPTWGg0+/PBDhIeHw8vL6679zp49ix49eiAgIAD9+vXD\nsmXLUH1HwCWXy+Hu7g57e3uddl9fX2g0GsjlcqPrYQmMiIiIGkypVN61hOXo6KjXvmnTJpw5cwaf\nf/75Xefs2LEjevfuDQ8PDxQXF2Pr1q1YtGgR8vLyMGvWLKGfQqGAs7Oz3nipVAoA9coAMQAiIiIS\nCxNugk5JScGSJUv02idNmoTJkyfrtBUXF2PBggUYP348nJyc7jrn7Nmzdb6OiIjAlClTsH79esTE\nxKBz584AgPLyckgkEr3x1tbWAICKigqj62cAREREJBb3eXz9dtHR0YiIiNBrN5T9Wbp0KSQSCV5+\n+eUG32fcuHHIzMzEvn37hADIxsYGarVar29t4FMbCNWFARARERE12N1KXXcqKChASkoKpkyZgsLC\nQqG9oqICarUaly5dgoODA5o3b25wfNu2bQEARUVFQptUKjVY5lIoFABQZ5apFgMgIiIisTDDc4Cu\nXbsGtVqNxMREJCYm6l0fOHAgYmNjMW3aNIPjL168CABo1aqV0Obp6Ym1a9eipKREZyN0VlaWcN0Y\nBkBERERiYYYAqEOHDgY3PiclJaG0tBTvvPMOOnXqhOLiYlhZWcHKykroU1VVheXLl8PS0hJBQUFC\ne0hICFatWoUNGzYIzwFSqVRIS0tDQECAwQ3Sd2IARERERA+Mg4MDgoOD9dpTUlLQpEkT4dq+ffvw\n5ptvIjQ0FK6urigtLUVGRgZycnIQGxuLjh07CmP9/PwQEhKCxMREKBQKuLq6Ij09HXl5eZgzZ069\n1sUAiIiISCzu4fk9D4uLiwsCAgKwY8cOFBYWwtLSEl27dsXcuXMNbraeN28ekpKSIJPJUFRUBA8P\nD6xYsQKBgYH1up+FRmPCLeH3SV14ztxLIBIdW5cnzL0EItGqVF1+qPcrXaH/UtF7ZTd+kcnmMgc+\nCZqIiIhEhyUwIiIisWgEb4NvLBgAERERiUUj3gP0sLEERkRERKLDDBAREZFYVDeac09mxwCIiIhI\nLLgHSMAAiIiISCwYAAkYAP1L+fQdbLDd1tYGB35Kr3OsurISsxcuxfGTp5GXX4CS0lI4tWkNHy8P\nvBoVCa9uXR7EkutFpVJhRcp32LJ9JwoKr8FZ2gYRzw3CuJciIWmq/et6VVGIzRk/Y/e+g7hw8TJK\nSkrh0tYZTwT1xKtRo9CiufEX9BHdi7f/Own+/t0R4N8dnTu74cKFi+jS7fEGzRH76kt44oneCAjw\nRdcu7mjSpAmaWrV/QCuuv3btnDH743cQ8uxTaNbMDsdPnMb8xC/w/fdbdfr59/DBmNERePKpvnDv\nVPN03jNnLyAlZT1WfpmKyspKcyyfqEEYAP2LBfr5YGSYbiDUtEkTo+PU6kocP/kXenR/FEOffRp2\ndna4crUAm374EWNi47F84YfoHdjjQS27TtPem4udv+9FROgg+Hl7Ieu4HJ8lr0HupTx8PPNNod8v\nf+zDF6u+Rv+gXnj5hSDY29ni2InT+Hr9JmT8/CvWrVyMNq1b1XEnonvz8UcJuHbtBo4cOYYWLe4t\n0H77v5PQunVLHD2aA3s7O3Ts6GLiVTZcy5Yt8OuudDg5tUHS4hW4dOkKxowOx7pvl+OVV+ORsma9\n0HfatAkY+PQTkG3OxJdffoMmTSzx3JBgLPlsNoYNHYQhoS+a8TuhOjWeZx+bHZ8E/S/l03cwwgYH\n6wQF90tReB3PDB+LoJ7+WLrgQ5PNu2nbj5g5eyG2b/wK7dvd/QV1v+3ZjwlvvY/o0cPx1uRYoX3+\nZ8lI+S4Na5ctgH/3RwEAZ879jRbNHfSCnI2bM/HBJ4sRPWY43poUCzKOT4JuGHd3V5w/nwsAOHrk\nZzSzt29wBsjNrQNycy9Do9FAlp6C554LfmAZoC9XLsKA/kFG1zh3zgxMe3MCwiNisHXbjwAAS0tL\n7P59Mzp37oTOXXqhpKQUANAn6DEcOnwMFRUVOnOkfPUpXnxhBMLCo7Hth58eyPfz/81DfxL0QtP9\nXLSbmmyyucyBx+D/5dRqNUpLy0wyV6uWzWFlZQXlrWK9a4rC65g1/zMEDx+LHgOG4qlhL+KDTxbj\n2o2bJrk3AGz78RcAwEujwnXaa7/eun2n0Nals5vBDM/ggf0B1ARIRA9CbfBzP/7++xIa8rtnYIAv\nNm5Yify8Yyi5dQ7Hc35DwvT/oEk9Mr71Nfr5CJw5c14IfgCguroaS75YjdatW2Lw4IFC+569B/WC\nHwDYsGELAMDb28Nk6yJ6UBpUAissLIRcLkdBQQHKy8thY2MDJycneHp6QiqVPqg10l3s+OUPbN2x\nE1VV1WjVojlCBvbH5PHRcGhmX6/xVVVVUN4qRlVVFa4UKPDVN2koLSvDE0E9dfpdyS/Ai69NhbpS\njeGhz6Jj+3bIvZSHdenbsP9wNtZ9+Wm971mX4/LTcJa2Rjtn3b9L7ZylcGrTGjny00bnyFcUAgBa\nt2xx3+shagyGDB6IDeuTcebsBSxKWo7r12/i8ccD8cH70+Dn543RY16773u0beuEDh3aIfWb7/Wu\n7dt3GADQM9APGzduqXOe9u3bAQCuXlXc95roAeExeEG9AqCsrCwkJibi0KFD0Gg0er+5WFhYIDAw\nENOmTUOPHubZOyI23R/1wKCn+sG1gwuKS0rx+94D+Ob7LThw9BhSly2EnZ2t0TnO/X0REVFvCF87\nNLPHq1HP49Wo53X6zV60FJWVldiwegnaOmmDk0FPPYEXX4vHmnXpmPjKS/f9PRUUXscj7q4GrzlJ\nW+NqQaHROb748msAQNiQ4PteD5G5WVtbY8XyROzffwTBg0ahqqoKAJC88mtkZ5/AgsQPMKB/EH79\nbe993cfln9J0Xl6+3rXLl6/U9Gnfts457O3t8ObU13HzZhE2b9lxX+uhB4hPghYYDYD27t2L2NhY\nuLi4IC4uDt27d4eTkxOsrKygUqlQUFCArKwspKenIyoqCsnJyXj88YbVw6nhvk1O0vk6bHAwuj3i\njk9XpGDthk14LXqM0Tnat2uL5KTZUKvVyL18BVu370RxSQlUajWaNq1Jrd8qLsGve/YjfMgzsLay\nwo2bRbeNd4Zrexfs2X9YCIDUlZUoLi7RuU9pWU2JTnnrFuxsbXSutWzRXPhzeUUFrCQSg2u1trJC\nuYGU++2++vZ7bN/5OyLDBpttEzeRKT0T3B9t2zphxsw5ehuuMzJ3YkHiB3jmmQFCAGRra6P3y4+1\ntRUsLS3RunVLnXa1uhJK5S0AEMZUVKj01lBeXqHTxxBLS0usSfkMnTu74cWoCbhhwtI40YNiNABK\nSkpC9+7dkZKSAisrK73rjzzyCIKCgjBu3DiMHTsWCxcuxPr16w3MRA/ayy+OxNLVqfhtz4F6BUB2\ntjYI6ukvfD38uUGIHDcZ8TM+wvKFHwEALuReQnV1NdK2bkfa1u0G5+ngov3N8Ej2CYyb/LbBfpEv\nT9Zry9mdIfzZxtoaKrXa4NgKlQo21tZ3/V42bs7Egs+/RP8+vfDO1Al37Uf0b+LpWfNIii9XLrpr\nHyenNsKf35o2Ae+9a/hgxNUrOTpf//rrHgx8JhIAhH2E1tb6P+NtbKx1+tzJwsICK5MXImxYCGa+\nOxfr1snuulZqBFgCExgNgE6ePImZM2caDH5uZ2VlheHDh+Pjjz822eKoYSRNm8KpTWvcLFLe03g7\nO1sED+iDL7/egNxLeXDt4CKUO0OffRphgw2XlW7/oenRxR3JSbN1ru/Zfxirv9mIue+9hdatWt45\nXODUphUKFIbLXAWKa3CStjZ4LW3rdvxv3qfo0ysASR/P0HleENG/mYWFBQDgv2/PQlbWCYN98q5o\ny1Zrv96I3bsP6Fx/c+rr8PV9FNEx/9Fpvz1Lk3flKgDAxUW/zFW7ryfvsn55zMLCAiuWJ2JsVCRm\nfbgAcz/5rD7fFpmRhg9CFBj9l8LR0RG5ufU79ZCbmwtHRz6AzlwqKlS4WlAIX2/Pe56j/J8UeNGt\nmtS4awcXWFhYQK2u1MkW3U1zRwe9frV7d/x9ves8Bu/t1Q3bduzClasKnY3QV64qUFB4DU/26603\nJm3rdrw/dzEef6wHPp3zntFAnejf5K8z5wEAJSVl+Hnn70b7nz+fq3dK7YUXhqNbt0fqHJ+fX4BL\nl66gd68AvWu9e9e0HTycrdNeG/y8HDMaH89OwqwPFxpdH1FjYvQY/LBhw/DVV19h7dq1KCsznAIt\nKyvDmjVrkJKSgmHDhpl8kaTrbhmez5LXoLKqSidQUBRex7m/L6KsvFxou37jJqoN/BZQeO06duz8\nHXa2tuji7gYAaNHcEU8E9cTPv+5GVo5cb4xGo8F1E9X7hzzzJADg6/WbdNprvw4d9JRO+6ZtP+KD\nTz5F70A/fPbJ+wbT90Tm1LGjCzw8HkHTe8xK7tjxC65eVeC/b01ESwMnG21sbNDMBCcwAWDduk3o\n0sUdoc89I7RZWlpi0oSXcePGTWRk/KzTf/my+Xg5ZjTmzP0U738w3yRroIegWmO6z7+c0f8rp0yZ\ngitXruDjjz/GvHnz0LlzZ0ilUmETtEKhwLlz56BWqxESEoIpU6Y8jHWL2vKvvkX28ZPoGeCHds5S\nlJaV4fe9B7H/cBZ8H/XACyO1QWjSstWQZfyEVZ99gl4BvgCAbTt2Ye36TRjYvw/au7SFpGlT/H3x\nMmQZP0F5qxj/mz4FtjbazcrvTpuEsW9MQ/TE/2JYyEB4dXsE1dXVuJSXj11//ImhIQNNcgpsQJ9e\nGNC3F1K+S8Ot4hL4+XghK0eOtK3bEfrs0wjw8xH67vr9T7w3NwnN7O0QMrA/fvxlt85cdrY2GNi/\nz32viehOL744Am6uHQAA0jatYWUlwTsJNT/3/s69hNRU7VHyr1YtxoABffBI1974++9LQnvoc8/A\n17fmoZ6PdOkEAMIcN28W4YulXwGo2Xfz8rgp+H7jlziR8xu+SvkOZ85cQIsWjvDw6IKI8MEYGfnq\nfZ8CA4BP5n+OESNCsXbNEiQtXoHLl/Mx+vlw9Ozpj9jxb+ocbpg3912Me3kMjmYdh/zkX3jhheE6\nc507+zf+3HfovtdEDwBPgQmMBkBWVlZYuHAhYmJikJmZiZMnT+Lq1avCc4CkUin69u2LkJAQ+Pr6\nPow1i17PAF+cvZCLzRk/4aZSiSaWlnDt0B7/GR+N6NHDjWZCAvx8kHPyL/y6Zz8U165Dra5E61Yt\n8Phj/nhpVJjwtOVa7ZylWL/qU3z59Qbs+uNPbN2xE9ZWVmjrJMWAvr0R8rTpniS88MMZWJ7yLbZu\n34kt23+Gs7QNJr0ahVeiRun0O3H6DKqrq6G8VYwPPvlUbx6Xtk4MgOiBGBczGgMG6P7dmvW//wKo\n2Vh8ewB0NxERQxA9VvfvdO0cFy5cFAIgANjx4694vM9z+O9bE/HCmOGQSlvjxo0inDv3N5IWJyP7\nmH5m9l5cv34D/Z8Mx+yP38Ebr8egWTM7nJD/hTEvvoENGzbr9A0MrPlZ38PPG2u+0t/3k7JmPQMg\navT4KgwikeOrMIjM52G/CqNklune02b/XqrJ5jIHHpchIiISC54CE/BdYERERCQ6zAARERGJxf+D\n01umwgCIiIhILHgKTMASGBEREYkOM0BERERiwRKYgAEQERGRSPBdYFosgREREZHoMANEREQkFiyB\nCRgAERERiQUDIAFLYERERPRQJScnw8PDA2FhYXrXDh8+jDFjxsDPzw99+/bFRx99hLKyMr1+KpUK\n8+fPR79+/eDr64tRo0Zh7976vxiYARAREZFYaKpN97lHCoUCS5cuhZ2dnd41uVyOmJgYVFRUYPr0\n6Rg5ciTWrVuH+Ph4vb7Tp09HSkoKhg0bhhkzZsDS0hKxsbE4cuRIvdbBEhgREZFYNIIS2IIFC+Dj\n4wONRgOlUqlzbeHChWjRogXWrl0Le3t7AECHDh0wc+ZM7N27F0FBQQCA7OxsbNu2DQkJCYiJiQEA\nhIeHIzQ0FImJiUhNNf6iVmaAiIiI6KHIzs7G5s2bkZCQoHetuLgYe/bsQXh4uBD8AEBYWBjs7OyQ\nkZEhtGVmZkIikSAyMlJos7a2xsiRI3Ho0CEUFBQYXQszQERERCKhMWMGSKPR4MMPP0R4eDi8vLz0\nrp86dQqVlZXw8fHRabeysoKXlxfkcrnQJpfL4e7urhMoAYCvry80Gg3kcjmcnJzqXA8DICIiIrEw\nYQCkVCr1SlgA4OjoCEdHR732TZs24cyZM/j8888NzqdQKAAAUqlU75pUKsXRo0d1+jo7OxvsB4AZ\nICIiInowUlJSsGTJEr32SZMmYfLkyTptxcXFWLBgAcaPH3/XzEx5eTmAmozPnaytrYXrtX0lEonB\nfgBQUVFhdP0MgIiIiMTChK/CiI6ORkREhF67oezP0qVLIZFI8PLLL991PhsbGwA1x9vvVFFRIVyv\n7atWqw32A7SBUF0YABEREYmFCUtgdyt13amgoAApKSmYMmUKCgsLhfaKigqo1WpcunQJDg4OQvmq\nthR2O4VCoZM5kkqlBstctWON7f8BeAqMiIiIHqBr165BrVYjMTERAwcOFD5ZWVk4e/YsBg4ciOTk\nZHTr1g1NmzZFTk6OzniVSgW5XK6zcdrT0xPnz59HSUmJTt+srCzhujHMABEREYmFGU6BdejQweDG\n56SkJJSWluKdd95Bp06d4ODggKCgIMhkMrz22mvCCS+ZTIbS0lKEhIQIY0NCQrBq1Sps2LBBeA6Q\nSqVCWloaAgICDG6QvhMDICIiIpHQaB5+AOTg4IDg4GC99pSUFDRp0kTnWnx8PEaPHo2oqChERkYi\nPz8fq1evRv/+/dGnTx+hn5+fH0JCQpCYmAiFQgFXV1ekp6cjLy8Pc+bMqde6GAARERFRo+Dt7Y3V\nq1cjMTERc+bMQbNmzTBq1ChMnTpVr++8efOQlJQEmUyGoqIieHh4YMWKFQgMDKzXvSw05ggH70Jd\neM7cSyASHVuXJ8y9BCLRqlRdfqj3U8YOMtlcjsk7TDaXOTADREREJBaN4F1gjQVPgREREZHoMANE\nREQkEuZ8F1hjwwCIiIhILBgACVgCIyIiItFhBoiIiEgsTPcqsH89BkBEREQiwT1AWiyBERERkegw\nA0RERCQWzAAJGAARERGJBfcACVgCIyIiItFhBoiIiEgkuAlaiwEQERGRWLAEJmAJjIiIiESHGSAi\nIiKRYAlMiwEQERGRWLAEJmAAREREJBIaBkAC7gEiIiIi0WEGiIiISCyYARIwACIiIhIJlsC0WAIj\nIiIi0WEGiIiISCyYARIwACIiIhIJlsC0WAIjIiIi0WEGiIiISCSYAdJiAERERCQSDIC0WAIjIiIi\n0WEGiIiISCw0FuZeQaPBAIiIiEgkWALTYgmMiIiIRIcZICIiIpHQVLMEVosBEBERkUiYqwR27Ngx\nLFu2DCdOnMC1a9fg4OAAT09PTJw4EQEBAUK/qKgo7N+/X2/8kCFDsGjRIp02lUqFxYsXQyaTQalU\nwtPTE/Hx8QgKCqrXmhgAERER0QN18eJFVFVVITIyElKpFLdu3cKWLVvw0ksvITk5GX379hX6uri4\nIC4uTmd8+/bt9eacPn06duzYgbFjx8LNzQ3p6emIjY3F2rVr4e/vb3RNFhqNRnP/35ppqAvPmXsJ\nRKJj6/KEuZdAJFqVqssP9X6Xg5422Vzt9+68r/FlZWUIDg6Gj48Pli9fDqAmA6RUKiGTyeocm52d\njcjISCQkJCAmJgYAUFFRgdDQUDg5OSE1NdXo/bkJmoiISCQ01ab73C9bW1u0atUKSqVS71plZSVK\nSkruOjYzMxMSiQSRkZFCm7W1NUaOHIlDhw6hoKDA6P0ZABEREdFDUVxcjOvXr+PcuXNYuHAhTp8+\nrbdn5+zZs+jRowcCAgLQr18/LFu2DNXVuhGXXC6Hu7s77O3tddp9fX2h0Wggl8uNroV7gIiIiETC\nlKfAlEqlweyNo6MjHB0dDY555513sH37dgCARCLB6NGj8frrrwvXO3bsiN69e8PDwwPFxcXYunUr\nFi1ahLy8PMyaNUvop1Ao4OzsrDe/VCoFgHplgBgAERERiYQpd/2mpKRgyZIleu2TJk3C5MmTDY6Z\nOHEinn/+eeTn50Mmk0GlUkGtVsPKygoAMHv2bJ3+ERERmDJlCtavX4+YmBh07twZAFBeXg6JRKI3\nv7W1NYCa/UDGMAAiIiKiBouOjkZERIRe+92yPwDg4eEBDw8PAMCwYcMwYsQIJCQk4NNPP73rmHHj\nxiEzMxP79u0TAiAbGxuo1R7eLuMAACAASURBVGq9vrWBT20gVBcGQERERCJhyhJYXaWu+pBIJBg4\ncCCWLl2K8vJy2NjYGOzXtm1bAEBRUZHQJpVKDZa5FAoFAMDJycno/bkJmoiISCQ01RYm+5hCeXk5\nNBpNnSe+Ll68CABo1aqV0Obp6Ynz58/rjcvKyhKuG8MAiIiIiB6o69ev67UVFxdj+/btaNeuHVq3\nbo3i4mKoVCqdPlVVVVi+fDksLS11TouFhIRArVZjw4YNQptKpUJaWhoCAgIMbpC+E0tgREREImGu\nRx/HxcXB2toa/v7+kEqluHLlCtLS0pCfn4+FCxcCAI4fP44333wToaGhcHV1RWlpKTIyMpCTk4PY\n2Fh07NhRmM/Pzw8hISFITEyEQqGAq6sr0tPTkZeXhzlz5tRrTQyAiIiIRMJcL0MdNmwYZDIZ1q5d\nC6VSCQcHB/To0QPz5s1Dr169ANS8AiMgIAA7duxAYWEhLC0t0bVrV8ydO9fgZut58+YhKSkJMpkM\nRUVF8PDwwIoVKxAYGFivNfFVGEQix1dhEJnPw34Vxrnug0w2V+djO0w2lzkwA0RERCQSGo15MkCN\nEQMgIiIikTDFO7z+v+ApMCIiIhIdZoCIiIhEopolMAEDICIiIpHgHiAtlsCIiIhIdJgBIiIiEglz\nPQeoMWIAREREJBKN58l/5scSGBEREYkOM0BEREQiwRKYFgMgIiIikeAxeC2WwIiIiEh0mAEiIiIS\nCT4HSIsBEBERkUjwFJgWS2BEREQkOswAERERiQQ3QWsxACIiIhIJ7gHSYgmMiIiIRIcZICIiIpHg\nJmgtBkBEREQiwT1AWo0qAPL3fsHcSyASnVs7PjT3EoiIHrpGFQARERHRg8NN0FoMgIiIiESCJTAt\nngIjIiIi0WEGiIiISCR4CEyLARAREZFIsASmxQCIiIhIJLgJWot7gIiIiEh0mAEiIiISiWpzL6AR\nYQBEREQkEhqwBFaLJTAiIiISHQZAREREIlGtMd2nIY4dO4aJEyfiqaeegq+vL/r27YtXXnkFhw8f\n1ut7+PBhjBkzBn5+fujbty8++ugjlJWV6fVTqVSYP38++vXrB19fX4waNQp79+6t95oYABEREYlE\nNSxM9mmIixcvoqqqCpGRkXj33Xfxyiuv4Pr163jppZewe/duoZ9cLkdMTAwqKiowffp0jBw5EuvW\nrUN8fLzenNOnT0dKSgqGDRuGGTNmwNLSErGxsThy5Ei91mSh0WgazXORfJwfN/cSiETnwLrx5l4C\nkWjZPjnuod5vp/Mok8319NX19zW+rKwMwcHB8PHxwfLlywEAsbGxOHXqFDIyMmBvbw8A2LBhA2bO\nnImvvvoKQUFBAIDs7GxERkYiISEBMTExAICKigqEhobCyckJqampRu/PDBAREZFIaGBhss/9srW1\nRatWraBUKgEAxcXF2LNnD8LDw4XgBwDCwsJgZ2eHjIwMoS0zMxMSiQSRkZFCm7W1NUaOHIlDhw6h\noKDA6P15CoyIiEgkzH0Mvri4GCqVCjdv3sSmTZtw+vRpTJw4EQBw6tQpVFZWwsfHR2eMlZUVvLy8\nIJfLhTa5XA53d3edQAkAfH19odFoIJfL4eTkVOdaGAARERFRgymVSiF7cztHR0c4OjoaHPPOO+9g\n+/btAACJRILRo0fj9ddfBwAoFAoAgFQq1RsnlUpx9OhR4WuFQgFnZ2eD/QAwA0RERERapnwOUEpK\nCpYsWaLXPmnSJEyePNngmIkTJ+L5559Hfn4+ZDIZVCoV1Go1rKysUF5eDqAm43Mna2tr4ToAlJeX\nQyKRGOwH1OwHMoYBEBERkUiYsgQWHR2NiIgIvfa7ZX8AwMPDAx4eHgCAYcOGYcSIEUhISMCnn34K\nGxsbADXH2+9UUVEhXAcAGxsbqNVqg/0AbSBUFwZARERE1GB1lbrqQyKRYODAgVi6dCnKy8uF8lVt\nKex2CoVCZ0+PVCo1WOaqHWts/w/AU2BERESiUW3CjymUl5dDo9GgpKQE3bp1Q9OmTZGTk6PTR6VS\nQS6Xw8vLS2jz9PTE+fPnUVJSotM3KytLuG4MAyAiIiKRMNcx+OvXr+u1FRcXY/v27WjXrh1at24N\nBwcHBAUFQSaT6QQ2MpkMpaWlCAkJEdpCQkKgVquxYcMGoU2lUiEtLQ0BAQEGN0jfiSUwIiIieqDi\n4uJgbW0Nf39/SKVSXLlyBWlpacjPz8fChQuFfvHx8Rg9ejSioqIQGRmJ/Px8rF69Gv3790efPn2E\nfn5+fggJCUFiYiIUCgVcXV2Rnp6OvLw8zJkzp15r4pOgiUSOT4ImMp+H/SToLW3HmGyuofnf1rvv\nxo0bIZPJcObMGSiVSjg4OKBHjx4YN24cevXqpdP34MGDSExMxIkTJ9CsWTMMGTIEU6dOhZ2dnU6/\niooKJCUlYcuWLSgqKoKHhwemTp2qEyjVhQEQkcgxACIyn4cdAMnavmCyucLyvzHZXObAPUBEREQk\nOtwDREREJBKNpuTTCDAAIiIiEglzvwusMWEJjIiIiESHGSAiIiKRqLYw3bvA/u0YABEREYkE9wBp\nsQRGREREosMMEBERkUhwE7QWAyAiIiKRqOYWIAFLYERERCQ6zAARERGJRHUD3+L+/xkDICIiIpHg\nKTAtlsCIiIhIdJgBIiIiEglugtZiAERERCQSPAavxRIYERERiQ4zQERERCLBTdBaDICIiIhEgnuA\ntFgCIyIiItFhBoiIiEgkuAlaiwEQERGRSDAA0mIJjIiIiESHGSAiIiKR0HATtIABEBERkUiwBKbF\nEhgRERGJDjNAREREIsEMkBYDICIiIpHgk6C1WAIjIiIi0WEGiIiISCT4KgwtBkBEREQiwT1AWiyB\nERERkegwA0RERCQS5soAZWdnIz09Hfv27UNeXh5atGgBf39/xMXFwc3NTegXFRWF/fv3640fMmQI\nFi1apNOmUqmwePFiyGQyKJVKeHp6Ij4+HkFBQfVaEwMgIiIikTDXKbCVK1fi8OHDCAkJgYeHBxQK\nBVJTUxEeHo6NGzfikUceEfq6uLggLi5OZ3z79u315pw+fTp27NiBsWPHws3NDenp6YiNjcXatWvh\n7+9vdE0MgIiIiOiBiomJQWJiIqysrIS2IUOGYOjQoUhOTsbcuXOFdkdHR4SFhdU5X3Z2NrZt24aE\nhATExMQAAMLDwxEaGorExESkpqYaXRP3ABEREYlEtYXpPg0REBCgE/wAQKdOndC1a1ecPXtWr39l\nZSVKSkruOl9mZiYkEgkiIyOFNmtra4wcORKHDh1CQUGB0TUxACIiIhKJahN+7pdGo0FhYSFatmyp\n03727Fn06NEDAQEB6NevH5YtW4bqat07yuVyuLu7w97eXqfd19cXGo0Gcrnc6P1ZAiMiIhIJU+4B\nUiqVUCqVeu2Ojo5wdHQ0On7z5s24evUq4uPjhbaOHTuid+/e8PDwQHFxMbZu3YpFixYhLy8Ps2bN\nEvopFAo4OzvrzSmVSgGgXhkgBkBERETUYCkpKViyZIle+6RJkzB58uQ6x549exazZs1CYGCgzn6f\n2bNn6/SLiIjAlClTsH79esTExKBz584AgPLyckgkEr15ra2tAQAVFRVG188AiIiISCSqTZgDio6O\nRkREhF67seyPQqHAa6+9hubNm2Px4sWwtKx7N864ceOQmZmJffv2CQGQjY0N1Gq1Xt/awKc2EKoL\nAyAiIiKRMOVzgOpb6rrdrVu3EBsbi1u3buHbb78VSlZ1adu2LQCgqKhIaJNKpQbLXAqFAgDg5ORk\ndF5ugiYiIqIHrqKiAq+//jouXLiA5cuXC9kcYy5evAgAaNWqldDm6emJ8+fP650Uy8rKEq4bwwCI\niIhIJDQm/DREVVUV4uLicPToUSxevBg9evTQ61NcXAyVSqU3bvny5bC0tNR5wnNISAjUajU2bNgg\ntKlUKqSlpSEgIMDgBuk7sQRGREQkEuZ6FcbcuXOxc+dOPPXUU7h58yZkMplwzd7eHsHBwTh+/Dje\nfPNNhIaGwtXVFaWlpcjIyEBOTg5iY2PRsWNHYYyfnx9CQkKQmJgIhUIBV1dXpKenIy8vD3PmzKnX\nmhgAERER0QN18uRJAMCuXbuwa9cunWvt27dHcHAwXFxcEBAQgB07dqCwsBCWlpbo2rUr5s6da3Cz\n9bx585CUlASZTIaioiJ4eHhgxYoVCAwMrNeaLDQajbleDaLHx/lxcy+BSHQOrBtv7iUQiZbtk+Me\n6v3e6/SiyeaadcH46yYaM2aAiIiIRMKUx+D/7bgJmoiIiESHGSAiIiKRYP5HiwEQERGRSJjrFFhj\nxBIYERERiQ4zQERERCLBTdBaDICIiIhEguGPFktgREREJDrMABEREYkEN0FrMQAiIiISCe4B0mIJ\njIiIiESHGSAiIiKRYP5HiwEQERGRSHAPkBZLYERERCQ6zAARERGJhIZFMAEDICIiIpFgCUyLJTAi\nIiISHWaAiIiIRILPAdJiAPQv9ep/xsKruwce9fNER7f2uJx7Bc/2jGjwPDa21oh540WEhAWjg5sL\nKsorcP5sLlYv+Ro/Z/z6AFZunNS5DeJnTkC/p4NgZ2+Ls6fO48sla7Fjy06dfl7dPfDc8EHo1e8x\ntHdtBwC4eP4S0r/bhu+/3oTKyipzLJ9Epkylxsj/fYnLhUV4/skAJIx5xugYjUaDjANyfLfrEHKv\n3oCqsgptWzng2ce88OLAx9DM1vohrFzfrbIKfL7pN/x89DSKisvQQdoCo58KRGT/HrCwsBD6/X31\nOrbtO469J87jkuImKtRV6CBtgWcCPfDSwMdga21llvWTcQx/tBgA/UvFzZiAm9eLID92Co6Oze5p\nDsfmDli5cQncOnfApu+2Yc3yv2BrZ4vOXTuhXce2Jl5xPdfUwhFrtixH6zYtkbLsW1y9UoDnIp7F\nwpWzMXPKh9j03Tah77hJL+HxJ3piZ+Zv+P5rGSybWGLAM33x7idv4emQJ/Da6DizfA8kLks3/44b\nt8oaNGaJ7Hd8mbEXvTzc8FpoXzRtYomDp3OxdMsf+CPnLNa8HaUTcDwM6soqvJ60Dqdyr2L0UwFw\nb9cau3POY/Y3O3BNWYI3hvYT+m7anY11vxzBAL8uGNLLG02bWOLAqVx8LvsdPx46iTVvR8HGSvJQ\n10/UUAyA/qVCeg3Hpb/zAADpv6bCzs6uwXMkfDwVru4dMGbwKzh3+oKJV6jro8XvomefAKNZqlcn\nj0VHt/aYGDUNv+74AwCQlroFqT8kY9r7/8H2zTtRVlrzj803Kzdgxn8+hKpCJYz/dtVGzP38A4SO\nDMGAZ/ri1x93P7hvikRPnpuP1J8PIm74k1iwcVe9xlRWVSP154PwcnXGsrjnYWlZE+hEDvBHE0tL\n/LD/BE5dKoBnR2eTrHHplj+wfOtuHF3+dp390v7IwvELV/D288EY83QgAGDEEz3w5rJ0fJmxF2F9\nusOldXMAQHCAB8YNDoLDbZmqyAH+cN3UEisz9mLT7myMfirQJOsn02IJTIuboP+laoOfe+XSsR2G\nDB+EjV/LcO70BVhaWsLWzrbOMd5+nli8ei5+P5GJw7m/YcvudRgfF4MmTZrc11puN2T4IOSevygE\nPwBQXV2Nb1ZuQItWzdE/uI/QfuRAtk7wUytD9hMAoItnZ5Oti+hOVdXVmLU2E328O+Npf496j6us\nqkKFWo3WjvZC8FNL2qImm2t7R/bk76vXMWPVVgS/tQSPTZiPwe8sxcKNu1Bm4O//vcrYfwI2VhIM\nf8JPp/3FgY+hsqoa2w+eFNq8O7XTCX5qPdvTCwBwJq/QZOsi06o24effzuQBUGpqKgYOHGjqacnE\n+j31OJo0aYJzp89jzpL3cfDCLzhwfhd+OrIZUa+N1uvfP7gP1m5ZAbfOrkhZ9g3mzFiIrIM5mPjf\nWMxbNsska2rj1BptXZyQfei43rWsf9p8engZnadtOycAwDXFdZOsi8iQr386gPP51zF9tPE9P7ez\nsZIgoGtH7Dl+Hqsz/0RuwQ1cLiyCbM8xrP/1CJ7r7Q0351ZC/xN/5+PF2Sk4/NdFjOjfAwljnkH/\n7l3w7a5DeC1pHdRV97/Xrbpag5O5V+HZ0QnWEt3CgE+ndrCwAI5fuGJ0nqs3lACAVg4Nz0gTPWwm\nL4EplUrk5d1fdoIevE5dXAEAU2a8gZvXijDrv59Ara7EqLEReHtWHBwdm+Hz+SsBAFbWVpi1aAay\nDx/HKyMmoeqfH7gb1m7CqRN/4e1Zcfhu9fc4sOfwfa3JqW0bAMDVKwq9awX5BTV92knrnMPWzhYx\nE1+EsugWdmb+fl/rIbqby4U3sXTLbrz2XB+0b9MclwuLGjR+9riheC9lGxan/4rF6TWHDSwsgFcH\nB2HCsCd0+n6w5ge0ad4Mqe+Mhb2NNuvS29MNU5el44d9JxDWp/t9fT/K0nKUqyvh1MJB75qVpCla\nNLNDwc1bdc5RVV2NFdv2oKmlJYb0evS+1kMPDh+EqFWvAOjAgQP1nvDSpUv3vBh6eOyb2QMAJBIJ\nxoa9hqJ/fnPbLvsJst+/w8sTX8LaFeugLLqFoAG90MapNZI+/gIOzXU3XP/+0x68PSsOfZ7sJQRA\nNrbWsLG10elnZS2BpaUFWrRqrtNeqa5E8a2Sf8bVjFGp9NP6FeUqnT6GWFpaYu4XH6CjW3u89fq7\nUN5U1vu/B1FDfJS6HR3aNMdLz/S8p/FWkiZo37oFQh93QD8fdwDAT4dPI/mHvbCSNEXskJpS71+X\nFTh9SYE3hvaDqrIKquJSYY4eXTrA1lqCvSfOCwGQSl2JkjvKYuUqNQDgxm1jAaCJhSUc7W10+lhJ\nDJezrZs2Qbmqss7vaf66n5F9Lg+Tw/ujU9vW9frvQA/f/4fSlanUKwCKiqr/iQSNRvPQTy9Qw5WX\nVwAAfvtxtxD8AEBlZRV+SNuON6a9Ct9AH/yxcy86d+0EoGYj8920lmpT9uMmRmHCW68a7PeHfLvO\n1wd2H8bLwyfUrKmsHABgZaV/hNbaxkqnz50sLCzwYdIMDBw8AItnL0VG+o93XSvR/dj253H8Kb+A\nVdNehOQe9r+VqdSI/uRreLk645PYMKE9pOejeDtZhqWb/8AzAR7o1LY1zl2p2UuzdMsfWLrlD4Pz\nXVdqA5uMA3K8n/KDwX5PvfmZztftWjsiY/YbACCc2FKpDZfTKiqrYGN1938uPpf9hu9+OYwRT/jh\nlcFBd+1H1JjUKwCys7ODp6cnxo0bZ7RvZmYmtm3bZrQfmdfVvJqSUmHBNb1riqs1bY7/pMNrA9rE\nDz7FyeN/GZyvIF9bttq84Qcc3p+lc/3lCS/Cw7srpk/8QKf99ixNQX7ND3tnA2Uup7Y1+3oKDJTH\nLCwsMGvROwh7/jl8MX8lkhenGFwj0f1SqSuRuHEn+vk8gtaO9sgtuAEAQnmouKwCuQU30KKZLRzt\nDGcrfzp0CrkFN/CfiAF6154J9MT2gydx5MylmizKP9WKsc/0RB9vw5v6b79PH293LIt7Xuf61j9z\nsPXP43rtt+/1cbSzgY2kqcEyl0pdiZvFpQjs2tHg/Zdu+QPJP9ScEpv54rMG+1DjwRKYVr0CIB8f\nH1y9ehXBwcFG+/71l+F/IKlxOXakZlOxs4uT3rXatuuFNT/cc89dBACUlZbjz9+Ml0Mv/Z2nd0ot\ndEQIOj3iVuf4woJryM8rgG+gt941v3/ajmed1GmvDX4ixgzFsoWr8EXiSqPrI7pX5epK3LhVit+P\nncXvx87qXd+27zi27TuO+BFPInpQb4Nz1AYZVdX6xYiqqup/rtX8I+Xq3BIAYGlhice9Ohldn7R5\nM0jvKFMfOVOzLaGu8ZaWFvB0dcbJiwVQqSthdVtwlHPhCjQawNtN/9lgtUfshwb54P2owcz+/wuw\nBKZVr1Ngvr6+yM3NRVGR8Y1+Go0GGg0jzMakbXtnuHdxQ9Om2nT9ob1HcTn3CgYM6genttqMi62d\nDYZFDkbRTSWyDh4DAOz+5U9cU1zHK5Oj4NjCUW9+axtr2Nmb5tRHRvoOuLp3xIBB2oeuWVpa4oVX\nI1F0U4nfftqj0/9/CxMQMWYoViR9hSWfrDDJGojuxtZagvnjw/Q+77wwCADQ19sd88eH4Um/rgAA\nRVExzudfQ9k/e2wAoHO7ms3+W/bm6M2/+c+aNu9ONcGGZ0dndHFpg42/HcElxU29/pVV1SgqadhD\nGO8mpOejKFep8f3vutnb1J8PoqmlJQY9pnsCc/nW3Vi+dTdCH/fG/8YO0TvST9TY1SsDFB0djf79\n+0MiMf5kzwkTJmDChAn3vTCq29CRIWjXseb1D61at0RTSVOMj38ZAHDl4hVs2Zgp9J3z2fvo2TcA\ngx6LQN7FmqOs1dXV+Gj6PHy2Zj5StyXju6++h1pVifAxz6Fdh7Z4N+4jlJXW7LcpKy1HwqT/4dOv\nPsHW3euQ/u1W5F64BEfHZnDv6oaBQ55E3MvT7/sUGACs/GwNBg0diHlL/4c1y77F1XwFhkQMQnd/\nb7wX/zFKS7T7Haa9PxnDXxiGkzmnce70BYSOCNGZ6+Lfl5B1UP8fGaJ7JWnSBM8Eeuq1154C6yBt\nqXP90/RfsWVvDpKnjkFPj5qTl/19H4FPp3b4I+ccxs1PxdP+3QAAO4+cxuEzl/BMoAe8XGsCIAsL\nC3w0LhTjF36HyA9XIbyPLx5xaYMylRoXC25g55HTmBwx4L5PgQHAiCf8sHnPMSzYsBN514rg3q41\n/jh2DjuPnkbskCC0b6M9wPDdrsNYuuUPtGvliN6enZCx/4TOXK0c7RD0qPt9r4lMr5oJCkG9AiCp\nVAqptO7jx/RwDX9hGHr2DdBp+8/01wDUbCy+PQC6m99/3otXR07GG9Newfi4GFhaNsHJnNM6T2Gu\nteeXfRgdMg6vTh6L0JHPolXrllAW3cLFC5ewZtm3OHXijEm+r6IbSkQNHY/4mRMw+uWRNe8CO30B\n08bPROY/Dzis5e1X8xupp083zP3iA725Nn23jQEQNTpNLC2xPP55fJnxJ3YeOY3F6b/CAoCrU0tM\nGf4kooJ1T5Z5dnTGdzNjsCrzT/yS/Rc2/HYE9jZWcGndHMP6dEdvTzeTrEvStAmWxT+Pz2W/I/OA\nHDdLytBR2gLTRwfj+Sd1f9Yc/7vmF6kr15V49yv9PZ+B3ToyAGqkGP5oWWgaUb3Kx/lxcy+BSHQO\nrBtv7iUQiZbtk8YPF5nSS27DTTbX13+n1btvdnY20tPTsW/fPuTl5aFFixbw9/dHXFwc3Nx0g/jD\nhw9j/vz5OHHiBJo1a4bBgwfjzTffhK2t7tsKVCoVFi9eDJlMBqVSCU9PT8THxyMoqH4nEfkqDCIi\nIpGohsZkn4ZYuXIlfvzxR/Tp0wczZszAqFGjsH//foSHh+PsWe2BArlcjpiYGFRUVGD69OkYOXIk\n1q1bh/j4eL05p0+fjpSUFAwbNgwzZsyApaUlYmNjceTIkXqtiS9DJSIiEglzHYOPiYlBYmKiznPe\nhgwZgqFDhyI5ORlz584FACxcuBAtWrTA2rVrYW9f88DeDh06YObMmdi7d6+Q3cnOzsa2bduQkJCA\nmJgYAEB4eDhCQ0ORmJiI1NRUo2tiBoiIiIgeqICAAL2H3Hbq1Aldu3YVMkDFxcXYs2cPwsPDheAH\nAMLCwmBnZ4eMjAyhLTMzExKJBJGRkUKbtbU1Ro4ciUOHDqGgoMDomhgAERERiURjehu8RqNBYWEh\nWrased7VqVOnUFlZCR8fH51+VlZW8PLyglwuF9rkcjnc3d11AiWg5rE9Go1Gp+/dsARGREQkEg3d\nu1MXpVIJpVL/nYuOjo5wdNR/ZtydNm/ejKtXrwr7exSKmif9Gzp1LpVKcfToUeFrhUIBZ2dng/0A\n1CsDxACIiIiIGiwlJQVLlizRa580aRImT55c59izZ89i1qxZCAwMRFhYzTvxysvreB+ktbVwvbav\noWcTWltbAwAqKiqMrp8BEBERkUiYchN0dHQ0IiIi9NqNZX8UCgVee+01NG/eHIsXL4alZc1uHBub\nmvfaqVQqvTEVFRXC9dq+arXaYD9AGwjVhQEQERGRSJjyXWD1LXXd7tatW4iNjcWtW7fw7bff6pS7\nav9cWwq7nUKhgJOTk05fQ2Wu2rG3970bboImIiKiB66iogKvv/46Lly4gOXLl6Nz584617t164am\nTZsiJ0f3Cf4qlQpyuRxeXtr30Xl6euL8+fMoKSnR6ZuVlSVcN4YBEBERkUjUvrDcFJ+GqKqqQlxc\nHI4ePYrFixejR48een0cHBwQFBQEmUymE9jIZDKUlpYiJET7vseQkBCo1Wps2LBBaFOpVEhLS0NA\nQIDBDdJ3YgmMiIhIJEx5Cqwh5s6di507d+Kpp57CzZs3IZPJhGv29vYIDg4GAMTHx2P06NGIiopC\nZGQk8vPzsXr1avTv3x99+vQRxvj5+SEkJASJiYlQKBRwdXVFeno68vLyMGfOnHqtiQEQERERPVAn\nT54EAOzatQu7du3Suda+fXshAPL29sbq1auRmJiIOXPmoFmzZhg1ahSmTp2qN+e8efOQlJQEmUyG\noqIieHh4YMWKFQgMDKzXmvgyVCKR48tQicznYb8MdahrqMnm2pK71WRzmQMzQERERCJhrneBNUYM\ngIiIiETCXHuAGiOeAiMiIiLRYQaIiIhIJBrRtl+zYwBEREQkEqZ8EvS/HUtgREREJDrMABEREYkE\nT4FpMQAiIiISCZ4C02IJjIiIiESHGSAiIiKR4CkwLQZAREREIsESmBZLYERERCQ6zAARERGJBE+B\naTEAIiIiEolq7gESsARGREREosMMEBERkUgw/6PFAIiIiEgkeApMiyUwIiIiEh1mgIiIiESCGSAt\nBkBEREQiwSdBa7EERkRERKLDDBAREZFIsASmxQCIiIhIJPgkaC2WwIiIiEh0mAEiIiISCW6C1mIA\nREREJBLcA6TFEhgRe1IZUQAAB3JJREFUERGJDjNAREREIsESmBYDICIiIpFgCUyLJTAiIiISHWaA\niIiIRILPAdJiAERERCQS1WbaA1RQUIA1a9YgKysLOTk5KC0txZo1a9C7d2+dfk8//TQuX76sNz42\nNhbTpk3TaVMqlZg/fz5+/PFHlJeXw9fXFwkJCfDy8qrXmhgAERER0QN1/vx5JCcnw83NDR4eHjhy\n5Mhd+3p7eyM6OlqnrVu3bjpfV1dXY/z48Th9+jTGjRuHli1b4ptvvkFUVBTS0tLg6upqdE0MgIiI\niETCXCUwb29v/Pnnn2jZsiV++uknTJw48a5927Zti7CwsDrny8zMxJEjR/D5558jODgYADB48GA8\n++yzWLJkCebNm2d0TQyAiIiIRMJcJbBmzZo1qL9KpUJVVRVsbW0NXt++fTucnJwwcOBAoa1Vq1YY\nPHgwtm7dCrVaDYlEUuc9eAqMiIiIGo3du3ejR48e6NGjB4KDg7Fu3Tq9PnK5HN7e3rCwsNBp7969\nO0pKSpCbm2v0PswAERERiYQpS2BKpRJKpVKv3dHREY6Ojvc0Z7du3fDYY4+hU6dOuHHjBtavX4/3\n3nsPRUVFGD9+vNBPoVDg8ccf1xvv5OQEoGbT9SOPPFLnvRgAERERiYQpS2ApKSlYsmSJXvukSZMw\nefLke5pz2bJlOl8PHz4cL7zwAr744guMGTMGDg4OAIDy8nJYWVnpja9tKy8vN3ovBkBERP/X3h2E\nRLmucQB/Tl3T24Qgh7FACFqN4M2sTRARJ1o4iwiCbGUYQXfTqhZhtGkXlIsWIUG0KIg4BIm0cWFu\nIy6DJJErMQiCZriiUNAo6FkcdAg7p5pzOt/o+/stnz6nZ/nned73+4DvNjAwECdPnlxXr3f68yVb\nt26NgYGBuHjxYkxOTsaRI0ciIqKlpSUWFxfXPb9aa2lp+epvC0AAkIi/cwX2V1Zd32PXrl0REbGw\nsLBWy+fzUS6X1z27Wltdhf0ZAQgAEpHVLbC/4u3btxHx+y2vVZ2dnTE5ORkrKyufHYSempqK7du3\nf9N7gNwCAwAyNz8/H8vLy5/VqtVq3Lt3L3K5XPT09KzVi8VilMvlePbs2Vptbm4uxsbG4tixY1+9\nAh9hAgQAycjyW2DDw8MRETEzMxMREaOjo1EqlaK1tTX6+/tjYmIi7ty5E729vdHR0RHz8/MxMjIS\nb968iWvXrkUul1v7rd7e3ujp6YnLly+vvQn60aNHsby8/M0HsH9aWWmcedh/dq6/0gb8WP/79b9f\nfwj4If79y7l/9P/b8/O+v+23Zv//8rueLxQKX6x3dHTExMREvHr1Km7fvh2vX7+Oubm52LZtW3R1\ndcW5c+fi6NGj6/5uYWEhbty4EePj41GtVmPv3r0xODgYXV1d39SPAASJE4AgOykFoEZjBQYAiVjO\ncAXWaAQgAEhEAy19MucWGACQHBMgAEiEFViNAAQAibACq7ECAwCSYwIEAInYiJ/C+FEEIABIRJZv\ngm40VmAAQHJMgAAgEQ5B1whAAJAI1+BrBCAASIQJUI0zQABAckyAACARrsHXCEAAkAgrsBorMAAg\nOSZAAJAIt8BqBCAASIQVWI0VGACQHBMgAEiEW2A1AhAAJMLHUGuswACA5JgAAUAirMBqBCAASIRb\nYDVWYABAckyAACARDkHXCEAAkAgrsBorMAAgOSZAAJAIE6AaAQgAEiH+1Py0Ig4CAIlxBggASI4A\nBAAkRwACAJIjAAEAyRGAAIDkCEAAQHIEIAAgOQIQAJAcAQgASI4ABAAkRwCibouLi3Hz5s04fPhw\ndHd3x+nTp+P58+dZtwWbXrlcjqGhoThz5kzs378/CoVCvHjxIuu2YEMRgKjb4OBg3L9/P06cOBFX\nr16NLVu2xPnz52NycjLr1mBTm52djbt378b79++jUChk3Q5sSD6GSl2mpqair68vrly5EmfPno2I\niGq1GsePH4/29vZ4+PBhtg3CJvbhw4dYWlqKtra2GB8fjwsXLsSDBw/i4MGDWbcGG4YJEHUZGxuL\npqam6OvrW6s1NzfHqVOnolQqRblczrA72Nx27NgRbW1tWbcBG5oARF2mp6djz549kcvlPqt3d3fH\nyspKTE9PZ9QZAHydAERdKpVKtLe3r6vn8/mICBMgABqaAERdPn36FE1NTevqzc3NEfH7eSAAaFQC\nEHVpaWmJpaWldfXV4LMahACgEQlA1CWfz39xzVWpVCIivrgeA4BGIQBRl87OzpidnY2PHz9+Vn/5\n8uXavwNAoxKAqEuxWIylpaV4/PjxWm1xcTGePHkSBw4ciJ07d2bYHQD8uX9l3QAb0759+6JYLMbQ\n0FBUKpXYvXt3jIyMxLt37+L69etZtweb3vDwcEREzMzMRETE6OholEqlaG1tjf7+/ixbgw3Bm6Cp\nW7VajVu3bsXTp09jYWEhCoVCXLp0KQ4dOpR1a7Dp/dEnMDo6OmJiYuIf7gY2HgEIAEiOM0AAQHIE\nIAAgOQIQAJAcAQgASI4ABAAkRwACAJIjAAEAyRGAAIDkCEAAQHIEIAAgOb8BP+mk/daManEAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KceMm2E-0dF1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We can't look at accuracy with confidence. Indeed, our sample is really unbalanced and thus classifying all text as male would already give a 0.75 accuracy. This is exactly what happens here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PviHO7vHm65o",
        "colab_type": "code",
        "outputId": "00032d11-3495-4d1b-b6b8-286956797b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Valid F1_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.604016</td>\n",
              "      <td>0.519108</td>\n",
              "      <td>0.781348</td>\n",
              "      <td>0.766779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.464184</td>\n",
              "      <td>0.637196</td>\n",
              "      <td>0.793887</td>\n",
              "      <td>0.785306</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Valid F1_score\n",
              "epoch                                                           \n",
              "1           0.604016     0.519108       0.781348        0.766779\n",
              "2           0.464184     0.637196       0.793887        0.785306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJY9ZzP2JwK",
        "colab_type": "code",
        "outputId": "5a63a311-8ea1-482f-9152-7ce0c46b723d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwAAAAGaCAYAAAC44ySCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQERERESkR1gAEBERERHpkf8Prd4yqh7PaUEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCuM7GlhsZyg",
        "colab_type": "text"
      },
      "source": [
        "Our model is completely over fitting after 2 epochs !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXyz5wUidkY",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnQ2WrXicsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f0e97454-d670-4cc4-838f-580d789034f9"
      },
      "source": [
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df_dev.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "texts = df_dev.Texte.values\n",
        "labels = df_dev.sexe.values\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "# Apply function to our corpus\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                   )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        " \n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 8  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 130\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXSH_9xVjA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "990416d3-f141-44b1-a9a2-802de94403ce"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "gender_model1.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = gender_model1(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  #predictions.append(logits)\n",
        "  #true_labels.append(label_ids)\n",
        "  val_batch_preds = np.argmax(logits, axis=1)\n",
        "  val_batch_labels = label_ids\n",
        "  predictions.extend(val_batch_preds)\n",
        "  true_labels.extend(val_batch_labels)\n",
        "\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 130 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhpJRYTPjJ-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP9p0dsbptCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_tags = [i for i in predictions]\n",
        "true_tags = [i for i in true_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrjnt2WUqaDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZW4IKLjjMMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c312cc06-e0f6-401a-8eb8-f18f512ee92b"
      },
      "source": [
        "f1_score(true_tags,pred_tags)"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.732824427480916"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4crdydWqx6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5a00999-fa0f-41be-af6a-9ef02a00179f"
      },
      "source": [
        "accuracy_score(true_tags, pred_tags)"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7307692307692307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrKoHdGWrXtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_true_pred=pd.DataFrame([true_tags,pred_tags]).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBpl6xQtrvQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_true_pred.columns=['true_tags','pred_tags']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8G2vJn0ryWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56a10572-5330-48dd-f5d6-9f30503368a3"
      },
      "source": [
        "df_true_pred[df_true_pred['true_tags']!=df_true_pred['pred_tags']]"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>true_tags</th>\n",
              "      <th>pred_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     true_tags  pred_tags\n",
              "2            1          0\n",
              "12           0          1\n",
              "16           1          0\n",
              "25           1          0\n",
              "28           0          1\n",
              "30           0          1\n",
              "32           1          0\n",
              "33           1          0\n",
              "35           1          0\n",
              "37           0          1\n",
              "39           1          0\n",
              "44           1          0\n",
              "45           0          1\n",
              "46           1          0\n",
              "49           1          0\n",
              "53           1          0\n",
              "60           0          1\n",
              "62           0          1\n",
              "65           1          0\n",
              "72           1          0\n",
              "79           1          0\n",
              "86           1          0\n",
              "87           1          0\n",
              "94           1          0\n",
              "95           1          0\n",
              "96           1          0\n",
              "97           1          0\n",
              "115          1          0\n",
              "116          0          1\n",
              "117          0          1\n",
              "118          0          1\n",
              "122          0          1\n",
              "125          1          0\n",
              "128          1          0\n",
              "129          1          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGOO8vm3jwL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdmReVji0KF5",
        "colab_type": "text"
      },
      "source": [
        "RESTE A FAIRE A VOIR AVEC MELCHIOR \n",
        "A discuter : \n",
        "- Texte preprocessing ? ENlever monsieur, madame ? \n",
        "- quel autre modèle réaliser (type de texte qu'on prend...etc) \n",
        "- optimizer\n",
        "- number of batch / sample / epochs (low) \n",
        "- change criterion \n",
        "- More analysis ? what to add ? \n",
        "\n",
        "A demander : \n",
        "- Comment je split le texte en plusieurs \n",
        "\n",
        "\n",
        "RESTE A FAIRE MORGANE : \n",
        "- improve loop ? \n",
        "- Analyse dernière partie \n",
        "- Analyse test set (which one were not well predicted)\n",
        "- Enregistrer les modèles ? (A voir pour montrer les résultats un a un avec les différentes données de base A DISCUTER) \n",
        "\n",
        "\n",
        "TRUC QUE J'AIMERAIS FAIRE : \n",
        "Analyser une phrase avec le score. \n",
        "\n",
        "INTERVIEW TEST ???? Si base de données déjà prête ca run sur ce code sans soucis \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2heoobpm69S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5oSWe6nm63Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqcRkgeSm60n",
        "colab_type": "code",
        "outputId": "103fcb28-f464-4e65-8b5f-41bb710cac79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "@article{Wolf2019HuggingFacesTS,\n",
        "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
        "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
        "  journal={ArXiv},\n",
        "  year={2019},\n",
        "  volume={abs/1910.03771}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-c848a20e4352>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    @article{Wolf2019HuggingFacesTS,\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2xLw0mY6tzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}