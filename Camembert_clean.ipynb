{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP9Z3Df+PSCDwOWs0ginwCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerezamo/NLP_brouillon/blob/master/Camembert_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# BERT classification model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "7615f14e-23b6-41bd-a13c-1ac21470d2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "43e349d4-f4e6-4121-b594-da5eff773ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "d7792c6c-d3be-464e-eda9-832c4fa00840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "7ee37fc9-4753-4674-8022-915da2e5d66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Import medium_df_desq in \"files\"\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0FNej5emFx8",
        "colab_type": "text"
      },
      "source": [
        "**CHOOSE ONE OF THE OPTIONS BELOW**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRDpDKe9_j-",
        "colab_type": "text"
      },
      "source": [
        "1. Unbalanced sample \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Gl1QBWAlTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrqgCPYFApFQ",
        "colab_type": "text"
      },
      "source": [
        "2. Balanced sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOIKxWaXHPVB",
        "colab_type": "code",
        "outputId": "8d539235-2a32-48b2-e53e-10fff8b678c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=1).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mesdames, Messieurs,Après avoir salué une nouv...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M. le président. L'ordre du jour appelle le dé...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Madame et messieurs les ministres,Mesdames, me...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M. le président. L'ordre du jour appelle le dé...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Monsieur le maire, mon cher Gilbert,Monsieur ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  Mesdames, Messieurs,Après avoir salué une nouv...     0\n",
              "1  M. le président. L'ordre du jour appelle le dé...     0\n",
              "2  Madame et messieurs les ministres,Mesdames, me...     1\n",
              "3  M. le président. L'ordre du jour appelle le dé...     1\n",
              "4   Monsieur le maire, mon cher Gilbert,Monsieur ...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNHgnNbAsoq",
        "colab_type": "text"
      },
      "source": [
        "3. Spliting texts in order to feed the model with all parts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JmisLKH2L8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "11b0599d-bbe6-4c59-cae9-94152d2dec12"
      },
      "source": [
        "from itertools import repeat\n",
        "n=2000\n",
        "chunks, label_split=[],[]\n",
        "j=0\n",
        "for text in df.Texte :\n",
        "    txt=[text[i:i+n] for i in range(0, len(text), n)]\n",
        "    chunks.append(txt)\n",
        "    label_split.extend(repeat(df.sexe[j], len(txt)))\n",
        "    j+=1\n",
        "\n",
        "chunks = [item for sublist in chunks for item in sublist]\n",
        "df=pd.DataFrame([chunks,label_split]).transpose()\n",
        "df.columns=['Texte','sexe']\n",
        "len(df)\n",
        "\n",
        "# Let's take a balanced sample (model classifying all in men otherwise)\n",
        "df_m = df.loc[df['sexe'] == 0]\n",
        "df_f = df.loc[df['sexe'] == 1] \n",
        "df_m = df_m[0:len(df_f)]\n",
        "df = df_f.append(df_m)\n",
        "\n",
        "#Shuffle the data \n",
        "df=df.sample(frac=0.5).reset_index()\n",
        "\n",
        "# Reduce to the variables we are interested in \n",
        "df=df[['Texte','sexe']]\n",
        "\n",
        "# Put as integer \n",
        "df['sexe'] = df['sexe'].astype(int)\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this corpus : {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "# In this case we will cut the sample \n"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 7,981\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Plus largement, l'éducation à la vie suppose d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qui restent aux communes, les taxes foncières ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>omme je l'évoquais en parlant de la cible que ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>raison pour laquelle je vous propose d'ores e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>romeuvent la langue française, la culture fran...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texte  sexe\n",
              "0  Plus largement, l'éducation à la vie suppose d...     1\n",
              "1  qui restent aux communes, les taxes foncières ...     1\n",
              "2  omme je l'évoquais en parlant de la cible que ...     0\n",
              "3   raison pour laquelle je vous propose d'ores e...     0\n",
              "4  romeuvent la langue française, la culture fran...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDafeOtBg9T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "model, dev = train_test_split(df, test_size=0.02)\n",
        "\n",
        "df= model\n",
        "df_dev= dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXJZbQHl9Is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4eaf114-65f4-4145-f5f8-5323322929a8"
      },
      "source": [
        "len(df_dev)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "160"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbgs39TYNqRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08yLt5edOhL3",
        "colab_type": "code",
        "outputId": "c1549880-ab5a-45d6-fbee-d7c0d78aec17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Plus largement, l'éducation à la vie suppose d'aborder l'ensemble des questions liées à la santé des jeunes filles. J'aime à parler, en cette matière, d'un \" capital santé \" des femmes qu'il importe de ne pas gaspiller et cela depuis le plus jeune âge. Cela implique une éducation au sens des responsabilités, qui doit faire partie des messages de notre école, aujourd'hui. La prévention dans le domaine de la santé des femmes est un élément essentiel de la mission que m'ont confiée le Président de la République et la Premier Ministre et j'entends m'investir dans cette tâche. Et puis, comment ne pas être émus par toutes les violences émergentes dans certains quartiers, notamment les violences sexuelles, qui témoignent avant tout d'une absence de respect de l'autre. Les violences chez les jeunes sont souvent liées à une sexualité mal comprise, instrument de domination des garçons sur les filles. Il convient donc de réadapter le discours, tant pour les filles que pour les garçons, et peut-être aussi de réorienter ses vecteurs pour atteindre toutes les cibles de publics mieux identifiés. Ces violences qui ont cours surtout dans les quartiers difficiles, à l'école ou en dehors de l'école, posent le second problème qu'il me paraît essentiel d'aborder à présent dans vos groupes de travail, il s'agit de l'accès à cette information pour les personnes issues de l'immigration. Je souhaiterais donc que l'on s'attache à étudier l'accès à l'information sexuelle des femmes issues de l'immigration qui peuvent se trouver éloignées des circuits habituels. Les femmes sont malheureusement inégales, face à l'information à la sexualité. Or, une politique d'intégration réussie ne peut faire l'économie de l'information sur la prévention sanitaire. Au-delà d'une certaine connaissance, il s'agit de conduire une pédagogie de la responsabilité qui commence par le respect de sa propre personne et la maîtrise de son corps et de sa sexualité. Dans cette approche, il est capital que toutes les femmes\n",
            "Tokenized:  ['▁Plus', '▁largement', ',', '▁l', \"'\", 'éducation', '▁à', '▁la', '▁vie', '▁suppose', '▁d', \"'\", 'aborder', '▁l', \"'\", 'ensemble', '▁des', '▁questions', '▁liées', '▁à', '▁la', '▁santé', '▁des', '▁jeunes', '▁filles', '.', '▁J', \"'\", 'aime', '▁à', '▁parler', ',', '▁en', '▁cette', '▁matière', ',', '▁d', \"'\", 'un', '▁\"', '▁capital', '▁santé', '▁\"', '▁des', '▁femmes', '▁qu', \"'\", 'il', '▁importe', '▁de', '▁ne', '▁pas', '▁gaspille', 'r', '▁et', '▁cela', '▁depuis', '▁le', '▁plus', '▁jeune', '▁âge', '.', '▁Cela', '▁implique', '▁une', '▁éducation', '▁au', '▁sens', '▁des', '▁responsabilités', ',', '▁qui', '▁doit', '▁faire', '▁partie', '▁des', '▁messages', '▁de', '▁notre', '▁école', ',', '▁aujourd', \"'\", 'hui', '.', '▁La', '▁prévention', '▁dans', '▁le', '▁domaine', '▁de', '▁la', '▁santé', '▁des', '▁femmes', '▁est', '▁un', '▁élément', '▁essentiel', '▁de', '▁la', '▁mission', '▁que', '▁m', \"'\", 'ont', '▁confiée', '▁le', '▁Président', '▁de', '▁la', '▁République', '▁et', '▁la', '▁Premier', '▁Ministre', '▁et', '▁j', \"'\", 'entends', '▁m', \"'\", 'investir', '▁dans', '▁cette', '▁tâche', '.', '▁Et', '▁puis', ',', '▁comment', '▁ne', '▁pas', '▁être', '▁ému', 's', '▁par', '▁toutes', '▁les', '▁violences', '▁émergent', 'es', '▁dans', '▁certains', '▁quartiers', ',', '▁notamment', '▁les', '▁violences', '▁sexuelles', ',', '▁qui', '▁témoignent', '▁avant', '▁tout', '▁d', \"'\", 'une', '▁absence', '▁de', '▁respect', '▁de', '▁l', \"'\", 'autre', '.', '▁Les', '▁violences', '▁chez', '▁les', '▁jeunes', '▁sont', '▁souvent', '▁liées', '▁à', '▁une', '▁sexualité', '▁mal', '▁comprise', ',', '▁instrument', '▁de', '▁domination', '▁des', '▁garçons', '▁sur', '▁les', '▁filles', '.', '▁Il', '▁convient', '▁donc', '▁de', '▁ré', 'adapter', '▁le', '▁discours', ',', '▁tant', '▁pour', '▁les', '▁filles', '▁que', '▁pour', '▁les', '▁garçons', ',', '▁et', '▁peut', '-', 'être', '▁aussi', '▁de', '▁ré', 'orienter', '▁ses', '▁vecteur', 's', '▁pour', '▁atteindre', '▁toutes', '▁les', '▁cibles', '▁de', '▁publics', '▁mieux', '▁identifiés', '.', '▁Ces', '▁violences', '▁qui', '▁ont', '▁cours', '▁surtout', '▁dans', '▁les', '▁quartiers', '▁difficiles', ',', '▁à', '▁l', \"'\", 'école', '▁ou', '▁en', '▁dehors', '▁de', '▁l', \"'\", 'école', ',', '▁posent', '▁le', '▁second', '▁problème', '▁qu', \"'\", 'il', '▁me', '▁paraît', '▁essentiel', '▁d', \"'\", 'aborder', '▁à', '▁présent', '▁dans', '▁vos', '▁groupes', '▁de', '▁travail', ',', '▁il', '▁s', \"'\", 'agit', '▁de', '▁l', \"'\", 'accès', '▁à', '▁cette', '▁information', '▁pour', '▁les', '▁personnes', '▁issues', '▁de', '▁l', \"'\", 'immigration', '.', '▁Je', '▁souhaiterais', '▁donc', '▁que', '▁l', \"'\", 'on', '▁s', \"'\", 'attache', '▁à', '▁étudier', '▁l', \"'\", 'accès', '▁à', '▁l', \"'\", 'information', '▁sexuelle', '▁des', '▁femmes', '▁issues', '▁de', '▁l', \"'\", 'immigration', '▁qui', '▁peuvent', '▁se', '▁trouver', '▁éloignée', 's', '▁des', '▁circuits', '▁habituels', '.', '▁Les', '▁femmes', '▁sont', '▁malheureusement', '▁inégale', 's', ',', '▁face', '▁à', '▁l', \"'\", 'information', '▁à', '▁la', '▁sexualité', '.', '▁Or', ',', '▁une', '▁politique', '▁d', \"'\", 'intégration', '▁réussie', '▁ne', '▁peut', '▁faire', '▁l', \"'\", 'économie', '▁de', '▁l', \"'\", 'information', '▁sur', '▁la', '▁prévention', '▁sanitaire', '.', '▁Au', '-', 'delà', '▁d', \"'\", 'une', '▁certaine', '▁connaissance', ',', '▁il', '▁s', \"'\", 'agit', '▁de', '▁conduire', '▁une', '▁pédagogie', '▁de', '▁la', '▁responsabilité', '▁qui', '▁commence', '▁par', '▁le', '▁respect', '▁de', '▁sa', '▁propre', '▁personne', '▁et', '▁la', '▁maîtrise', '▁de', '▁son', '▁corps', '▁et', '▁de', '▁sa', '▁sexualité', '.', '▁Dans', '▁cette', '▁approche', ',', '▁il', '▁est', '▁capital', '▁que', '▁toutes', '▁les', '▁femmes']\n",
            "Token IDs:  [869, 2170, 7, 17, 11, 1997, 15, 13, 157, 6531, 18, 11, 14204, 17, 11, 649, 20, 756, 3496, 15, 13, 561, 20, 538, 1134, 9, 121, 11, 660, 15, 639, 7, 22, 78, 763, 7, 18, 11, 59, 87, 2414, 561, 87, 20, 389, 46, 11, 62, 3422, 8, 45, 34, 31083, 81, 14, 207, 176, 16, 40, 393, 2018, 9, 683, 5096, 28, 6785, 36, 437, 20, 7217, 7, 31, 279, 85, 245, 20, 2486, 8, 127, 2391, 7, 405, 11, 265, 9, 61, 3530, 29, 16, 813, 8, 13, 561, 20, 389, 30, 23, 3228, 4095, 8, 13, 1308, 27, 115, 11, 263, 14517, 16, 1850, 8, 13, 1547, 14, 13, 2616, 6375, 14, 76, 11, 10923, 115, 11, 10201, 29, 78, 3736, 9, 139, 264, 7, 404, 45, 34, 98, 12504, 10, 37, 208, 19, 7090, 11931, 80, 29, 420, 4459, 7, 410, 19, 7090, 5677, 7, 31, 14525, 178, 66, 18, 11, 70, 5573, 8, 1346, 8, 17, 11, 369, 9, 74, 7090, 222, 19, 538, 56, 355, 3496, 15, 28, 9459, 295, 13060, 7, 5011, 8, 8710, 20, 5024, 32, 19, 1134, 9, 69, 2480, 145, 8, 425, 4773, 16, 2081, 7, 376, 24, 19, 1134, 27, 24, 19, 5024, 7, 14, 104, 26, 177, 99, 8, 425, 15993, 89, 13656, 10, 24, 2463, 208, 19, 13619, 8, 1825, 334, 18493, 9, 515, 7090, 31, 96, 307, 381, 29, 19, 4459, 4610, 7, 15, 17, 11, 1025, 47, 22, 2058, 8, 17, 11, 1025, 7, 11618, 16, 1552, 577, 46, 11, 62, 103, 3944, 4095, 18, 11, 14204, 15, 689, 29, 140, 1484, 8, 225, 7, 51, 52, 11, 567, 8, 17, 11, 1288, 15, 78, 2322, 24, 19, 242, 8188, 8, 17, 11, 7529, 9, 100, 20642, 145, 27, 17, 11, 88, 52, 11, 8554, 15, 7147, 17, 11, 1288, 15, 17, 11, 1070, 4128, 20, 389, 8188, 8, 17, 11, 7529, 31, 316, 48, 356, 16391, 10, 20, 9298, 18356, 9, 74, 389, 56, 3125, 30121, 10, 7, 461, 15, 17, 11, 1070, 15, 13, 9459, 9, 1051, 7, 28, 462, 18, 11, 4378, 9713, 45, 104, 85, 17, 11, 1803, 8, 17, 11, 1070, 32, 13, 3530, 6901, 9, 277, 26, 1942, 18, 11, 70, 1761, 1267, 7, 51, 52, 11, 567, 8, 4013, 28, 11369, 8, 13, 1717, 31, 1246, 37, 16, 1346, 8, 77, 627, 314, 14, 13, 3135, 8, 58, 486, 14, 8, 77, 9459, 9, 211, 78, 2312, 7, 51, 30, 2414, 27, 208, 19, 389]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = df.Texte.values\n",
        "labels = df.sexe.values\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "# Apply function to our corpus\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                   )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        " \n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "outputId": "07525cdc-acc7-4dc6-ab4d-95ac010ee7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Original text and transformed tensor print \n",
        "print('Original: ', texts[0])\n",
        "print('IDs:', input_ids[0])"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ecteur hospitalier la situation n'est pas meilleure,On assiste dans de trop nombreux services à une certaine dégradation des conditions de travail,Les patients et les professionnels souffrent de l'insuffisant renouvellement des équipements,ainsi que d'un manque d'effectifs soignants. La RTT a fortement aggravé une situation déjà très difficile et les congés estivaux sont aujourd'hui, je le sais, une menace sur la permanence du service public hospitalier,la situation financière des établissements est tendue, ce qui crée reports de charges et déficits,l'absence de financement suffisant pour la RTT ou le protocole \"filières professionnelles\" constitue une menace sur les droits sociaux des personnelsEn un mot, j'ai trouvé à mon arrivée un monde de la santé qui traverse une grave crise matérielle et morale s'exprimant par des revendications nombreuses et par une profonde exaspération. Le découragement et la démotivation guettent des personnels traditionnellement habités par l'enthousiasme de servir.Je voudrais terminer ce constat assez sombre, j'en conviens, en vous faisant part de mes réflexions sur la situation financière du régime général. La situation des comptes dont j'hérite est préoccupante.La situation financière du régime général est, c'est indéniable, dégradée. Les rapporteurs de l'audit sur les finances publiques l'avaient constaté sans ambages: le déficit du régime général atteindrait 2,4 milliards d'euros en 2002. C'est le premier déficit constaté du régime général après trois années d'excédents. Nous retrouvons les niveaux de déficit de 1998 et, de façon plus éloignée dans le temps, de 1991 et 1992.On est surtout bien loin des excédents prévisionnels affichés par le gouvernement précédent lors de la commission des comptes de septembre 2001. Les prévisions étaient alors bien peu réalistes. Toutefois, nous sommes face à une situation assez contrastée des différentes branches.Ainsi, la branche maladie est clairement dans le rouge, avec un déficit prévisionnel \n",
            "IDs: tensor([    5,   599,  8444, 11589,    13,   595,    49,    11,    41,    34,\n",
            "         1058,     7,  3317, 12147,    29,     8,   237,   490,   440,    15,\n",
            "           28,  1761, 10681,    20,   643,     8,   225,     7,  1607,  2205,\n",
            "           14,    19,   941, 13064,     8,    17,    11,  1210,  3348,  3230,\n",
            "         7847,    20,  2891,     7, 20602,    27,    18,    11,    59,   971,\n",
            "           18,    11, 17108,    10, 16843,    10,     9,    61,   345,  7965,\n",
            "           33,  2712, 28239,    28,   595,   235,    95,   863,    14,    19,\n",
            "        14516,    30,  3089,   483,    56,   405,    11,   265,     7,    50,\n",
            "           16,   555,     7,    28,  3456,    32,    13,  5184,    25,   366,\n",
            "          525, 11589,     7,   321,   595,  2903,    20,  2743,    30, 16651,\n",
            "            7,    44,    31,  2750, 10884,    10,     8,  3739,    14,  7394,\n",
            "           10,     7,   219,    11,  2593,     8,  2483,  6158,    24,    13,\n",
            "          345,  7965,    47,    16,  5996,    87,  6825,  3660,  1050,    10,\n",
            "          130,  2176,    28,  3456,    32,    19,   873,  1148,    20,  3863,\n",
            "         1855,    23,   853,     7,    76,    11,    73,   964,    15,   129,\n",
            "         2363,    23,   164,     8,    13,   561,    31,  7801,    28,  2252,\n",
            "         1662, 16262,    14,  4382,    52,    11, 22342,   113,    37,    20,\n",
            "        14360,   720,    14,    37,    28,  4440,  1017, 10491,  6944,     9,\n",
            "           54, 23632,   131,    14,    13,   570, 19932,   472,    21, 15353,\n",
            "          113,    20,  3863,  3416,   131, 19146,    10,    37,    17,    11,\n",
            "        12935,     8,  1950,     9,  1684,  4318,  4385,    44,  4683,   424,\n",
            "         3824,     7,    76,    11,    90,  1113,  9348,     7,    22,    39,\n",
            "         1208,   292,     8,   249,  9213,    32,    13,   595,  2903,    25,\n",
            "         1540,   606,     9,    61,   595,    20,  2938,   174,    76,    11,\n",
            "        16352,   487,    30,   790, 11574,    35,     9,  1003,   595,  2903,\n",
            "           25,  1540,   606,    30,     7,    60,    11,    41, 13422,     7,\n",
            "        14589,    35,     9,    74, 18204,    10,     8,    17,    11, 18793,\n",
            "           32,    19,  5779,  2761,    17,    11,  4383,  6653,   112,    21,\n",
            "         6188,  3169,    92,    16,  7394,    25,  1540,   606,    33,   880,\n",
            "         3778,  1176, 27872,  1885,    18,    11,  1958,    22,  5162,     9,\n",
            "           84,    11,    41,    16,   246,  7394,  6653,    25,  1540,   606,\n",
            "          182,   303,   318,    18,    11, 20122,    10,     9,   170, 18678,\n",
            "           19,  2336,     8,  7394,     8,  7416,    14,     7,     8,   429,\n",
            "           40, 16391,    29,    16,   125,     7,     8,  8667,    14,  7741,\n",
            "            9,  3317,    30,   381,    72,   583,    20,    21, 20122,    10,\n",
            "        24737,    10, 19510,    37,    16,   754,  2501,   298,     8,    13,\n",
            "         2585,    20,  2938,     8,   652,  5268,     9,    74,  8059,   530,\n",
            "          183,    72,   126,  7632,    10,     9,  2972,     7,    63,   464,\n",
            "          461,    15,    28,   595,   424, 24263,    35,    20,   678,  7983,\n",
            "            9,   243,  1210,   233,     7,    13,  5563,  1596,    30,  2449,\n",
            "           29,    16,   934,     7,    42,    23,  7394, 24737,     6,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OPWOx2-FG7g",
        "colab_type": "code",
        "outputId": "cb9c76db-980a-4033-ff99-e92743fff542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFc6MW5df566",
        "colab_type": "text"
      },
      "source": [
        "#### Train and validation dataset construction \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y3MT9B-gAHU",
        "colab_type": "code",
        "outputId": "bfe4d898-8d1f-4f2e-8f42-868bd0e2137f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine all above\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Let's create a 80-20 train / validation dataset \n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('We have {} training samples'.format(train_size))\n",
        "print('We have {} validation samples'.format(val_size))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 6256 training samples\n",
            "We have 1565 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF9rj3AWgynY",
        "colab_type": "text"
      },
      "source": [
        "In order to save on memory we use the convenient DataLoader of pytorch.utils "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr-fhDIQgAFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# We set the size of the batch lower than what is usually set (16 of 32)\n",
        "batch_size = 4\n",
        "\n",
        "# We create data loaders for the train and validation dataset. \n",
        "train_dataloader = DataLoader(\n",
        "            train_set,  # The training samples.\n",
        "            sampler = RandomSampler(train_set), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "            val_set, # The validation samples.\n",
        "            sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poTTEJX1hoUK",
        "colab_type": "text"
      },
      "source": [
        "### CamemBERT Sequence Classification model tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VeJI0lDwf",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MPOVB7iRcl",
        "colab_type": "text"
      },
      "source": [
        "We will finally build up our model. We will use the  CamemBERT model for sequence classification which includes a special top layer designed for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHhHzjKgAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing from transformers\n",
        "from transformers import CamembertForSequenceClassification, CamembertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMdM-QqgAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "# Ici je ne suis pas sure pour le 'cased' ou pas (je crois que oui)\n",
        "gender_model1 = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKynoykf_9y",
        "colab_type": "code",
        "outputId": "452dfb0f-1996-4410-9fe1-b9de3d00f0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We run the model on the colab GPU \n",
        "gender_model1.cuda()"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWyHWg5xlBck",
        "colab_type": "text"
      },
      "source": [
        "Optimizers and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go-uY70Mloqv",
        "colab_type": "text"
      },
      "source": [
        "We will choose the AdamW optimizer and set for this first model the learning rate and the epsilon to default. At the batch is little we might want to increase the learning rate a bit from what is usually used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyP0__vf_7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#Implements Adam algorithm with weight decay fix.\n",
        "optimizer = AdamW(gender_model1.parameters(),\n",
        "                  lr = 5e-5, # Adaptative (yes i think)\n",
        "                  eps = 1e-8 # prevent division by 0 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYr-iTzVmXZu",
        "colab_type": "text"
      },
      "source": [
        "We fiw the number of epochs to 4\n",
        "We also configure the learning rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ1ymlmqf_4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# set number of epochs\n",
        "epochs = 2\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgwmHirxnEie",
        "colab_type": "text"
      },
      "source": [
        "### Constructing the training and validation loop \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6rIz4UnLwq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDt2ZElwcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZt12wxqh7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQxSQsQxm6mv",
        "colab_type": "code",
        "outputId": "5ee3b7ae-523a-4e96-f687-87a8cbcb2505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# https://github.com/huggingface/transformers \n",
        "# https://github.com/chambliss/Multilingual_NER/blob/master/python/utils/main_utils.py#L404  \n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
        "\n",
        "import random\n",
        "# Let's put a seed to make this result reproducible \n",
        "seed=2020\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# We want to evaluate the training phase \n",
        "training_stats = []\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "  print('Training starts')\n",
        "\n",
        "  \n",
        "\n",
        "  ################################### TRAINING ################################\n",
        "\n",
        "  # Set the train loss for the epoch to 0 \n",
        "  total_train_loss = 0\n",
        "\n",
        "  #Put the model in training \n",
        "  gender_model1.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Cpy the 3 batch to GPU \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    # Clear gradients \n",
        "    gender_model1.zero_grad() \n",
        "    \n",
        "    #return loss and logits\n",
        "    loss, logits = gender_model1(b_input_ids, \n",
        "                         token_type_ids=None, \n",
        "                         attention_mask=b_input_mask, \n",
        "                         labels=b_labels) \n",
        "    \n",
        "    # Accumulate training loss for all batches \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Backward to calculate gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Prevent exploding gradients problem \n",
        "    torch.nn.utils.clip_grad_norm_(gender_model1.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters \n",
        "    optimizer.step()\n",
        "\n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "  #Calculate the average training loss over all batches  \n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "  print('')\n",
        "  print('And now, validation STARTS')\n",
        "\n",
        "  ###################### VALIDATION #############################\n",
        "\n",
        "  # Put model in evaluation mode \n",
        "  gender_model1.eval()\n",
        "\n",
        "  # Set statistics to 0\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  nb_eval_steps = 0\n",
        "\n",
        "  # Confusion matrix ?\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "     \n",
        "     # We don't care about gradients for eval\n",
        "    with torch.no_grad(): \n",
        "      (loss, logits) = gender_model1(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "    total_eval_loss += loss.item()\n",
        "\n",
        "      # Move logits and labels to CPU \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Confusion matrix ?\n",
        "    val_batch_preds = np.argmax(logits, axis=1)\n",
        "    val_batch_labels = label_ids\n",
        "    predictions.extend(val_batch_preds)\n",
        "    true_labels.extend(val_batch_labels)\n",
        "\n",
        "    # Accumulation accuracy for all batch\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    #Final accuracy on all batch\n",
        "  avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
        "  print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #Final loss over all batch\n",
        "  avg_val_loss = total_eval_loss / len(val_dataloader)\n",
        "  print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "  # confusion matrix ? \n",
        "  pred_tags = [i for i in predictions]\n",
        "  valid_tags = [i for i in true_labels]\n",
        "\n",
        "  # f1 score \n",
        "  F1_score_val = f1_score(valid_tags,pred_tags)\n",
        "\n",
        "\n",
        "  training_stats.append(\n",
        "        {\n",
        "            'epoch': ep + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Valid F1_score' : F1_score_val\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Done !\")"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 2 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.58\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.49\n",
            "===========Starting Epoch 2 / 2 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.48\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.57\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqfBkNDKVxW",
        "colab_type": "code",
        "outputId": "defafb3b-e9ea-43d3-f3ea-47ee61c67dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Confusion matrix sur le dernier epoch (à insérer pour avoir les trois ? )\n",
        "confusion_matrix(valid_tags, pred_tags)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[621, 142],\n",
              "       [155, 647]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKUBFeBAJTOK",
        "colab_type": "code",
        "outputId": "faefc344-4c09-4c3f-e1df-ddbadc40cccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_mat = confusion_matrix(valid_tags, pred_tags)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat)\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.show()"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGmCAYAAACUbzs0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KceMm2E-0dF1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We can't look at accuracy with confidence. Indeed, our sample is really unbalanced and thus classifying all text as male would already give a 0.75 accuracy. This is exactly what happens here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PviHO7vHm65o",
        "colab_type": "code",
        "outputId": "27f83f38-3413-4751-e751-96cc7a4fc34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Valid F1_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.578801</td>\n",
              "      <td>0.490902</td>\n",
              "      <td>0.774872</td>\n",
              "      <td>0.783303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.478315</td>\n",
              "      <td>0.566289</td>\n",
              "      <td>0.810587</td>\n",
              "      <td>0.813325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Valid F1_score\n",
              "epoch                                                           \n",
              "1           0.578801     0.490902       0.774872        0.783303\n",
              "2           0.478315     0.566289       0.810587        0.813325"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJY9ZzP2JwK",
        "colab_type": "code",
        "outputId": "a27ed377-08e4-457f-f8bf-4429c3d33820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvUAAAGaCAYAAACPCLyfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCuM7GlhsZyg",
        "colab_type": "text"
      },
      "source": [
        "Our model is completely over fitting after 2 epochs !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXyz5wUidkY",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnQ2WrXicsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a86461ae-6ff6-4f34-9081-9f7668051867"
      },
      "source": [
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df_dev.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "texts = df_dev.Texte.values\n",
        "labels = df_dev.sexe.values\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "# Apply function to our corpus\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                   )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        " \n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 8  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXSH_9xVjA6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b2c6e05-ad83-4e9f-9f0e-3a09022017c7"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "gender_model1.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = gender_model1(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  #predictions.append(logits)\n",
        "  #true_labels.append(label_ids)\n",
        "  val_batch_preds = np.argmax(logits, axis=1)\n",
        "  val_batch_labels = label_ids\n",
        "  predictions.extend(val_batch_preds)\n",
        "  true_labels.extend(val_batch_labels)\n",
        "\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 160 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhpJRYTPjJ-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP9p0dsbptCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_tags = [i for i in predictions]\n",
        "true_tags = [i for i in true_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrjnt2WUqaDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZW4IKLjjMMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72c1be10-d616-4210-ca3f-dbd4b03eeb21"
      },
      "source": [
        "f1_score(true_tags,pred_tags)"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8251748251748251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4crdydWqx6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c17b9a5e-6029-4171-a3fb-383e3aeeba99"
      },
      "source": [
        "accuracy_score(true_tags, pred_tags)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrKoHdGWrXtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_true_pred=pd.DataFrame([true_tags,pred_tags]).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBpl6xQtrvQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_true_pred.columns=['true_tags','pred_tags']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8G2vJn0ryWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "0d67bc5a-5eac-4b64-db20-c8b236767feb"
      },
      "source": [
        "df_true_pred[df_true_pred['true_tags']!=df_true_pred['pred_tags']]"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>true_tags</th>\n",
              "      <th>pred_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     true_tags  pred_tags\n",
              "26           0          1\n",
              "31           0          1\n",
              "39           0          1\n",
              "46           1          0\n",
              "50           1          0\n",
              "55           1          0\n",
              "59           1          0\n",
              "63           0          1\n",
              "65           1          0\n",
              "75           0          1\n",
              "78           0          1\n",
              "82           1          0\n",
              "87           1          0\n",
              "89           1          0\n",
              "91           1          0\n",
              "92           1          0\n",
              "103          1          0\n",
              "104          1          0\n",
              "106          0          1\n",
              "107          0          1\n",
              "119          1          0\n",
              "120          1          0\n",
              "129          1          0\n",
              "150          0          1\n",
              "159          1          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcGOO8vm3jwL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdmReVji0KF5",
        "colab_type": "text"
      },
      "source": [
        "RESTE A FAIRE A VOIR AVEC MELCHIOR \n",
        "A discuter : \n",
        "- Texte preprocessing ? ENlever monsieur, madame ? \n",
        "- quel autre modèle réaliser (type de texte qu'on prend...etc) \n",
        "- optimizer\n",
        "- number of batch / sample / epochs (low) \n",
        "- change criterion \n",
        "- More analysis ? what to add ? \n",
        "\n",
        "A demander : \n",
        "- Comment je split le texte en plusieurs \n",
        "\n",
        "\n",
        "RESTE A FAIRE MORGANE : \n",
        "- improve loop ? \n",
        "- Analyse test set (which one were not well predicted)\n",
        "- Enregistrer les modèles ? (A voir pour montrer les résultats un a un avec les différentes données de base A DISCUTER) \n",
        "\n",
        "TRUC QUE J'AIMERAIS FAIRE : \n",
        "Analyser une phrase avec le score. \n",
        "\n",
        "INTERVIEW TEST ???? Si base de données déjà prête ca run sur ce code sans soucis \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2heoobpm69S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5oSWe6nm63Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqcRkgeSm60n",
        "colab_type": "code",
        "outputId": "103fcb28-f464-4e65-8b5f-41bb710cac79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "@article{Wolf2019HuggingFacesTS,\n",
        "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
        "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
        "  journal={ArXiv},\n",
        "  year={2019},\n",
        "  volume={abs/1910.03771}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-c848a20e4352>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    @article{Wolf2019HuggingFacesTS,\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2xLw0mY6tzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}