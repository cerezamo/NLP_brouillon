{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZlRcvjw8h2RwrlJicTRHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "713b44cc1ef749a2a4d0ead10a83d402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3a7d302bf5384eaeb07952b6afeca30a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de400a0d1af24222a8f75b076be7ba79",
              "IPY_MODEL_db0667f82c75482a8507f979964e2e10"
            ]
          }
        },
        "3a7d302bf5384eaeb07952b6afeca30a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de400a0d1af24222a8f75b076be7ba79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a80f2f5a8321407393d12d16dea21f93",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 810912,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 810912,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_758251fe358e474e8747a07cea6e6fff"
          }
        },
        "db0667f82c75482a8507f979964e2e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06baf63cf6ea4d2bbc973b19d134dac3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 811k/811k [03:08&lt;00:00, 4.31kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c95adb112c844d15b360c58860032f41"
          }
        },
        "a80f2f5a8321407393d12d16dea21f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "758251fe358e474e8747a07cea6e6fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06baf63cf6ea4d2bbc973b19d134dac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c95adb112c844d15b360c58860032f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerezamo/NLP_brouillon/blob/master/Camembert_clean_automated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# CamemBERT classification model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "82093f50-4845-4f50-f863-d1f28a3861a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "fa418033-9361-4086-e0ac-0919439bcbdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "8c2a82df-ad0c-45f4-886f-4d4dd1057f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "55ca23e2-f80b-4585-adfb-26c7eb132eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 13.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f3d4b49fc971e1db5b324ea3a1a563d662d7d5ad49f96ec1b345fb7c97cd510d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "# Import medium_df_desq in \"files\"\n",
        "# Load the dataset into a pandas dataframe.\n",
        "#df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgllpcqJevp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make results reproducible \n",
        "seed_val = 2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8ICVGvUBLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unbalanced_preprocess(df,seed_val):\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #Shuffle the data \n",
        "  df_unbalanced=df.sample(frac=1).reset_index()\n",
        "\n",
        "  # Reduce to the variables we are interested in \n",
        "  df_unbalanced=df[['Texte','sexe']]\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in the unbalanced corpus : {:,}\\n'.format(df_unbalanced.shape[0]))\n",
        "  prop = (len(df_unbalanced[df_unbalanced.sexe==1])/len(df_unbalanced))*100\n",
        "  print('Proportions of women in the unbalanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  # We keep one little sample for evaluation \n",
        "  model_unbalanced, dev_unbalanced = train_test_split(df_unbalanced, test_size=0.02,random_state=seed_val)\n",
        "\n",
        "  return model_unbalanced, dev_unbalanced"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IdYJSSRUdD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dafee277-152b-481d-a61f-ca716b677bcd"
      },
      "source": [
        "df_unbalanced = unbalanced_preprocess(df,seed_val)[0]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in the unbalanced corpus : 5,000\n",
            "\n",
            "Proportions of women in the unbalanced corpus : 25.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd4SCY6LUuwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_preprocess(df,seed_val):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Let's take a balanced sample (model classifying all in men otherwise)\n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  #Shuffle the data \n",
        "  df=df.sample(frac=1).reset_index()\n",
        "\n",
        "  # Reduce to the variables we are interested in \n",
        "  df_balanced=df[['Texte','sexe']]\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this corpus : {:,}\\n'.format(df_balanced.shape[0]))\n",
        "  prop = (len(df_balanced[df_balanced.sexe==1])/len(df_balanced))*100\n",
        "  print('Proportions of women in the balanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  # Keep one little sample for evaluation \n",
        "  model_balanced, dev_balanced = train_test_split(df_balanced, test_size=0.02,random_state=seed_val)\n",
        "\n",
        "  return model_balanced, dev_balanced\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeQif_N0X4uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "91cd7a80-93ea-4973-9007-fdbb1c0bb699"
      },
      "source": [
        "df_balanced = balanced_preprocess(df,seed_val)[0]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 2,500\n",
            "\n",
            "Proportions of women in the balanced corpus : 50.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E85CuAStX_qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_splitted(df):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  from itertools import repeat\n",
        "  n=2500\n",
        "  chunks, label_split=[],[]\n",
        "  j=0\n",
        "  for text in df.Texte :\n",
        "      txt=[text[i:i+n] for i in range(0, len(text), n)]\n",
        "      chunks.append(txt)\n",
        "      label_split.extend(repeat(df.sexe[j], len(txt)))\n",
        "      j+=1\n",
        "\n",
        "  chunks = [item for sublist in chunks for item in sublist]\n",
        "  df=pd.DataFrame([chunks,label_split]).transpose()\n",
        "  df.columns=['Texte','sexe']\n",
        "  len(df)\n",
        "\n",
        "  # Let's take a balanced sample (model classifying all in men otherwise)\n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  #Shuffle the data \n",
        "  df=df.sample(frac=1).reset_index()\n",
        "\n",
        "  # Reduce to the variables we are interested in \n",
        "  df=df[['Texte','sexe']]\n",
        "\n",
        "  # Put as integer \n",
        "  df['sexe'] = df['sexe'].astype(int)\n",
        "\n",
        "  df_balanced_split= df\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this balanced splitted corpus : {:,}\\n'.format(df_balanced_split.shape[0]))\n",
        "  prop = (len(df_balanced_split[df_balanced_split.sexe==1])/len(df_balanced_split))*100\n",
        "  print('Proportions of women in the balanced splitted corpus : {}\\n'.format(prop))\n",
        "\n",
        "  # Keep one little sample for evaluation \n",
        "  model_balanced_split, dev_balance_split = train_test_split(df_balanced_split, test_size=0.02,random_state=seed_val)\n",
        "  \n",
        "  return model_balanced_split, dev_balance_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEpbT1JYf5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f44e1f69-c41c-4ba7-8b2b-9069ae04b57e"
      },
      "source": [
        "df_balanced_split = balanced_splitted(df)[0]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 13,000\n",
            "\n",
            "Proportions of women in the balanced splitted corpus : 50.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTKQ0QnAZ_wu",
        "colab_type": "text"
      },
      "source": [
        "**We propose 3 samples to train our model :**\n",
        "\n",
        "\n",
        "1.   **Unbalanced sample**\n",
        "\n",
        "We take the raw data without any further treatment.\n",
        "\n",
        "2.   **Balanced sample**\n",
        "\n",
        "The second option consist in deleting randomly part of male speeches in order to get a balanced sample. Indeed, in the case of unbalanced sample our model could decide to classify all speakers in the male category which would lead to a 0.75 accuracy in our case study. In order to avoid this we feed the model with the same proportions of male and female speakers. Other kind of treatments exist to deal with unbalanced sample (oversampling for example). This one is the simpliest one and we could argue that there is a possibility that the deleted sample contains important information that we therefore miss. However we believe that in our case this is not a big issue. Our unbalanced sample is quite large for both female and male.\n",
        "\n",
        "3. **Balanced and splitted sample**\n",
        "\n",
        "The third option is a response to the max length constraint of BERT models. Our text samples are big and contain much more tokens than the 512 limit. In the first two options we decide to just feed the model with the 512 first tokens and thus delete the rest of them. In this third option we cut the text into x parts containing 500 tokens each. All parts of the speech will serve to feed the model. By this technique we do not loose potential important informations at the end of the text. A lot of other techniques have been employed (see ref !!! PUT). We decide to stick to this one in this project. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mggkz5R9g8dD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "713b44cc1ef749a2a4d0ead10a83d402",
            "3a7d302bf5384eaeb07952b6afeca30a",
            "de400a0d1af24222a8f75b076be7ba79",
            "db0667f82c75482a8507f979964e2e10",
            "a80f2f5a8321407393d12d16dea21f93",
            "758251fe358e474e8747a07cea6e6fff",
            "06baf63cf6ea4d2bbc973b19d134dac3",
            "c95adb112c844d15b360c58860032f41"
          ]
        },
        "outputId": "d6e14c5c-f94b-4cce-dce5-af94797b2439"
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713b44cc1ef749a2a4d0ead10a83d402",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=810912, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoaZUUBChM_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "560272b8-2878-477e-ebf2-73419e46df6c"
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Messieurs,Je suis heureux de vous saluer. Quand je dis que je suis heureux de vous saluer, ce n'est pas une simple affirmation de politesse. Je le disais à l'instant à Monsieur le Ministre de la Défense, M. Richard, c'est pour moi un instant où il y a un peu d'émotion; je vais vous dire pourquoi.Vous êtes la première classe d'âge qui ne fera pas de service militaire. C'est une décision que j'ai prise, il y a deux ans, après une vraie réflexion et un vrai débat. Après tout, le service militaire c'est une vieille tradition nationale, il était plus que centenaire. Il y avait toutes sortes de raisons à cela, notamment la nécessité d'avoir une armée nombreuse et donc d'avoir des jeunes formés aux combats, à l'utilisation des armes de l'époque.On pouvait s'interroger sur la nécessité de poursuivre dans cette voie. Il y avait naturellement des critiques, il y avait beaucoup de jeunes qui se disaient qu'ils perdaient un peu leur temps, d'autres qui étaient satisfaits. Mais il y avait surtout ceux qui, dans notre pays, disaient: le service militaire c'est nécessaire pour la cohésion nationale, c'est un moyen de mettre ensemble les garçons issus de milieux et d'origines géographiques, culturelles, sociales, différents et de faire en sorte qu'ils se connaissent, qu'ils soient ensemble, et ce que l'on appelait la fraternité des casernes était considérée par beaucoup comme un élément important de la cohésion nationale. Cet argument était celui de beaucoup de conservateurs. Les conservateurs ont toujours des arguments forts, il faut en tenir compte. D'autre part, il m'était apparu que tout cela, au fond, appartenait un peu à une conception dépassée de la défense, et faisait peser au total sur les jeunes une contrainte obligatoire et excessive qui n'était pas justifiée, ou qui ne l'était plus.L'armée moderne n'est naturellement pas celle dont on avait besoin en 1914 ou 1939, ou même après, au moment des guerres de décolonisation en Algérie ou autres. Tout cela justifiait que l'on passe 1 an ou 2, voire plus, de service militaire, - parfois 3 ans au début du siècle, 2 ans et demi pendant la période de la guerre d'Algérie pour certains -.L'évolution des choses mettait cela en cause. L'évolution, c'était d'abord l'installation de la paix en Europe. Et cela a été une grande réforme, à partir du moment ou l'on a lancé la construction européenne: nous n'avons plus été menacés sur nos frontières. Il fut un temps où nous devions pouvoir opposer un barrage de poitrines aux poitrines allemandes qui risquaient de s'avancer chez nous, il fallait encore beaucoup d'hommes. La construction européenne a fait disparaître cette menace. Nous ne sommes plus menacés à nos frontières: on n'imagine pas l'Allemagne attaquant la France, on ne l'image plus. Donc, en toute hypothèse, la menace s'était éloignée. Il y avait aussi, naturellement, l'armement qui avait beaucoup changé, les techniques militaires qui s'étaient profondément transformées. Aujourd'hui, une opération militaire, la guerre le cas échéant, mobilise des hommes et des femmes qui ont une très haute compétence, qui sont aptes à utiliser des moyens extraordinairement sophistiqués, qui sont totalement disponibles et prêts à faire mouvement immédiatement, et qui n'ont besoin d'aucune formation préalable pour le faire.C'est donc un autre type de besoin que nous avons et qui ne correspond plus au service militaire tel qu'on le concevait avant. C'est donc une armée professionnelle qu'il nous faut, c'est une armée de métier, ce sont des vrais professionnels.A partir de cette réflexion, j'ai pensé que finalement, quels que soient les arguments, de nature sociale ou nationale qui plaidaient en faveur du service militaire, ce n'était plus adapté, et j'ai décidé de le supprimer. Mais la suppression du service militaire, cela ne veut pas dire la suppression de tout lien entre la jeunesse et l'armée. Il faut que les jeunes sachent ce que c'est que leur armée. Il faut que vous le sachiez, parce que vous êtes dépositaires de l'avenir. On s'imagine que tout va bien, qu'il n'y a plus de problème. Quand on est né dans les années 1980, on a toujours connu la liberté, la démocratie, les embêtements de la vie, les problèmes, le chômage, ou d'autres choses, mais on a pas connu la guerre naturellement. Vos grands-pères l'ont connue, pas vous. Cela ne vous dit rien, et pourtant c'est en permanence une menace. La France a des intérêts dans le monde, elle doit le cas échéant les défendre, et il faut que la France puisse apporter sa contribution militaire pour permettre de maintenir la paix ici où là, ou pour permettre de sauver des vies quand c'est nécessaire. Lorsque la Bosnie s'engage dans une opération suicidaire, qui fait des quantités de morts, il faut que la France, comme d'autres, puisse aller maintenir un peu l'ordre pour éviter les carnages. C'est la même chose aujourd'hui, peut-être -on ne sait pas, j'espère que non- dans le Kosovo. Et tout cela, c'est aux portes de l'Europe, c'est à moins de deux heures de chez nous en avion.Lorsqu'il y a des problèmes, hélas, en Afrique et cela arrive même si l'Afrique est sur la bonne voie, il faut que nous ayons la possibilité, en quelques heures, d'aller récupérer des Français ou des ressortissants d'autres pays qui sont là-bas et qui sont menacés dans leur vie, et qu'il faut extraire très rapidement. Cela demande une armée qui soit non seulement capable et équipée, mais qui soit aussi connue, respectée et aimée. D'où, naturellement, l'idée que les jeunes doivent connaître leur armée, doivent savoir ce qu'elle est, et quand je dis les jeunes c'est en réalité aujourd'hui les garçons comme les filles. Alors pour le moment et pour des raisons matérielles, techniques, les filles ne sont pas soumises à la même obligation que les garçons, mais dès 2000, le temps de mettre les choses en place, les garçons et les filles seront sur le même pied, auront les mêmes droits et devoirs, les mêmes obligations. Ce sera de ce point de vue une bonne chose, parce qu'il n'y a aucune raison que la prise de conscience des problèmes de la Défense, notamment de la défense de nos intérêts, de nos valeurs, de la liberté, de la dignité de l'homme, de l'égalité, il n'y a aucune raison qu'elle ne soit pas défendue de la même façon par les filles et par les garçons.Cette nouvelle armée, il faut en gros que vous sachiez ce que c'est. Elle ne vous demande pas d'obligation, ou très peu. Elle vous demande de venir passer une journée. C'est une journée d'information, de mobilisation, qui est de nature à vous permettre d'apprécier le cas échéant les choses et, pour ceux que cela intéresse, de les approfondir. Alors elle ne vous demande pas d'obligation, mais en revanche elle vous ouvre des possibilités importantes, et ne les sous-estimez pas, et pour en profiter il faut les connaître.Il y en a probablement parmi vous, ou parmi tous ceux qui se présenteront aujourd'hui pour la journée d'information, qui voudront être militaires. C'est un superbe métier, noble s'il en est. Ceux-là par conséquent ont besoin de savoir comment l'on fait si l'on veut s'engager dans cette voie. Il y a aussi ceux qui, pour des raisons personnelles, ou parce qu'ils ont le coeur à cela, veulent se consacrer pendant quelques temps - un an, un an et demi - à des tâches, je dirais généralement humanitaires au sens large du terme, qu'elles soient militaires ou qu'elles soient civiles, à apporter leur concours à l'éducation nationale pour apprendre à des gamins défavorisés à lire ou à écrire, pour faciliter la préservation de l'environnement, pour lutter contre les incendies, pour servir dans des unités chargées de maintenir la sécurité des personnes et des biens, je pense à la gendarmerie ou à d'autres domaines. Bref, il y en a certainement qui seront intéressés par l'idée de consacrer un an, un an et demi de leur coeur, de leur générosité à d'autres, dans ces domaines civils ou militaires, qui très largement peuvent être appelés des domaines humanitaires. J'ajoute que pour eux, ce sera aussi la possibilité d'acquérir une expérience, et parfois un métier, et de sortir de cette période avec la possibilité ou la capacité d'exercer un métier qu'ils auront appris. Il y aura ceux qui s'intéressent à l'armée, sans vouloir en faire leur métier. L'armée aura besoin de réservistes, beaucoup moins qu'avant, alors il y en a peut-être qui veulent ou qui voudront, parce que cela les intéresse, acquérir le minimum de connaissances leur permettant d'être versé dans ce que l'on appelle \"les réserves\" et le cas échéant d'être réservistes.Je ne veux pas entrer dans le détail, vous aurez toute la journée pour en entendre parler, mais l'objectif de cette journée est de vous faire comprendre que l'armée est l'une des institutions les plus nobles et les plus nécessaires de la nation et qu'il faut la connaître. Parfois on a un peu tendance à la critiquer, sans savoir. Il faut la connaître parce qu'elle fait des choses formidables, notamment, je le répète, dans le monde moderne sur le plan humanitaire et enfin, parce que c'est le dernier rempart de notre sécurité, de nos libertés, de nos intérêts, si par hasard les choses vont mal. Alors elle mérite d'être respectée, elle a droit au respect. Mais encore faut-il savoir pourquoi et c'est cela que l'on va vous dire.D'autre part, on veut vous ouvrir les voies et les possibilités que, directement ou indirectement, cette armée peut vous donner et enfin vous rappeler quels sont les principes que nous avons en commun, les principes de solidarité, les principes de respect de l'homme, de sa dignité, de l'égalité des uns et des autres, et tout cela est également important.Voilà. J'ai dû certainement oublier des choses importantes, mais on veut vous dire quels sont vos droits et aussi quels sont vos devoirs à l'égard de la Nation, et cette journée est consacrée à cela. Je souhaite qu'elle soit pour vous intéressante, qu'elle vous apprenne des choses, qu'elle vous ouvre des perspectives, qu'elle vous donne davantage conscience de vos droits mais aussi de vos devoirs.Je termine en souhaitant surtout, quelle que soit la voie que vous emprunterez et les conclusions que vous tirerez de cette journée, je vous souhaite surtout bonne chance et bon vent\n",
            "Tokenized:  ['▁Messieurs', ',', 'Je', '▁suis', '▁heureux', '▁de', '▁vous', '▁saluer', '.', '▁Quand', '▁je', '▁dis', '▁que', '▁je', '▁suis', '▁heureux', '▁de', '▁vous', '▁saluer', ',', '▁ce', '▁n', \"'\", 'est', '▁pas', '▁une', '▁simple', '▁affirmation', '▁de', '▁politesse', '.', '▁Je', '▁le', '▁disais', '▁à', '▁l', \"'\", 'instant', '▁à', '▁Monsieur', '▁le', '▁Ministre', '▁de', '▁la', '▁Défense', ',', '▁M', '.', '▁Richard', ',', '▁c', \"'\", 'est', '▁pour', '▁moi', '▁un', '▁instant', '▁où', '▁il', '▁y', '▁a', '▁un', '▁peu', '▁d', \"'\", 'émotion', ';', '▁je', '▁vais', '▁vous', '▁dire', '▁pourquoi', '.', 'Vous', '▁êtes', '▁la', '▁première', '▁classe', '▁d', \"'\", 'âge', '▁qui', '▁ne', '▁fera', '▁pas', '▁de', '▁service', '▁militaire', '.', '▁C', \"'\", 'est', '▁une', '▁décision', '▁que', '▁j', \"'\", 'ai', '▁prise', ',', '▁il', '▁y', '▁a', '▁deux', '▁ans', ',', '▁après', '▁une', '▁vraie', '▁réflexion', '▁et', '▁un', '▁vrai', '▁débat', '.', '▁Après', '▁tout', ',', '▁le', '▁service', '▁militaire', '▁c', \"'\", 'est', '▁une', '▁vieille', '▁tradition', '▁nationale', ',', '▁il', '▁était', '▁plus', '▁que', '▁centenaire', '.', '▁Il', '▁y', '▁avait', '▁toutes', '▁sortes', '▁de', '▁raisons', '▁à', '▁cela', ',', '▁notamment', '▁la', '▁nécessité', '▁d', \"'\", 'avoir', '▁une', '▁armée', '▁nombreuse', '▁et', '▁donc', '▁d', \"'\", 'avoir', '▁des', '▁jeunes', '▁formés', '▁aux', '▁combats', ',', '▁à', '▁l', \"'\", 'utilisation', '▁des', '▁armes', '▁de', '▁l', \"'\", 'époque', '.', 'On', '▁pouvait', '▁s', \"'\", 'interroger', '▁sur', '▁la', '▁nécessité', '▁de', '▁poursuivre', '▁dans', '▁cette', '▁voie', '.', '▁Il', '▁y', '▁avait', '▁naturellement', '▁des', '▁critiques', ',', '▁il', '▁y', '▁avait', '▁beaucoup', '▁de', '▁jeunes', '▁qui', '▁se', '▁disaient', '▁qu', \"'\", 'ils', '▁perd', 'aient', '▁un', '▁peu', '▁leur', '▁temps', ',', '▁d', \"'\", 'autres', '▁qui', '▁étaient', '▁satisfaits', '.', '▁Mais', '▁il', '▁y', '▁avait', '▁surtout', '▁ceux', '▁qui', ',', '▁dans', '▁notre', '▁pays', ',', '▁disaient', ':', '▁le', '▁service', '▁militaire', '▁c', \"'\", 'est', '▁nécessaire', '▁pour', '▁la', '▁cohésion', '▁nationale', ',', '▁c', \"'\", 'est', '▁un', '▁moyen', '▁de', '▁mettre', '▁ensemble', '▁les', '▁garçons', '▁issus', '▁de', '▁milieux', '▁et', '▁d', \"'\", 'origine', 's', '▁géographiques', ',', '▁culturelles', ',', '▁sociales', ',', '▁différents', '▁et', '▁de', '▁faire', '▁en', '▁sorte', '▁qu', \"'\", 'ils', '▁se', '▁connaissent', ',', '▁qu', \"'\", 'ils', '▁soient', '▁ensemble', ',', '▁et', '▁ce', '▁que', '▁l', \"'\", 'on', '▁appelait', '▁la', '▁fraternité', '▁des', '▁caserne', 's', '▁était', '▁considérée', '▁par', '▁beaucoup', '▁comme', '▁un', '▁élément', '▁important', '▁de', '▁la', '▁cohésion', '▁nationale', '.', '▁Cet', '▁argument', '▁était', '▁celui', '▁de', '▁beaucoup', '▁de', '▁conservateurs', '.', '▁Les', '▁conservateurs', '▁ont', '▁toujours', '▁des', '▁arguments', '▁forts', ',', '▁il', '▁faut', '▁en', '▁tenir', '▁compte', '.', '▁D', \"'\", 'autre', '▁part', ',', '▁il', '▁m', \"'\", 'était', '▁apparu', '▁que', '▁tout', '▁cela', ',', '▁au', '▁fond', ',', '▁appartenait', '▁un', '▁peu', '▁à', '▁une', '▁conception', '▁dépassé', 'e', '▁de', '▁la', '▁défense', ',', '▁et', '▁faisait', '▁peser', '▁au', '▁total', '▁sur', '▁les', '▁jeunes', '▁une', '▁contrainte', '▁obligatoire', '▁et', '▁excessive', '▁qui', '▁n', \"'\", 'était', '▁pas', '▁justifiée', ',', '▁ou', '▁qui', '▁ne', '▁l', \"'\", 'était', '▁plus', '.', 'L', \"'\", 'armée', '▁moderne', '▁n', \"'\", 'est', '▁naturellement', '▁pas', '▁celle', '▁dont', '▁on', '▁avait', '▁besoin', '▁en', '▁1914', '▁ou', '▁1939', ',', '▁ou', '▁même', '▁après', ',', '▁au', '▁moment', '▁des', '▁guerres', '▁de', '▁déco', 'lon', 'isation', '▁en', '▁Algérie', '▁ou', '▁autres', '.', '▁Tout', '▁cela', '▁just', 'ifi', 'ait', '▁que', '▁l', \"'\", 'on', '▁passe', '▁1', '▁an', '▁ou', '▁2', ',', '▁voire', '▁plus', ',', '▁de', '▁service', '▁militaire', ',', '▁-', '▁parfois', '▁3', '▁ans', '▁au', '▁début', '▁du', '▁siècle', ',', '▁2', '▁ans', '▁et', '▁demi', '▁pendant', '▁la', '▁période', '▁de', '▁la', '▁guerre', '▁d', \"'\", 'Algérie', '▁pour', '▁certains', '▁-', '.', 'L', \"'\", 'évolution', '▁des', '▁choses', '▁mettait', '▁cela', '▁en', '▁cause', '.', '▁L', \"'\", 'évolution', ',', '▁c', \"'\", 'était', '▁d', \"'\", 'abord', '▁l', \"'\", 'installation', '▁de', '▁la', '▁paix', '▁en', '▁Europe', '.', '▁Et', '▁cela', '▁a', '▁été', '▁une', '▁grande', '▁réforme', ',', '▁à', '▁partir', '▁du', '▁moment', '▁ou', '▁l', \"'\", 'on', '▁a', '▁lancé', '▁la', '▁construction', '▁européenne', ':', '▁nous', '▁n', \"'\", 'avons', '▁plus', '▁été', '▁menacé', 's', '▁sur', '▁nos', '▁frontières', '.', '▁Il', '▁fut', '▁un', '▁temps', '▁où', '▁nous', '▁devions', '▁pouvoir', '▁opposer', '▁un', '▁barrage', '▁de', '▁poitrine', 's', '▁aux', '▁poitrine', 's', '▁allemandes', '▁qui', '▁risqu', 'aient', '▁de', '▁s', \"'\", 'avancer', '▁chez', '▁nous', ',', '▁il', '▁fallait', '▁encore', '▁beaucoup', '▁d', \"'\", 'hommes', '.', '▁La', '▁construction', '▁européenne', '▁a', '▁fait', '▁disparaître', '▁cette', '▁menace', '.', '▁Nous', '▁ne', '▁sommes', '▁plus', '▁menacé', 's', '▁à', '▁nos', '▁frontières', ':', '▁on', '▁n', \"'\", 'imagine', '▁pas', '▁l', \"'\", 'Allemagne', '▁attaquant', '▁la', '▁France', ',', '▁on', '▁ne', '▁l', \"'\", 'image', '▁plus', '.', '▁Donc', ',', '▁en', '▁toute', '▁hypothèse', ',', '▁la', '▁menace', '▁s', \"'\", 'était', '▁éloignée', '.', '▁Il', '▁y', '▁avait', '▁aussi', ',', '▁naturellement', ',', '▁l', \"'\", 'armement', '▁qui', '▁avait', '▁beaucoup', '▁changé', ',', '▁les', '▁techniques', '▁militaires', '▁qui', '▁s', \"'\", 'étaient', '▁profondément', '▁transformée', 's', '.', '▁Aujourd', \"'\", 'hui', ',', '▁une', '▁opération', '▁militaire', ',', '▁la', '▁guerre', '▁le', '▁cas', '▁échéant', ',', '▁mobilise', '▁des', '▁hommes', '▁et', '▁des', '▁femmes', '▁qui', '▁ont', '▁une', '▁très', '▁haute', '▁compétence', ',', '▁qui', '▁sont', '▁apte', 's', '▁à', '▁utiliser', '▁des', '▁moyens', '▁extraordinaire', 'ment', '▁sophistiqué', 's', ',', '▁qui', '▁sont', '▁totalement', '▁disponibles', '▁et', '▁prêts', '▁à', '▁faire', '▁mouvement', '▁immédiatement', ',', '▁et', '▁qui', '▁n', \"'\", 'ont', '▁besoin', '▁d', \"'\", 'aucune', '▁formation', '▁préalable', '▁pour', '▁le', '▁faire', '.', 'C', \"'\", 'est', '▁donc', '▁un', '▁autre', '▁type', '▁de', '▁besoin', '▁que', '▁nous', '▁avons', '▁et', '▁qui', '▁ne', '▁correspond', '▁plus', '▁au', '▁service', '▁militaire', '▁tel', '▁qu', \"'\", 'on', '▁le', '▁concev', 'ait', '▁avant', '.', '▁C', \"'\", 'est', '▁donc', '▁une', '▁armée', '▁professionnelle', '▁qu', \"'\", 'il', '▁nous', '▁faut', ',', '▁c', \"'\", 'est', '▁une', '▁armée', '▁de', '▁métier', ',', '▁ce', '▁sont', '▁des', '▁vrais', '▁professionnels', '.', 'A', '▁partir', '▁de', '▁cette', '▁réflexion', ',', '▁j', \"'\", 'ai', '▁pensé', '▁que', '▁finalement', ',', '▁quels', '▁que', '▁soient', '▁les', '▁arguments', ',', '▁de', '▁nature', '▁sociale', '▁ou', '▁nationale', '▁qui', '▁plaid', 'aient', '▁en', '▁faveur', '▁du', '▁service', '▁militaire', ',', '▁ce', '▁n', \"'\", 'était', '▁plus', '▁adapté', ',', '▁et', '▁j', \"'\", 'ai', '▁décidé', '▁de', '▁le', '▁supprimer', '.', '▁Mais', '▁la', '▁suppression', '▁du', '▁service', '▁militaire', ',', '▁cela', '▁ne', '▁veut', '▁pas', '▁dire', '▁la', '▁suppression', '▁de', '▁tout', '▁lien', '▁entre', '▁la', '▁jeunesse', '▁et', '▁l', \"'\", 'armée', '.', '▁Il', '▁faut', '▁que', '▁les', '▁jeunes', '▁sache', 'nt', '▁ce', '▁que', '▁c', \"'\", 'est', '▁que', '▁leur', '▁armée', '.', '▁Il', '▁faut', '▁que', '▁vous', '▁le', '▁sa', 'chi', 'ez', ',', '▁parce', '▁que', '▁vous', '▁êtes', '▁dépositaire', 's', '▁de', '▁l', \"'\", 'avenir', '.', '▁On', '▁s', \"'\", 'imagine', '▁que', '▁tout', '▁va', '▁bien', ',', '▁qu', \"'\", 'il', '▁n', \"'\", 'y', '▁a', '▁plus', '▁de', '▁problème', '.', '▁Quand', '▁on', '▁est', '▁né', '▁dans', '▁les', '▁années', '▁1980', ',', '▁on', '▁a', '▁toujours', '▁connu', '▁la', '▁liberté', ',', '▁la', '▁démocratie', ',', '▁les', '▁emb', 'ête', 'ments', '▁de', '▁la', '▁vie', ',', '▁les', '▁problèmes', ',', '▁le', '▁chômage', ',', '▁ou', '▁d', \"'\", 'autres', '▁choses', ',', '▁mais', '▁on', '▁a', '▁pas', '▁connu', '▁la', '▁guerre', '▁naturellement', '.', '▁Vos', '▁grands', '-', 'père', 's', '▁l', \"'\", 'ont', '▁connue', ',', '▁pas', '▁vous', '.', '▁Cela', '▁ne', '▁vous', '▁dit', '▁rien', ',', '▁et', '▁pourtant', '▁c', \"'\", 'est', '▁en', '▁permanence', '▁une', '▁menace', '.', '▁La', '▁France', '▁a', '▁des', '▁intérêts', '▁dans', '▁le', '▁monde', ',', '▁elle', '▁doit', '▁le', '▁cas', '▁échéant', '▁les', '▁défendre', ',', '▁et', '▁il', '▁faut', '▁que', '▁la', '▁France', '▁puisse', '▁apporter', '▁sa', '▁contribution', '▁militaire', '▁pour', '▁permettre', '▁de', '▁maintenir', '▁la', '▁paix', '▁ici', '▁où', '▁là', ',', '▁ou', '▁pour', '▁permettre', '▁de', '▁sauver', '▁des', '▁vie', 's', '▁quand', '▁c', \"'\", 'est', '▁nécessaire', '.', '▁Lorsque', '▁la', '▁Bosnie', '▁s', \"'\", 'engage', '▁dans', '▁une', '▁opération', '▁suicidaire', ',', '▁qui', '▁fait', '▁des', '▁quantités', '▁de', '▁morts', ',', '▁il', '▁faut', '▁que', '▁la', '▁France', ',', '▁comme', '▁d', \"'\", 'autres', ',', '▁puisse', '▁aller', '▁maintenir', '▁un', '▁peu', '▁l', \"'\", 'ordre', '▁pour', '▁éviter', '▁les', '▁car', 'nage', 's', '.', '▁C', \"'\", 'est', '▁la', '▁même', '▁chose', '▁aujourd', \"'\", 'hui', ',', '▁peut', '-', 'être', '▁-', 'on', '▁ne', '▁sait', '▁pas', ',', '▁j', \"'\", 'espère', '▁que', '▁non', '-', '▁dans', '▁le', '▁Kosovo', '.', '▁Et', '▁tout', '▁cela', ',', '▁c', \"'\", 'est', '▁aux', '▁portes', '▁de', '▁l', \"'\", 'Europe', ',', '▁c', \"'\", 'est', '▁à', '▁moins', '▁de', '▁deux', '▁heures', '▁de', '▁chez', '▁nous', '▁en', '▁avion', '.', 'L', 'ors', 'qu', \"'\", 'il', '▁y', '▁a', '▁des', '▁problèmes', ',', '▁hélas', ',', '▁en', '▁Afrique', '▁et', '▁cela', '▁arrive', '▁même', '▁si', '▁l', \"'\", 'Afrique', '▁est', '▁sur', '▁la', '▁bonne', '▁voie', ',', '▁il', '▁faut', '▁que', '▁nous', '▁ayons', '▁la', '▁possibilité', ',', '▁en', '▁quelques', '▁heures', ',', '▁d', \"'\", 'aller', '▁récupérer', '▁des', '▁Français', '▁ou', '▁des', '▁ressortissants', '▁d', \"'\", 'autres', '▁pays', '▁qui', '▁sont', '▁là', '-', 'bas', '▁et', '▁qui', '▁sont', '▁menacé', 's', '▁dans', '▁leur', '▁vie', ',', '▁et', '▁qu', \"'\", 'il', '▁faut', '▁extraire', '▁très', '▁rapidement', '.', '▁Cela', '▁demande', '▁une', '▁armée', '▁qui', '▁soit', '▁non', '▁seulement', '▁capable', '▁et', '▁équipée', ',', '▁mais', '▁qui', '▁soit', '▁aussi', '▁connue', ',', '▁respectée', '▁et', '▁aimé', 'e', '.', '▁D', \"'\", 'où', ',', '▁naturellement', ',', '▁l', \"'\", 'idée', '▁que', '▁les', '▁jeunes', '▁doivent', '▁connaître', '▁leur', '▁armée', ',', '▁doivent', '▁savoir', '▁ce', '▁qu', \"'\", 'elle', '▁est', ',', '▁et', '▁quand', '▁je', '▁dis', '▁les', '▁jeunes', '▁c', \"'\", 'est', '▁en', '▁réalité', '▁aujourd', \"'\", 'hui', '▁les', '▁garçons', '▁comme', '▁les', '▁filles', '.', '▁Alors', '▁pour', '▁le', '▁moment', '▁et', '▁pour', '▁des', '▁raisons', '▁matérielles', ',', '▁techniques', ',', '▁les', '▁filles', '▁ne', '▁sont', '▁pas', '▁soumises', '▁à', '▁la', '▁même', '▁obligation', '▁que', '▁les', '▁garçons', ',', '▁mais', '▁dès', '▁2000,', '▁le', '▁temps', '▁de', '▁mettre', '▁les', '▁choses', '▁en', '▁place', ',', '▁les', '▁garçons', '▁et', '▁les', '▁filles', '▁seront', '▁sur', '▁le', '▁même', '▁pied', ',', '▁auront', '▁les', '▁mêmes', '▁droits', '▁et', '▁devoirs', ',', '▁les', '▁mêmes', '▁obligations', '.', '▁Ce', '▁sera', '▁de', '▁ce', '▁point', '▁de', '▁vue', '▁une', '▁bonne', '▁chose', ',', '▁parce', '▁qu', \"'\", 'il', '▁n', \"'\", 'y', '▁a', '▁aucune', '▁raison', '▁que', '▁la', '▁prise', '▁de', '▁conscience', '▁des', '▁problèmes', '▁de', '▁la', '▁Défense', ',', '▁notamment', '▁de', '▁la', '▁défense', '▁de', '▁nos', '▁intérêts', ',', '▁de', '▁nos', '▁valeurs', ',', '▁de', '▁la', '▁liberté', ',', '▁de', '▁la', '▁dignité', '▁de', '▁l', \"'\", 'homme', ',', '▁de', '▁l', \"'\", 'égalité', ',', '▁il', '▁n', \"'\", 'y', '▁a', '▁aucune', '▁raison', '▁qu', \"'\", 'elle', '▁ne', '▁soit', '▁pas', '▁défendu', 'e', '▁de', '▁la', '▁même', '▁façon', '▁par', '▁les', '▁filles', '▁et', '▁par', '▁les', '▁garçons', '.', 'Cette', '▁nouvelle', '▁armée', ',', '▁il', '▁faut', '▁en', '▁gros', '▁que', '▁vous', '▁sa', 'chi', 'ez', '▁ce', '▁que', '▁c', \"'\", 'est', '.', '▁Elle', '▁ne', '▁vous', '▁demande', '▁pas', '▁d', \"'\", 'obligation', ',', '▁ou', '▁très', '▁peu', '.', '▁Elle', '▁vous', '▁demande', '▁de', '▁venir', '▁passer', '▁une', '▁journée', '.', '▁C', \"'\", 'est', '▁une', '▁journée', '▁d', \"'\", 'information', ',', '▁de', '▁mobilisation', ',', '▁qui', '▁est', '▁de', '▁nature', '▁à', '▁vous', '▁permettre', '▁d', \"'\", 'apprécier', '▁le', '▁cas', '▁échéant', '▁les', '▁choses', '▁et', ',', '▁pour', '▁ceux', '▁que', '▁cela', '▁intéresse', ',', '▁de', '▁les', '▁approfondir', '.', '▁Alors', '▁elle', '▁ne', '▁vous', '▁demande', '▁pas', '▁d', \"'\", 'obligation', ',', '▁mais', '▁en', '▁revanche', '▁elle', '▁vous', '▁ouvre', '▁des', '▁possibilités', '▁importantes', ',', '▁et', '▁ne', '▁les', '▁sous', '-', 'estime', 'z', '▁pas', ',', '▁et', '▁pour', '▁en', '▁profiter', '▁il', '▁faut', '▁les', '▁connaître', '.', 'Il', '▁y', '▁en', '▁a', '▁probablement', '▁parmi', '▁vous', ',', '▁ou', '▁parmi', '▁tous', '▁ceux', '▁qui', '▁se', '▁présenter', 'ont', '▁aujourd', \"'\", 'hui', '▁pour', '▁la', '▁journée', '▁d', \"'\", 'information', ',', '▁qui', '▁voudront', '▁être', '▁militaires', '.', '▁C', \"'\", 'est', '▁un', '▁superbe', '▁métier', ',', '▁noble', '▁s', \"'\", 'il', '▁en', '▁est', '.', '▁Ceux', '-', 'là', '▁par', '▁conséquent', '▁ont', '▁besoin', '▁de', '▁savoir', '▁comment', '▁l', \"'\", 'on', '▁fait', '▁si', '▁l', \"'\", 'on', '▁veut', '▁s', \"'\", 'engager', '▁dans', '▁cette', '▁voie', '.', '▁Il', '▁y', '▁a', '▁aussi', '▁ceux', '▁qui', ',', '▁pour', '▁des', '▁raisons', '▁personnelles', ',', '▁ou', '▁parce', '▁qu', \"'\", 'ils', '▁ont', '▁le', '▁coeur', '▁à', '▁cela', ',', '▁veulent', '▁se', '▁consacrer', '▁pendant', '▁quelques', '▁temps', '▁-', '▁un', '▁an', ',', '▁un', '▁an', '▁et', '▁demi', '▁-', '▁à', '▁des', '▁tâches', ',', '▁je', '▁dirais', '▁généralement', '▁humanitaire', 's', '▁au', '▁sens', '▁large', '▁du', '▁terme', ',', '▁qu', \"'\", 'elles', '▁soient', '▁militaires', '▁ou', '▁qu', \"'\", 'elles', '▁soient', '▁civiles', ',', '▁à', '▁apporter', '▁leur', '▁concours', '▁à', '▁l', \"'\", 'éducation', '▁nationale', '▁pour', '▁apprendre', '▁à', '▁des', '▁gamin', 's', '▁dé', 'favoris', 'és', '▁à', '▁lire', '▁ou', '▁à', '▁écrire', ',', '▁pour', '▁faciliter', '▁la', '▁préservation', '▁de', '▁l', \"'\", 'environnement', ',', '▁pour', '▁lutter', '▁contre', '▁les', '▁incendie', 's', ',', '▁pour', '▁servir', '▁dans', '▁des', '▁unités', '▁chargée', 's', '▁de', '▁maintenir', '▁la', '▁sécurité', '▁des', '▁personnes', '▁et', '▁des', '▁bien', 's', ',', '▁je', '▁pense', '▁à', '▁la', '▁gendarmerie', '▁ou', '▁à', '▁d', \"'\", 'autres', '▁domaines', '.', '▁Bref', ',', '▁il', '▁y', '▁en', '▁a', '▁certainement', '▁qui', '▁seront', '▁intéressés', '▁par', '▁l', \"'\", 'idée', '▁de', '▁consacrer', '▁un', '▁an', ',', '▁un', '▁an', '▁et', '▁demi', '▁de', '▁leur', '▁coeur', ',', '▁de', '▁leur', '▁générosité', '▁à', '▁d', \"'\", 'autres', ',', '▁dans', '▁ces', '▁domaines', '▁civils', '▁ou', '▁militaires', ',', '▁qui', '▁très', '▁largement', '▁peuvent', '▁être', '▁appelés', '▁des', '▁domaines', '▁humanitaire', 's', '.', '▁J', \"'\", 'ajoute', '▁que', '▁pour', '▁eux', ',', '▁ce', '▁sera', '▁aussi', '▁la', '▁possibilité', '▁d', \"'\", 'acquérir', '▁une', '▁expérience', ',', '▁et', '▁parfois', '▁un', '▁métier', ',', '▁et', '▁de', '▁sortir', '▁de', '▁cette', '▁période', '▁avec', '▁la', '▁possibilité', '▁ou', '▁la', '▁capacité', '▁d', \"'\", 'exercer', '▁un', '▁métier', '▁qu', \"'\", 'ils', '▁auront', '▁appris', '.', '▁Il', '▁y', '▁aura', '▁ceux', '▁qui', '▁s', \"'\", 'intéressent', '▁à', '▁l', \"'\", 'armée', ',', '▁sans', '▁vouloir', '▁en', '▁faire', '▁leur', '▁métier', '.', '▁L', \"'\", 'armée', '▁aura', '▁besoin', '▁de', '▁réserv', 'istes', ',', '▁beaucoup', '▁moins', '▁qu', \"'\", 'avant', ',', '▁alors', '▁il', '▁y', '▁en', '▁a', '▁peut', '-', 'être', '▁qui', '▁veulent', '▁ou', '▁qui', '▁voudront', ',', '▁parce', '▁que', '▁cela', '▁les', '▁intéresse', ',', '▁acquérir', '▁le', '▁minimum', '▁de', '▁connaissances', '▁leur', '▁permettant', '▁d', \"'\", 'être', '▁versé', '▁dans', '▁ce', '▁que', '▁l', \"'\", 'on', '▁appelle', '▁\"', 'les', '▁réserves', '\"', '▁et', '▁le', '▁cas', '▁échéant', '▁d', \"'\", 'être', '▁réserv', 'istes', '.', 'Je', '▁ne', '▁veux', '▁pas', '▁entrer', '▁dans', '▁le', '▁détail', ',', '▁vous', '▁aurez', '▁toute', '▁la', '▁journée', '▁pour', '▁en', '▁entendre', '▁parler', ',', '▁mais', '▁l', \"'\", 'objectif', '▁de', '▁cette', '▁journée', '▁est', '▁de', '▁vous', '▁faire', '▁comprendre', '▁que', '▁l', \"'\", 'armée', '▁est', '▁l', \"'\", 'une', '▁des', '▁institutions', '▁les', '▁plus', '▁nobles', '▁et', '▁les', '▁plus', '▁nécessaires', '▁de', '▁la', '▁nation', '▁et', '▁qu', \"'\", 'il', '▁faut', '▁la', '▁connaître', '.', '▁Parfois', '▁on', '▁a', '▁un', '▁peu', '▁tendance', '▁à', '▁la', '▁critiquer', ',', '▁sans', '▁savoir', '.', '▁Il', '▁faut', '▁la', '▁connaître', '▁parce', '▁qu', \"'\", 'elle', '▁fait', '▁des', '▁choses', '▁formidable', 's', ',', '▁notamment', ',', '▁je', '▁le', '▁répète', ',', '▁dans', '▁le', '▁monde', '▁moderne', '▁sur', '▁le', '▁plan', '▁humanitaire', '▁et', '▁enfin', ',', '▁parce', '▁que', '▁c', \"'\", 'est', '▁le', '▁dernier', '▁rempart', '▁de', '▁notre', '▁sécurité', ',', '▁de', '▁nos', '▁libertés', ',', '▁de', '▁nos', '▁intérêts', ',', '▁si', '▁par', '▁hasard', '▁les', '▁choses', '▁vont', '▁mal', '.', '▁Alors', '▁elle', '▁mérite', '▁d', \"'\", 'être', '▁respectée', ',', '▁elle', '▁a', '▁droit', '▁au', '▁respect', '.', '▁Mais', '▁encore', '▁faut', '-', 'il', '▁savoir', '▁pourquoi', '▁et', '▁c', \"'\", 'est', '▁cela', '▁que', '▁l', \"'\", 'on', '▁va', '▁vous', '▁dire', '.', 'D', \"'\", 'autre', '▁part', ',', '▁on', '▁veut', '▁vous', '▁ouvrir', '▁les', '▁voies', '▁et', '▁les', '▁possibilités', '▁que', ',', '▁directement', '▁ou', '▁indirectement', ',', '▁cette', '▁armée', '▁peut', '▁vous', '▁donner', '▁et', '▁enfin', '▁vous', '▁rappeler', '▁quels', '▁sont', '▁les', '▁principes', '▁que', '▁nous', '▁avons', '▁en', '▁commun', ',', '▁les', '▁principes', '▁de', '▁solidarité', ',', '▁les', '▁principes', '▁de', '▁respect', '▁de', '▁l', \"'\", 'homme', ',', '▁de', '▁sa', '▁dignité', ',', '▁de', '▁l', \"'\", 'égalité', '▁des', '▁un', 's', '▁et', '▁des', '▁autres', ',', '▁et', '▁tout', '▁cela', '▁est', '▁également', '▁important', '.', 'Voi', 'là', '.', '▁J', \"'\", 'ai', '▁dû', '▁certainement', '▁oublier', '▁des', '▁choses', '▁importantes', ',', '▁mais', '▁on', '▁veut', '▁vous', '▁dire', '▁quels', '▁sont', '▁vos', '▁droits', '▁et', '▁aussi', '▁quels', '▁sont', '▁vos', '▁devoirs', '▁à', '▁l', \"'\", 'égard', '▁de', '▁la', '▁Nation', ',', '▁et', '▁cette', '▁journée', '▁est', '▁consacrée', '▁à', '▁cela', '.', '▁Je', '▁souhaite', '▁qu', \"'\", 'elle', '▁soit', '▁pour', '▁vous', '▁intéressante', ',', '▁qu', \"'\", 'elle', '▁vous', '▁', 'apprenne', '▁des', '▁choses', ',', '▁qu', \"'\", 'elle', '▁vous', '▁ouvre', '▁des', '▁perspectives', ',', '▁qu', \"'\", 'elle', '▁vous', '▁donne', '▁davantage', '▁conscience', '▁de', '▁vos', '▁droits', '▁mais', '▁aussi', '▁de', '▁vos', '▁devoirs', '.', 'Je', '▁termine', '▁en', '▁souhaitant', '▁surtout', ',', '▁quelle', '▁que', '▁soit', '▁la', '▁voie', '▁que', '▁vous', '▁emprunter', 'ez', '▁et', '▁les', '▁conclusions', '▁que', '▁vous', '▁tirer', 'ez', '▁de', '▁cette', '▁journée', ',', '▁je', '▁vous', '▁souhaite', '▁surtout', '▁bonne', '▁chance', '▁et', '▁bon', '▁vent']\n",
            "Token IDs:  [19923, 7, 1684, 146, 1941, 8, 39, 14778, 9, 877, 50, 701, 27, 50, 146, 1941, 8, 39, 14778, 7, 44, 49, 11, 41, 34, 28, 445, 14521, 8, 22897, 9, 100, 16, 9349, 15, 17, 11, 2337, 15, 2445, 16, 6375, 8, 13, 7682, 7, 188, 9, 5048, 7, 60, 11, 41, 24, 202, 23, 3334, 147, 51, 102, 33, 23, 126, 18, 11, 7972, 154, 50, 676, 39, 248, 590, 9, 5510, 495, 13, 272, 1010, 18, 11, 1445, 31, 45, 1438, 34, 8, 366, 2330, 9, 84, 11, 41, 28, 1141, 27, 76, 11, 73, 722, 7, 51, 102, 33, 116, 134, 7, 182, 28, 2278, 2284, 14, 23, 600, 2159, 9, 407, 66, 7, 16, 366, 2330, 60, 11, 41, 28, 3244, 2920, 945, 7, 51, 149, 40, 27, 16880, 9, 69, 102, 171, 208, 5032, 8, 1819, 15, 207, 7, 410, 13, 2966, 18, 11, 443, 28, 6841, 23960, 14, 145, 18, 11, 443, 20, 538, 11785, 68, 5892, 7, 15, 17, 11, 817, 20, 3072, 8, 17, 11, 1475, 9, 3317, 1189, 52, 11, 12944, 32, 13, 2966, 8, 3757, 29, 78, 1454, 9, 69, 102, 171, 3522, 20, 4528, 7, 51, 102, 171, 217, 8, 538, 31, 48, 21860, 46, 11, 240, 2639, 488, 23, 126, 97, 125, 7, 18, 11, 266, 31, 530, 18593, 9, 159, 51, 102, 171, 381, 320, 31, 7, 29, 127, 256, 7, 21860, 92, 16, 366, 2330, 60, 11, 41, 885, 24, 13, 13652, 945, 7, 60, 11, 41, 23, 694, 8, 328, 760, 19, 5024, 4434, 8, 6687, 14, 18, 11, 870, 10, 17414, 7, 5993, 7, 2201, 7, 579, 14, 8, 85, 22, 1055, 46, 11, 240, 48, 4688, 7, 46, 11, 240, 1053, 760, 7, 14, 44, 27, 17, 11, 88, 23308, 13, 18465, 20, 22204, 10, 149, 7538, 37, 217, 79, 23, 3228, 693, 8, 13, 13652, 945, 9, 1223, 6729, 149, 330, 8, 217, 8, 19827, 9, 74, 19827, 96, 179, 20, 8778, 5646, 7, 51, 213, 22, 1852, 287, 9, 160, 11, 369, 292, 7, 51, 115, 11, 230, 8448, 27, 66, 207, 7, 36, 729, 7, 24439, 23, 126, 15, 28, 1465, 7675, 35, 8, 13, 1923, 7, 14, 1321, 17197, 36, 1458, 32, 19, 538, 28, 9052, 3329, 14, 10930, 31, 49, 11, 230, 34, 21638, 7, 47, 31, 45, 17, 11, 230, 40, 9, 370, 11, 2677, 1558, 49, 11, 41, 3522, 34, 386, 174, 91, 171, 394, 22, 13906, 47, 18180, 7, 47, 93, 182, 7, 36, 262, 20, 9100, 8, 2341, 3190, 1385, 22, 6278, 47, 214, 9, 543, 207, 18340, 6126, 199, 27, 17, 11, 88, 507, 124, 674, 47, 118, 7, 1786, 40, 7, 8, 366, 2330, 7, 67, 610, 135, 134, 36, 479, 25, 740, 7, 118, 134, 14, 1644, 339, 13, 782, 8, 13, 775, 18, 11, 6108, 24, 420, 67, 9, 370, 11, 2010, 20, 541, 13079, 207, 22, 625, 9, 71, 11, 2010, 7, 60, 11, 230, 18, 11, 803, 17, 11, 2165, 8, 13, 1931, 22, 1532, 9, 139, 207, 33, 101, 28, 293, 3145, 7, 15, 350, 25, 262, 47, 17, 11, 88, 33, 2244, 13, 1015, 1467, 92, 63, 49, 11, 2200, 40, 101, 12319, 10, 32, 166, 5039, 9, 69, 547, 23, 125, 147, 63, 28196, 351, 19145, 23, 10230, 8, 4417, 10, 68, 4417, 10, 19912, 31, 27318, 488, 8, 52, 11, 13693, 222, 63, 7, 51, 2032, 143, 217, 18, 11, 5474, 9, 61, 1015, 1467, 33, 82, 8048, 78, 3456, 9, 170, 45, 464, 40, 12319, 10, 15, 166, 5039, 92, 91, 49, 11, 6305, 34, 17, 11, 3930, 17444, 13, 184, 7, 91, 45, 17, 11, 1106, 40, 9, 1416, 7, 22, 194, 13985, 7, 13, 3456, 52, 11, 230, 16391, 9, 69, 102, 171, 99, 7, 3522, 7, 17, 11, 15199, 31, 171, 217, 2548, 7, 19, 1054, 3788, 31, 52, 11, 2611, 5588, 14854, 10, 9, 1301, 11, 265, 7, 28, 3475, 2330, 7, 13, 775, 16, 203, 8540, 7, 24962, 20, 529, 14, 20, 389, 31, 96, 28, 95, 1118, 4956, 7, 31, 56, 15223, 10, 15, 881, 20, 1149, 4969, 131, 20656, 10, 7, 31, 56, 1366, 1339, 14, 4248, 15, 85, 1018, 2323, 7, 14, 31, 49, 11, 263, 394, 18, 11, 8855, 513, 3538, 24, 16, 85, 9, 228, 11, 41, 145, 23, 238, 460, 8, 394, 27, 63, 296, 14, 31, 45, 2174, 40, 36, 366, 2330, 861, 46, 11, 88, 16, 22984, 199, 178, 9, 84, 11, 41, 145, 28, 6841, 1050, 46, 11, 62, 63, 213, 7, 60, 11, 41, 28, 6841, 8, 2056, 7, 44, 56, 20, 6394, 941, 9, 243, 350, 8, 78, 2284, 7, 76, 11, 73, 3237, 27, 1360, 7, 4880, 27, 1053, 19, 8778, 7, 8, 696, 1039, 47, 945, 31, 10700, 488, 22, 2558, 25, 366, 2330, 7, 44, 49, 11, 230, 40, 2740, 7, 14, 76, 11, 73, 1258, 8, 16, 4446, 9, 159, 13, 5801, 25, 366, 2330, 7, 207, 45, 604, 34, 248, 13, 5801, 8, 66, 818, 128, 13, 2426, 14, 17, 11, 2677, 9, 69, 213, 27, 19, 538, 8679, 113, 44, 27, 60, 11, 41, 27, 97, 6841, 9, 69, 213, 27, 39, 16, 77, 2338, 267, 7, 398, 27, 39, 495, 30099, 10, 8, 17, 11, 2128, 9, 201, 52, 11, 6305, 27, 66, 198, 72, 7, 46, 11, 62, 49, 11, 105, 33, 40, 8, 577, 9, 877, 91, 30, 1776, 29, 19, 318, 6721, 7, 91, 33, 179, 1182, 13, 1297, 7, 13, 3543, 7, 19, 8640, 10058, 2299, 8, 13, 157, 7, 19, 1014, 7, 16, 4103, 7, 47, 18, 11, 266, 541, 7, 65, 91, 33, 34, 1182, 13, 775, 3522, 9, 4481, 726, 26, 6773, 10, 17, 11, 263, 3611, 7, 34, 39, 9, 683, 45, 39, 227, 254, 7, 14, 997, 60, 11, 41, 22, 5184, 28, 3456, 9, 61, 184, 33, 20, 2922, 29, 16, 164, 7, 109, 279, 16, 203, 8540, 19, 3773, 7, 14, 51, 213, 27, 13, 184, 1516, 2091, 77, 5109, 2330, 24, 1027, 8, 3450, 13, 1931, 323, 147, 241, 7, 47, 24, 1027, 8, 3900, 20, 157, 10, 206, 60, 11, 41, 885, 9, 1696, 13, 29045, 52, 11, 4308, 29, 28, 3475, 25934, 7, 31, 82, 20, 10355, 8, 2879, 7, 51, 213, 27, 13, 184, 7, 79, 18, 11, 266, 7, 1516, 632, 3450, 23, 126, 17, 11, 1243, 24, 1351, 19, 173, 2860, 10, 9, 84, 11, 41, 13, 93, 337, 405, 11, 265, 7, 104, 26, 177, 67, 88, 45, 900, 34, 7, 76, 11, 1612, 27, 165, 26, 29, 16, 29821, 9, 139, 66, 207, 7, 60, 11, 41, 68, 1905, 8, 17, 11, 1354, 7, 60, 11, 41, 15, 175, 8, 116, 511, 8, 222, 63, 22, 6439, 9, 370, 4605, 1358, 11, 62, 102, 33, 20, 1014, 7, 11253, 7, 22, 2971, 14, 207, 1242, 93, 86, 17, 11, 2582, 30, 32, 13, 317, 1454, 7, 51, 213, 27, 63, 25066, 13, 1088, 7, 22, 193, 511, 7, 18, 11, 1655, 3528, 20, 1455, 47, 20, 21507, 18, 11, 266, 256, 31, 56, 241, 26, 2787, 14, 31, 56, 12319, 10, 29, 97, 157, 7, 14, 46, 11, 62, 213, 17510, 95, 736, 9, 683, 400, 28, 6841, 31, 191, 165, 446, 1811, 14, 3899, 7, 65, 31, 191, 99, 3611, 7, 17686, 14, 2041, 35, 9, 160, 11, 2047, 7, 3522, 7, 17, 11, 1139, 27, 19, 538, 750, 1218, 97, 6841, 7, 750, 319, 44, 46, 11, 144, 30, 7, 14, 206, 50, 701, 19, 538, 60, 11, 41, 22, 1033, 405, 11, 265, 19, 5024, 79, 19, 1134, 9, 574, 24, 16, 262, 14, 24, 20, 1819, 21512, 7, 1054, 7, 19, 1134, 45, 56, 34, 13447, 15, 13, 93, 6178, 27, 19, 5024, 7, 65, 564, 9474, 16, 125, 8, 328, 19, 541, 22, 218, 7, 19, 5024, 14, 19, 1134, 519, 32, 16, 93, 942, 7, 2890, 19, 1952, 873, 14, 10735, 7, 19, 1952, 5641, 9, 148, 210, 8, 44, 299, 8, 477, 28, 317, 337, 7, 398, 46, 11, 62, 49, 11, 105, 33, 771, 539, 27, 13, 722, 8, 1582, 20, 1014, 8, 13, 7682, 7, 410, 8, 13, 1923, 8, 166, 2922, 7, 8, 166, 1784, 7, 8, 13, 1297, 7, 8, 13, 9352, 8, 17, 11, 698, 7, 8, 17, 11, 4293, 7, 51, 49, 11, 105, 33, 771, 539, 46, 11, 144, 45, 191, 34, 12138, 35, 8, 13, 93, 429, 37, 19, 1134, 14, 37, 19, 5024, 9, 11823, 304, 6841, 7, 51, 213, 22, 602, 27, 39, 77, 2338, 267, 44, 27, 60, 11, 41, 9, 195, 45, 39, 400, 34, 18, 11, 6604, 7, 47, 95, 126, 9, 195, 39, 400, 8, 894, 444, 28, 553, 9, 84, 11, 41, 28, 553, 18, 11, 1070, 7, 8, 6324, 7, 31, 30, 8, 696, 15, 39, 1027, 18, 11, 12882, 16, 203, 8540, 19, 541, 14, 7, 24, 320, 27, 207, 7255, 7, 8, 19, 19859, 9, 574, 109, 45, 39, 400, 34, 18, 11, 6604, 7, 65, 22, 2447, 109, 39, 3402, 20, 3349, 3830, 7, 14, 45, 19, 161, 26, 12444, 138, 34, 7, 14, 24, 22, 1153, 51, 213, 19, 1218, 9, 1799, 102, 22, 33, 2154, 865, 39, 7, 47, 865, 117, 320, 31, 48, 1442, 263, 405, 11, 265, 24, 13, 553, 18, 11, 1070, 7, 31, 30036, 98, 3788, 9, 84, 11, 41, 23, 2400, 2056, 7, 8708, 52, 11, 62, 22, 30, 9, 4499, 26, 1188, 37, 2962, 96, 394, 8, 319, 404, 17, 11, 88, 82, 86, 17, 11, 88, 604, 52, 11, 7376, 29, 78, 1454, 9, 69, 102, 33, 99, 320, 31, 7, 24, 20, 1819, 2681, 7, 47, 398, 46, 11, 240, 96, 16, 1016, 15, 207, 7, 1882, 48, 8977, 339, 193, 125, 67, 23, 674, 7, 23, 674, 14, 1644, 67, 15, 20, 4338, 7, 50, 7216, 1536, 8975, 10, 36, 437, 1071, 25, 788, 7, 46, 11, 734, 1053, 3788, 47, 46, 11, 734, 1053, 17660, 7, 15, 2091, 97, 1477, 15, 17, 11, 1997, 945, 24, 1891, 15, 20, 10090, 10, 570, 19754, 566, 15, 831, 47, 15, 2748, 7, 24, 3039, 13, 13995, 8, 17, 11, 1623, 7, 24, 4357, 192, 19, 9308, 10, 7, 24, 1950, 29, 20, 5240, 5808, 10, 8, 3450, 13, 548, 20, 242, 14, 20, 72, 10, 7, 50, 500, 15, 13, 12839, 47, 15, 18, 11, 266, 2380, 9, 2425, 7, 51, 102, 22, 33, 1975, 31, 519, 9532, 37, 17, 11, 1139, 8, 8977, 23, 674, 7, 23, 674, 14, 1644, 8, 97, 1016, 7, 8, 97, 11567, 15, 18, 11, 266, 7, 29, 119, 2380, 9356, 47, 3788, 7, 31, 95, 2170, 316, 98, 9690, 20, 2380, 8975, 10, 9, 121, 11, 10634, 27, 24, 474, 7, 44, 210, 99, 13, 1088, 18, 11, 10899, 28, 1005, 7, 14, 610, 23, 2056, 7, 14, 8, 1077, 8, 78, 782, 42, 13, 1088, 47, 13, 1381, 18, 11, 12337, 23, 2056, 46, 11, 240, 2890, 2714, 9, 69, 102, 711, 320, 31, 52, 11, 17815, 15, 17, 11, 2677, 7, 112, 2375, 22, 85, 97, 2056, 9, 71, 11, 2677, 711, 394, 8, 19566, 1350, 7, 217, 175, 46, 11, 1949, 7, 183, 51, 102, 22, 33, 104, 26, 177, 31, 1882, 47, 31, 30036, 7, 398, 27, 207, 19, 7255, 7, 8192, 16, 1858, 8, 2107, 97, 1177, 18, 11, 177, 13476, 29, 44, 27, 17, 11, 88, 2668, 87, 408, 8612, 130, 14, 16, 203, 8540, 18, 11, 177, 19566, 1350, 9, 1684, 45, 920, 34, 3305, 29, 16, 2636, 7, 39, 2222, 194, 13, 553, 24, 22, 3002, 639, 7, 65, 17, 11, 1960, 8, 78, 553, 30, 8, 39, 85, 822, 27, 17, 11, 2677, 30, 17, 11, 70, 20, 3847, 19, 40, 16833, 14, 19, 40, 1840, 8, 13, 6622, 14, 46, 11, 62, 213, 13, 1218, 9, 7242, 91, 33, 23, 126, 1367, 15, 13, 16245, 7, 112, 319, 9, 69, 213, 13, 1218, 398, 46, 11, 144, 82, 20, 541, 5823, 10, 7, 410, 7, 50, 16, 10726, 7, 29, 16, 164, 1558, 32, 16, 379, 8975, 14, 743, 7, 398, 27, 60, 11, 41, 16, 348, 18078, 8, 127, 548, 7, 8, 166, 8922, 7, 8, 166, 2922, 7, 86, 37, 2829, 19, 541, 774, 295, 9, 574, 109, 2705, 18, 11, 177, 17686, 7, 109, 33, 347, 36, 1346, 9, 159, 143, 213, 26, 62, 319, 590, 14, 60, 11, 41, 207, 27, 17, 11, 88, 198, 39, 248, 9, 342, 11, 369, 292, 7, 91, 604, 39, 2913, 19, 4723, 14, 19, 3349, 27, 7, 902, 47, 16765, 7, 78, 6841, 104, 39, 509, 14, 743, 39, 3318, 4880, 56, 19, 3189, 27, 63, 296, 22, 1330, 7, 19, 3189, 8, 3834, 7, 19, 3189, 8, 1346, 8, 17, 11, 698, 7, 8, 77, 9352, 7, 8, 17, 11, 4293, 20, 23, 10, 14, 20, 214, 7, 14, 66, 207, 30, 200, 693, 9, 17125, 1188, 9, 121, 11, 73, 1556, 1975, 2078, 20, 541, 3830, 7, 65, 91, 604, 39, 248, 4880, 56, 140, 873, 14, 99, 4880, 56, 140, 10735, 15, 17, 11, 5764, 8, 13, 15734, 7, 14, 78, 553, 30, 5670, 15, 207, 9, 100, 1282, 46, 11, 144, 191, 24, 39, 4659, 7, 46, 11, 144, 39, 21, 29732, 20, 541, 7, 46, 11, 144, 39, 3402, 20, 7699, 7, 46, 11, 144, 39, 556, 1921, 1582, 8, 140, 873, 65, 99, 8, 140, 10735, 9, 1684, 4592, 22, 10006, 381, 7, 1262, 27, 191, 13, 1454, 27, 39, 15980, 267, 14, 19, 9351, 27, 39, 2912, 267, 8, 78, 553, 7, 50, 39, 1282, 381, 317, 1073, 14, 212, 1601]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_to_feed(df,length):\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  texts = df.Texte.values\n",
        "  labels = df.sexe.values\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  num_truncated_tokens =[]\n",
        "  # Apply function to our corpus\n",
        "  for text in texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # text\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = length,           # We choose for now a max length of 500.\n",
        "                          pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                          return_attention_mask = True,   # Construct attention masks\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          return_overflowing_tokens =True, # return overflowing token information\n",
        "                    )\n",
        "      \n",
        "      # Map tokens to their id in the dictionnary \n",
        "      # We add this to our list    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "  \n",
        "      #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "      \n",
        "      # 3. Attention masks\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # We convert all this into tensors in order to be able to make it work on GPU \n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Original text and transformed tensor print \n",
        "  print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "  print(\" \")\n",
        "  print('Original: ', texts[0])\n",
        "  print('IDs:', input_ids[0])\n",
        "  print('Attention masks:', attention_masks[0])\n",
        "  print('labels',labels[0])\n",
        "\n",
        "\n",
        "  # Combine all above\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Let's create a 80-20 train / validation dataset \n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('We have {} training samples'.format(train_size))\n",
        "  print('We have {} validation samples'.format(val_size))\n",
        "\n",
        "  # We set the size of the batch lower than what is usually set (16 of 32)\n",
        "  batch_size = 16\n",
        "\n",
        "  # We create data loaders for the train and validation dataset. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_set,  # The training samples.\n",
        "              sampler = RandomSampler(train_set), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  val_dataloader = DataLoader(\n",
        "              val_set, # The validation samples.\n",
        "              sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "  \n",
        "  print('Data loaders created for train [0] and val [1]')\n",
        "\n",
        "  return train_dataloader, val_dataloader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepare_to_feed(df_balanced,length=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    }
  ]
}