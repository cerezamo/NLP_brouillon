{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Camembert_v2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPofqvjndNKqRvpBmB7PFW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f8eb756be254a179c8e075c99eace93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f3a30834ae094aefb4c9475980b65ba3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3c69cdc6eafa4816a4bc3545ccac13e9",
              "IPY_MODEL_5362eff3d3ba4379872de2ba6b7aa509"
            ]
          }
        },
        "f3a30834ae094aefb4c9475980b65ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c69cdc6eafa4816a4bc3545ccac13e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_398949bda1cc4864bb2f494d54ec7e57",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 637,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 637,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68fef0e569244a34bd08b71107a55f81"
          }
        },
        "5362eff3d3ba4379872de2ba6b7aa509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cda7972dcecb4f3baab8bbdb98cc4b3b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 637/637 [00:39&lt;00:00, 16.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72e46ac65ba740f5898649a52403d456"
          }
        },
        "398949bda1cc4864bb2f494d54ec7e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68fef0e569244a34bd08b71107a55f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cda7972dcecb4f3baab8bbdb98cc4b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72e46ac65ba740f5898649a52403d456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0a41efa0c9c4668a485aea81a4199e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68194341bf3049c196bed89473f7615c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86615c7d61a04bbaabde2e005b5b4da5",
              "IPY_MODEL_62d2d527bf814112954cb952824aa394"
            ]
          }
        },
        "68194341bf3049c196bed89473f7615c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86615c7d61a04bbaabde2e005b5b4da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90ebbaca818c42178737433696eb3cb2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 445032417,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 445032417,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a89b56ca3cd94d768f356d45390c6d80"
          }
        },
        "62d2d527bf814112954cb952824aa394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92bec7f4ff7d4cc581d081946e689455",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 445M/445M [00:37&lt;00:00, 11.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25fffe2a7bab47adb5e2be72cf7c298a"
          }
        },
        "90ebbaca818c42178737433696eb3cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a89b56ca3cd94d768f356d45390c6d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92bec7f4ff7d4cc581d081946e689455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25fffe2a7bab47adb5e2be72cf7c298a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerezamo/NLP_brouillon/blob/master/Camembert_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# CamemBERT classification model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "6036a7fe-0fe7-4c24-dc7c-f56d62fbaa7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "a28ac54f-5f2c-4812-cbdf-fd9912a543bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%tensorflow_version 1.x\n",
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "db6f994f-5390-48c7-ec59-bfdb4e91bf60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LcRSHffJyZm",
        "colab_type": "text"
      },
      "source": [
        "Please check GPU capacity that you were given. You might want to reduce the batch size further in the code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "68ccb314-5e90-4471-df38-9c0623977365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import medium_df_desq in \"files\" (on the left) => ICI prendre du github ??\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df=pd.read_csv('medium_df_deseq.csv',encoding='utf-8')\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgllpcqJevp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make results reproducible \n",
        "seed_val = 2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmpCJoY-pK0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f44e87cb-cacb-4a69-cd6f-243106e9f3d0"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Id', 'Titre', 'Type', 'Theme', 'Prenom', 'Nom',\n",
              "       'Fonction', 'Date', 'Tags', 'Texte', 'Lien', 'PRENOM', 'preusuel',\n",
              "       'sexe'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8ICVGvUBLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unbalanced_preprocess(df,seed_val,frac_val):\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #Shuffle the data \n",
        "  df_unbalanced=df.sample(frac=frac_val).reset_index()\n",
        "\n",
        "  # Reduce to the variables we are interested in \n",
        "  df_unbalanced=df_unbalanced[['Texte','sexe']]\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in the unbalanced corpus : {:,}\\n'.format(df_unbalanced.shape[0]))\n",
        "  prop = (len(df_unbalanced[df_unbalanced.sexe==1])/len(df_unbalanced))*100\n",
        "  print('Proportions of women in the unbalanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_unbalanced"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IdYJSSRUdD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3f09ae59-6ac2-43a9-bfa4-bc7ffd2799a7"
      },
      "source": [
        "df_unbalanced = unbalanced_preprocess(df,seed_val,frac_val=0.5)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in the unbalanced corpus : 2,500\n",
            "\n",
            "Proportions of women in the unbalanced corpus : 25.319999999999997\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd4SCY6LUuwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_preprocess(df,seed_val,frac_val):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Let's take a balanced sample \n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  #Shuffle the data and taking half of the sample in order not to have to many data compared to the other samples \n",
        "  df_balanced=df.sample(frac=1,random_state=seed_val).reset_index()\n",
        "\n",
        "  # Reduce to the variables we are interested in \n",
        "  df_balanced=df_balanced[['Texte','sexe']]\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this corpus : {:,}\\n'.format(df_balanced.shape[0]))\n",
        "  prop = (len(df_balanced[df_balanced.sexe==1])/len(df_balanced))*100\n",
        "  print('Proportions of women in the balanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_balanced\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeQif_N0X4uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "69e0b0ab-6aad-4bf4-dc2f-b2149d6cf44c"
      },
      "source": [
        "df_balanced = balanced_preprocess(df,seed_val,frac_val=1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this corpus : 2,500\n",
            "\n",
            "Proportions of women in the balanced corpus : 50.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E85CuAStX_qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_splitted(df,seed_val,frac_val):\n",
        "    # Let's take a balanced sample \n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  #We shuffle the data\n",
        "  df=df.sample(frac=1,random_state=seed_val).reset_index()\n",
        "\n",
        "  from itertools import repeat\n",
        "  n=2500\n",
        "  chunks, label_split,index_df=[],[],[]\n",
        "  j=0\n",
        "  for text in df.Texte :\n",
        "      txt=[text[i:i+n] for i in range(0, len(text), n)]\n",
        "      chunks.append(txt)\n",
        "      label_split.extend(repeat(df.sexe[j], len(txt)))\n",
        "      # We keep track of the text Id from the first database\n",
        "      index_df.extend(repeat(df.Id[j], len(txt)))\n",
        "      j+=1\n",
        "\n",
        "  chunks = [item for sublist in chunks for item in sublist]\n",
        "  df=pd.DataFrame([chunks,label_split,index_df]).transpose()\n",
        "  df.columns=['Texte','sexe','index_df']\n",
        "  df['sexe'] = df['sexe'].astype(int)\n",
        "\n",
        "  df=df.sample(frac=frac_val,random_state=seed_val).reset_index()\n",
        "\n",
        "  df_balanced_split= df\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this balanced splitted corpus : {:,}\\n'.format(df_balanced_split.shape[0]))\n",
        "  prop = (len(df_balanced_split[df_balanced_split.sexe==1])/len(df_balanced_split))*100\n",
        "  print('Proportions of women in the balanced splitted corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_balanced_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEpbT1JYf5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "be1aa4a2-2472-4f5e-8bdf-783a7af5f693"
      },
      "source": [
        "df_balanced_split = balanced_splitted(df,seed_val,frac_val=0.2)[['Texte','sexe']]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 2,528\n",
            "\n",
            "Proportions of women in the balanced splitted corpus : 52.76898734177215\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTKQ0QnAZ_wu",
        "colab_type": "text"
      },
      "source": [
        "**We propose 3 samples to train our model :**\n",
        "\n",
        "\n",
        "1.   **Unbalanced sample**\n",
        "\n",
        "We take the raw data without any further treatment.\n",
        "\n",
        "2.   **Balanced sample**\n",
        "\n",
        "The second option consist in deleting randomly part of male speeches in order to get a balanced sample. Indeed, in the case of unbalanced sample our model could decide to classify all speakers in the male category which would lead to a 0.75 accuracy in our case study. In order to avoid this we feed the model with the same proportions of male and female speakers. Other kind of treatments exist to deal with unbalanced sample. This one is the simpliest one and we could argue that there is a possibility that the deleted sample contains important information that we therefore miss. However we believe that in our case this is not a big issue. Our unbalanced sample is quite large for both female and male.\n",
        "\n",
        "3. **Balanced and splitted sample**\n",
        "\n",
        "The third option is a response to the max length constraint of BERT models. Our text samples are big and contain much more tokens than the 512 limit. In the first two options we decide to just feed the model with the 512 first tokens and thus delete the rest of them. In this third option we cut the text into x parts containing 500 tokens each. All parts of the speech will serve to feed the model. By this technique we do not loose potential important informations at the end of the text. A lot of other techniques have been employed (see ref !!! PUT). We decide to stick to this one in this project. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mggkz5R9g8dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoaZUUBChM_J",
        "colab_type": "code",
        "outputId": "60943deb-5162-4684-84da-ef1abfec979e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Messieurs,Je suis heureux de vous saluer. Quand je dis que je suis heureux de vous saluer, ce n'est pas une simple affirmation de politesse. Je le disais à l'instant à Monsieur le Ministre de la Défense, M. Richard, c'est pour moi un instant où il y a un peu d'émotion; je vais vous dire pourquoi.Vous êtes la première classe d'âge qui ne fera pas de service militaire. C'est une décision que j'ai prise, il y a deux ans, après une vraie réflexion et un vrai débat. Après tout, le service militaire c'est une vieille tradition nationale, il était plus que centenaire. Il y avait toutes sortes de raisons à cela, notamment la nécessité d'avoir une armée nombreuse et donc d'avoir des jeunes formés aux combats, à l'utilisation des armes de l'époque.On pouvait s'interroger sur la nécessité de poursuivre dans cette voie. Il y avait naturellement des critiques, il y avait beaucoup de jeunes qui se disaient qu'ils perdaient un peu leur temps, d'autres qui étaient satisfaits. Mais il y avait surtout ceux qui, dans notre pays, disaient: le service militaire c'est nécessaire pour la cohésion nationale, c'est un moyen de mettre ensemble les garçons issus de milieux et d'origines géographiques, culturelles, sociales, différents et de faire en sorte qu'ils se connaissent, qu'ils soient ensemble, et ce que l'on appelait la fraternité des casernes était considérée par beaucoup comme un élément important de la cohésion nationale. Cet argument était celui de beaucoup de conservateurs. Les conservateurs ont toujours des arguments forts, il faut en tenir compte. D'autre part, il m'était apparu que tout cela, au fond, appartenait un peu à une conception dépassée de la défense, et faisait peser au total sur les jeunes une contrainte obligatoire et excessive qui n'était pas justifiée, ou qui ne l'était plus.L'armée moderne n'est naturellement pas celle dont on avait besoin en 1914 ou 1939, ou même après, au moment des guerres de décolonisation en Algérie ou autres. Tout cela justifiait que l'on passe 1 an ou 2, voire plus, de service militaire, - parfois 3 ans au début du siècle, 2 ans et demi pendant la période de la guerre d'Algérie pour certains -.L'évolution des choses mettait cela en cause. L'évolution, c'était d'abord l'installation de la paix en Europe. Et cela a été une grande réforme, à partir du moment ou l'on a lancé la construction européenne: nous n'avons plus été menacés sur nos frontières. Il fut un temps où nous devions pouvoir opposer un barrage de poitrines aux poitrines allemandes qui risquaient de s'avancer chez nous, il fallait encore beaucoup d'hommes. La construction européenne a fait disparaître cette menace. Nous ne sommes plus menacés à nos frontières: on n'imagine pas l'Allemagne attaquant la France, on ne l'image plus. Donc, en toute hypothèse, la menace s'était éloignée. Il y avait aussi, naturellement, l'armement qui avait beaucoup changé, les techniques militaires qui s'étaient profondément transformées. Aujourd'hui, une opération militaire, la guerre le cas échéant, mobilise des hommes et des femmes qui ont une très haute compétence, qui sont aptes à utiliser des moyens extraordinairement sophistiqués, qui sont totalement disponibles et prêts à faire mouvement immédiatement, et qui n'ont besoin d'aucune formation préalable pour le faire.C'est donc un autre type de besoin que nous avons et qui ne correspond plus au service militaire tel qu'on le concevait avant. C'est donc une armée professionnelle qu'il nous faut, c'est une armée de métier, ce sont des vrais professionnels.A partir de cette réflexion, j'ai pensé que finalement, quels que soient les arguments, de nature sociale ou nationale qui plaidaient en faveur du service militaire, ce n'était plus adapté, et j'ai décidé de le supprimer. Mais la suppression du service militaire, cela ne veut pas dire la suppression de tout lien entre la jeunesse et l'armée. Il faut que les jeunes sachent ce que c'est que leur armée. Il faut que vous le sachiez, parce que vous êtes dépositaires de l'avenir. On s'imagine que tout va bien, qu'il n'y a plus de problème. Quand on est né dans les années 1980, on a toujours connu la liberté, la démocratie, les embêtements de la vie, les problèmes, le chômage, ou d'autres choses, mais on a pas connu la guerre naturellement. Vos grands-pères l'ont connue, pas vous. Cela ne vous dit rien, et pourtant c'est en permanence une menace. La France a des intérêts dans le monde, elle doit le cas échéant les défendre, et il faut que la France puisse apporter sa contribution militaire pour permettre de maintenir la paix ici où là, ou pour permettre de sauver des vies quand c'est nécessaire. Lorsque la Bosnie s'engage dans une opération suicidaire, qui fait des quantités de morts, il faut que la France, comme d'autres, puisse aller maintenir un peu l'ordre pour éviter les carnages. C'est la même chose aujourd'hui, peut-être -on ne sait pas, j'espère que non- dans le Kosovo. Et tout cela, c'est aux portes de l'Europe, c'est à moins de deux heures de chez nous en avion.Lorsqu'il y a des problèmes, hélas, en Afrique et cela arrive même si l'Afrique est sur la bonne voie, il faut que nous ayons la possibilité, en quelques heures, d'aller récupérer des Français ou des ressortissants d'autres pays qui sont là-bas et qui sont menacés dans leur vie, et qu'il faut extraire très rapidement. Cela demande une armée qui soit non seulement capable et équipée, mais qui soit aussi connue, respectée et aimée. D'où, naturellement, l'idée que les jeunes doivent connaître leur armée, doivent savoir ce qu'elle est, et quand je dis les jeunes c'est en réalité aujourd'hui les garçons comme les filles. Alors pour le moment et pour des raisons matérielles, techniques, les filles ne sont pas soumises à la même obligation que les garçons, mais dès 2000, le temps de mettre les choses en place, les garçons et les filles seront sur le même pied, auront les mêmes droits et devoirs, les mêmes obligations. Ce sera de ce point de vue une bonne chose, parce qu'il n'y a aucune raison que la prise de conscience des problèmes de la Défense, notamment de la défense de nos intérêts, de nos valeurs, de la liberté, de la dignité de l'homme, de l'égalité, il n'y a aucune raison qu'elle ne soit pas défendue de la même façon par les filles et par les garçons.Cette nouvelle armée, il faut en gros que vous sachiez ce que c'est. Elle ne vous demande pas d'obligation, ou très peu. Elle vous demande de venir passer une journée. C'est une journée d'information, de mobilisation, qui est de nature à vous permettre d'apprécier le cas échéant les choses et, pour ceux que cela intéresse, de les approfondir. Alors elle ne vous demande pas d'obligation, mais en revanche elle vous ouvre des possibilités importantes, et ne les sous-estimez pas, et pour en profiter il faut les connaître.Il y en a probablement parmi vous, ou parmi tous ceux qui se présenteront aujourd'hui pour la journée d'information, qui voudront être militaires. C'est un superbe métier, noble s'il en est. Ceux-là par conséquent ont besoin de savoir comment l'on fait si l'on veut s'engager dans cette voie. Il y a aussi ceux qui, pour des raisons personnelles, ou parce qu'ils ont le coeur à cela, veulent se consacrer pendant quelques temps - un an, un an et demi - à des tâches, je dirais généralement humanitaires au sens large du terme, qu'elles soient militaires ou qu'elles soient civiles, à apporter leur concours à l'éducation nationale pour apprendre à des gamins défavorisés à lire ou à écrire, pour faciliter la préservation de l'environnement, pour lutter contre les incendies, pour servir dans des unités chargées de maintenir la sécurité des personnes et des biens, je pense à la gendarmerie ou à d'autres domaines. Bref, il y en a certainement qui seront intéressés par l'idée de consacrer un an, un an et demi de leur coeur, de leur générosité à d'autres, dans ces domaines civils ou militaires, qui très largement peuvent être appelés des domaines humanitaires. J'ajoute que pour eux, ce sera aussi la possibilité d'acquérir une expérience, et parfois un métier, et de sortir de cette période avec la possibilité ou la capacité d'exercer un métier qu'ils auront appris. Il y aura ceux qui s'intéressent à l'armée, sans vouloir en faire leur métier. L'armée aura besoin de réservistes, beaucoup moins qu'avant, alors il y en a peut-être qui veulent ou qui voudront, parce que cela les intéresse, acquérir le minimum de connaissances leur permettant d'être versé dans ce que l'on appelle \"les réserves\" et le cas échéant d'être réservistes.Je ne veux pas entrer dans le détail, vous aurez toute la journée pour en entendre parler, mais l'objectif de cette journée est de vous faire comprendre que l'armée est l'une des institutions les plus nobles et les plus nécessaires de la nation et qu'il faut la connaître. Parfois on a un peu tendance à la critiquer, sans savoir. Il faut la connaître parce qu'elle fait des choses formidables, notamment, je le répète, dans le monde moderne sur le plan humanitaire et enfin, parce que c'est le dernier rempart de notre sécurité, de nos libertés, de nos intérêts, si par hasard les choses vont mal. Alors elle mérite d'être respectée, elle a droit au respect. Mais encore faut-il savoir pourquoi et c'est cela que l'on va vous dire.D'autre part, on veut vous ouvrir les voies et les possibilités que, directement ou indirectement, cette armée peut vous donner et enfin vous rappeler quels sont les principes que nous avons en commun, les principes de solidarité, les principes de respect de l'homme, de sa dignité, de l'égalité des uns et des autres, et tout cela est également important.Voilà. J'ai dû certainement oublier des choses importantes, mais on veut vous dire quels sont vos droits et aussi quels sont vos devoirs à l'égard de la Nation, et cette journée est consacrée à cela. Je souhaite qu'elle soit pour vous intéressante, qu'elle vous apprenne des choses, qu'elle vous ouvre des perspectives, qu'elle vous donne davantage conscience de vos droits mais aussi de vos devoirs.Je termine en souhaitant surtout, quelle que soit la voie que vous emprunterez et les conclusions que vous tirerez de cette journée, je vous souhaite surtout bonne chance et bon vent\n",
            "Tokenized:  ['▁Messieurs', ',', 'Je', '▁suis', '▁heureux', '▁de', '▁vous', '▁saluer', '.', '▁Quand', '▁je', '▁dis', '▁que', '▁je', '▁suis', '▁heureux', '▁de', '▁vous', '▁saluer', ',', '▁ce', '▁n', \"'\", 'est', '▁pas', '▁une', '▁simple', '▁affirmation', '▁de', '▁politesse', '.', '▁Je', '▁le', '▁disais', '▁à', '▁l', \"'\", 'instant', '▁à', '▁Monsieur', '▁le', '▁Ministre', '▁de', '▁la', '▁Défense', ',', '▁M', '.', '▁Richard', ',', '▁c', \"'\", 'est', '▁pour', '▁moi', '▁un', '▁instant', '▁où', '▁il', '▁y', '▁a', '▁un', '▁peu', '▁d', \"'\", 'émotion', ';', '▁je', '▁vais', '▁vous', '▁dire', '▁pourquoi', '.', 'Vous', '▁êtes', '▁la', '▁première', '▁classe', '▁d', \"'\", 'âge', '▁qui', '▁ne', '▁fera', '▁pas', '▁de', '▁service', '▁militaire', '.', '▁C', \"'\", 'est', '▁une', '▁décision', '▁que', '▁j', \"'\", 'ai', '▁prise', ',', '▁il', '▁y', '▁a', '▁deux', '▁ans', ',', '▁après', '▁une', '▁vraie', '▁réflexion', '▁et', '▁un', '▁vrai', '▁débat', '.', '▁Après', '▁tout', ',', '▁le', '▁service', '▁militaire', '▁c', \"'\", 'est', '▁une', '▁vieille', '▁tradition', '▁nationale', ',', '▁il', '▁était', '▁plus', '▁que', '▁centenaire', '.', '▁Il', '▁y', '▁avait', '▁toutes', '▁sortes', '▁de', '▁raisons', '▁à', '▁cela', ',', '▁notamment', '▁la', '▁nécessité', '▁d', \"'\", 'avoir', '▁une', '▁armée', '▁nombreuse', '▁et', '▁donc', '▁d', \"'\", 'avoir', '▁des', '▁jeunes', '▁formés', '▁aux', '▁combats', ',', '▁à', '▁l', \"'\", 'utilisation', '▁des', '▁armes', '▁de', '▁l', \"'\", 'époque', '.', 'On', '▁pouvait', '▁s', \"'\", 'interroger', '▁sur', '▁la', '▁nécessité', '▁de', '▁poursuivre', '▁dans', '▁cette', '▁voie', '.', '▁Il', '▁y', '▁avait', '▁naturellement', '▁des', '▁critiques', ',', '▁il', '▁y', '▁avait', '▁beaucoup', '▁de', '▁jeunes', '▁qui', '▁se', '▁disaient', '▁qu', \"'\", 'ils', '▁perd', 'aient', '▁un', '▁peu', '▁leur', '▁temps', ',', '▁d', \"'\", 'autres', '▁qui', '▁étaient', '▁satisfaits', '.', '▁Mais', '▁il', '▁y', '▁avait', '▁surtout', '▁ceux', '▁qui', ',', '▁dans', '▁notre', '▁pays', ',', '▁disaient', ':', '▁le', '▁service', '▁militaire', '▁c', \"'\", 'est', '▁nécessaire', '▁pour', '▁la', '▁cohésion', '▁nationale', ',', '▁c', \"'\", 'est', '▁un', '▁moyen', '▁de', '▁mettre', '▁ensemble', '▁les', '▁garçons', '▁issus', '▁de', '▁milieux', '▁et', '▁d', \"'\", 'origine', 's', '▁géographiques', ',', '▁culturelles', ',', '▁sociales', ',', '▁différents', '▁et', '▁de', '▁faire', '▁en', '▁sorte', '▁qu', \"'\", 'ils', '▁se', '▁connaissent', ',', '▁qu', \"'\", 'ils', '▁soient', '▁ensemble', ',', '▁et', '▁ce', '▁que', '▁l', \"'\", 'on', '▁appelait', '▁la', '▁fraternité', '▁des', '▁caserne', 's', '▁était', '▁considérée', '▁par', '▁beaucoup', '▁comme', '▁un', '▁élément', '▁important', '▁de', '▁la', '▁cohésion', '▁nationale', '.', '▁Cet', '▁argument', '▁était', '▁celui', '▁de', '▁beaucoup', '▁de', '▁conservateurs', '.', '▁Les', '▁conservateurs', '▁ont', '▁toujours', '▁des', '▁arguments', '▁forts', ',', '▁il', '▁faut', '▁en', '▁tenir', '▁compte', '.', '▁D', \"'\", 'autre', '▁part', ',', '▁il', '▁m', \"'\", 'était', '▁apparu', '▁que', '▁tout', '▁cela', ',', '▁au', '▁fond', ',', '▁appartenait', '▁un', '▁peu', '▁à', '▁une', '▁conception', '▁dépassé', 'e', '▁de', '▁la', '▁défense', ',', '▁et', '▁faisait', '▁peser', '▁au', '▁total', '▁sur', '▁les', '▁jeunes', '▁une', '▁contrainte', '▁obligatoire', '▁et', '▁excessive', '▁qui', '▁n', \"'\", 'était', '▁pas', '▁justifiée', ',', '▁ou', '▁qui', '▁ne', '▁l', \"'\", 'était', '▁plus', '.', 'L', \"'\", 'armée', '▁moderne', '▁n', \"'\", 'est', '▁naturellement', '▁pas', '▁celle', '▁dont', '▁on', '▁avait', '▁besoin', '▁en', '▁1914', '▁ou', '▁1939', ',', '▁ou', '▁même', '▁après', ',', '▁au', '▁moment', '▁des', '▁guerres', '▁de', '▁déco', 'lon', 'isation', '▁en', '▁Algérie', '▁ou', '▁autres', '.', '▁Tout', '▁cela', '▁just', 'ifi', 'ait', '▁que', '▁l', \"'\", 'on', '▁passe', '▁1', '▁an', '▁ou', '▁2', ',', '▁voire', '▁plus', ',', '▁de', '▁service', '▁militaire', ',', '▁-', '▁parfois', '▁3', '▁ans', '▁au', '▁début', '▁du', '▁siècle', ',', '▁2', '▁ans', '▁et', '▁demi', '▁pendant', '▁la', '▁période', '▁de', '▁la', '▁guerre', '▁d', \"'\", 'Algérie', '▁pour', '▁certains', '▁-', '.', 'L', \"'\", 'évolution', '▁des', '▁choses', '▁mettait', '▁cela', '▁en', '▁cause', '.', '▁L', \"'\", 'évolution', ',', '▁c', \"'\", 'était', '▁d', \"'\", 'abord', '▁l', \"'\", 'installation', '▁de', '▁la', '▁paix', '▁en', '▁Europe', '.', '▁Et', '▁cela', '▁a', '▁été', '▁une', '▁grande', '▁réforme', ',', '▁à', '▁partir', '▁du', '▁moment', '▁ou', '▁l', \"'\", 'on', '▁a', '▁lancé', '▁la', '▁construction', '▁européenne', ':', '▁nous', '▁n', \"'\", 'avons', '▁plus', '▁été', '▁menacé', 's', '▁sur', '▁nos', '▁frontières', '.', '▁Il', '▁fut', '▁un', '▁temps', '▁où', '▁nous', '▁devions', '▁pouvoir', '▁opposer', '▁un', '▁barrage', '▁de', '▁poitrine', 's', '▁aux', '▁poitrine', 's', '▁allemandes', '▁qui', '▁risqu', 'aient', '▁de', '▁s', \"'\", 'avancer', '▁chez', '▁nous', ',', '▁il', '▁fallait', '▁encore', '▁beaucoup', '▁d', \"'\", 'hommes', '.', '▁La', '▁construction', '▁européenne', '▁a', '▁fait', '▁disparaître', '▁cette', '▁menace', '.', '▁Nous', '▁ne', '▁sommes', '▁plus', '▁menacé', 's', '▁à', '▁nos', '▁frontières', ':', '▁on', '▁n', \"'\", 'imagine', '▁pas', '▁l', \"'\", 'Allemagne', '▁attaquant', '▁la', '▁France', ',', '▁on', '▁ne', '▁l', \"'\", 'image', '▁plus', '.', '▁Donc', ',', '▁en', '▁toute', '▁hypothèse', ',', '▁la', '▁menace', '▁s', \"'\", 'était', '▁éloignée', '.', '▁Il', '▁y', '▁avait', '▁aussi', ',', '▁naturellement', ',', '▁l', \"'\", 'armement', '▁qui', '▁avait', '▁beaucoup', '▁changé', ',', '▁les', '▁techniques', '▁militaires', '▁qui', '▁s', \"'\", 'étaient', '▁profondément', '▁transformée', 's', '.', '▁Aujourd', \"'\", 'hui', ',', '▁une', '▁opération', '▁militaire', ',', '▁la', '▁guerre', '▁le', '▁cas', '▁échéant', ',', '▁mobilise', '▁des', '▁hommes', '▁et', '▁des', '▁femmes', '▁qui', '▁ont', '▁une', '▁très', '▁haute', '▁compétence', ',', '▁qui', '▁sont', '▁apte', 's', '▁à', '▁utiliser', '▁des', '▁moyens', '▁extraordinaire', 'ment', '▁sophistiqué', 's', ',', '▁qui', '▁sont', '▁totalement', '▁disponibles', '▁et', '▁prêts', '▁à', '▁faire', '▁mouvement', '▁immédiatement', ',', '▁et', '▁qui', '▁n', \"'\", 'ont', '▁besoin', '▁d', \"'\", 'aucune', '▁formation', '▁préalable', '▁pour', '▁le', '▁faire', '.', 'C', \"'\", 'est', '▁donc', '▁un', '▁autre', '▁type', '▁de', '▁besoin', '▁que', '▁nous', '▁avons', '▁et', '▁qui', '▁ne', '▁correspond', '▁plus', '▁au', '▁service', '▁militaire', '▁tel', '▁qu', \"'\", 'on', '▁le', '▁concev', 'ait', '▁avant', '.', '▁C', \"'\", 'est', '▁donc', '▁une', '▁armée', '▁professionnelle', '▁qu', \"'\", 'il', '▁nous', '▁faut', ',', '▁c', \"'\", 'est', '▁une', '▁armée', '▁de', '▁métier', ',', '▁ce', '▁sont', '▁des', '▁vrais', '▁professionnels', '.', 'A', '▁partir', '▁de', '▁cette', '▁réflexion', ',', '▁j', \"'\", 'ai', '▁pensé', '▁que', '▁finalement', ',', '▁quels', '▁que', '▁soient', '▁les', '▁arguments', ',', '▁de', '▁nature', '▁sociale', '▁ou', '▁nationale', '▁qui', '▁plaid', 'aient', '▁en', '▁faveur', '▁du', '▁service', '▁militaire', ',', '▁ce', '▁n', \"'\", 'était', '▁plus', '▁adapté', ',', '▁et', '▁j', \"'\", 'ai', '▁décidé', '▁de', '▁le', '▁supprimer', '.', '▁Mais', '▁la', '▁suppression', '▁du', '▁service', '▁militaire', ',', '▁cela', '▁ne', '▁veut', '▁pas', '▁dire', '▁la', '▁suppression', '▁de', '▁tout', '▁lien', '▁entre', '▁la', '▁jeunesse', '▁et', '▁l', \"'\", 'armée', '.', '▁Il', '▁faut', '▁que', '▁les', '▁jeunes', '▁sache', 'nt', '▁ce', '▁que', '▁c', \"'\", 'est', '▁que', '▁leur', '▁armée', '.', '▁Il', '▁faut', '▁que', '▁vous', '▁le', '▁sa', 'chi', 'ez', ',', '▁parce', '▁que', '▁vous', '▁êtes', '▁dépositaire', 's', '▁de', '▁l', \"'\", 'avenir', '.', '▁On', '▁s', \"'\", 'imagine', '▁que', '▁tout', '▁va', '▁bien', ',', '▁qu', \"'\", 'il', '▁n', \"'\", 'y', '▁a', '▁plus', '▁de', '▁problème', '.', '▁Quand', '▁on', '▁est', '▁né', '▁dans', '▁les', '▁années', '▁1980', ',', '▁on', '▁a', '▁toujours', '▁connu', '▁la', '▁liberté', ',', '▁la', '▁démocratie', ',', '▁les', '▁emb', 'ête', 'ments', '▁de', '▁la', '▁vie', ',', '▁les', '▁problèmes', ',', '▁le', '▁chômage', ',', '▁ou', '▁d', \"'\", 'autres', '▁choses', ',', '▁mais', '▁on', '▁a', '▁pas', '▁connu', '▁la', '▁guerre', '▁naturellement', '.', '▁Vos', '▁grands', '-', 'père', 's', '▁l', \"'\", 'ont', '▁connue', ',', '▁pas', '▁vous', '.', '▁Cela', '▁ne', '▁vous', '▁dit', '▁rien', ',', '▁et', '▁pourtant', '▁c', \"'\", 'est', '▁en', '▁permanence', '▁une', '▁menace', '.', '▁La', '▁France', '▁a', '▁des', '▁intérêts', '▁dans', '▁le', '▁monde', ',', '▁elle', '▁doit', '▁le', '▁cas', '▁échéant', '▁les', '▁défendre', ',', '▁et', '▁il', '▁faut', '▁que', '▁la', '▁France', '▁puisse', '▁apporter', '▁sa', '▁contribution', '▁militaire', '▁pour', '▁permettre', '▁de', '▁maintenir', '▁la', '▁paix', '▁ici', '▁où', '▁là', ',', '▁ou', '▁pour', '▁permettre', '▁de', '▁sauver', '▁des', '▁vie', 's', '▁quand', '▁c', \"'\", 'est', '▁nécessaire', '.', '▁Lorsque', '▁la', '▁Bosnie', '▁s', \"'\", 'engage', '▁dans', '▁une', '▁opération', '▁suicidaire', ',', '▁qui', '▁fait', '▁des', '▁quantités', '▁de', '▁morts', ',', '▁il', '▁faut', '▁que', '▁la', '▁France', ',', '▁comme', '▁d', \"'\", 'autres', ',', '▁puisse', '▁aller', '▁maintenir', '▁un', '▁peu', '▁l', \"'\", 'ordre', '▁pour', '▁éviter', '▁les', '▁car', 'nage', 's', '.', '▁C', \"'\", 'est', '▁la', '▁même', '▁chose', '▁aujourd', \"'\", 'hui', ',', '▁peut', '-', 'être', '▁-', 'on', '▁ne', '▁sait', '▁pas', ',', '▁j', \"'\", 'espère', '▁que', '▁non', '-', '▁dans', '▁le', '▁Kosovo', '.', '▁Et', '▁tout', '▁cela', ',', '▁c', \"'\", 'est', '▁aux', '▁portes', '▁de', '▁l', \"'\", 'Europe', ',', '▁c', \"'\", 'est', '▁à', '▁moins', '▁de', '▁deux', '▁heures', '▁de', '▁chez', '▁nous', '▁en', '▁avion', '.', 'L', 'ors', 'qu', \"'\", 'il', '▁y', '▁a', '▁des', '▁problèmes', ',', '▁hélas', ',', '▁en', '▁Afrique', '▁et', '▁cela', '▁arrive', '▁même', '▁si', '▁l', \"'\", 'Afrique', '▁est', '▁sur', '▁la', '▁bonne', '▁voie', ',', '▁il', '▁faut', '▁que', '▁nous', '▁ayons', '▁la', '▁possibilité', ',', '▁en', '▁quelques', '▁heures', ',', '▁d', \"'\", 'aller', '▁récupérer', '▁des', '▁Français', '▁ou', '▁des', '▁ressortissants', '▁d', \"'\", 'autres', '▁pays', '▁qui', '▁sont', '▁là', '-', 'bas', '▁et', '▁qui', '▁sont', '▁menacé', 's', '▁dans', '▁leur', '▁vie', ',', '▁et', '▁qu', \"'\", 'il', '▁faut', '▁extraire', '▁très', '▁rapidement', '.', '▁Cela', '▁demande', '▁une', '▁armée', '▁qui', '▁soit', '▁non', '▁seulement', '▁capable', '▁et', '▁équipée', ',', '▁mais', '▁qui', '▁soit', '▁aussi', '▁connue', ',', '▁respectée', '▁et', '▁aimé', 'e', '.', '▁D', \"'\", 'où', ',', '▁naturellement', ',', '▁l', \"'\", 'idée', '▁que', '▁les', '▁jeunes', '▁doivent', '▁connaître', '▁leur', '▁armée', ',', '▁doivent', '▁savoir', '▁ce', '▁qu', \"'\", 'elle', '▁est', ',', '▁et', '▁quand', '▁je', '▁dis', '▁les', '▁jeunes', '▁c', \"'\", 'est', '▁en', '▁réalité', '▁aujourd', \"'\", 'hui', '▁les', '▁garçons', '▁comme', '▁les', '▁filles', '.', '▁Alors', '▁pour', '▁le', '▁moment', '▁et', '▁pour', '▁des', '▁raisons', '▁matérielles', ',', '▁techniques', ',', '▁les', '▁filles', '▁ne', '▁sont', '▁pas', '▁soumises', '▁à', '▁la', '▁même', '▁obligation', '▁que', '▁les', '▁garçons', ',', '▁mais', '▁dès', '▁2000,', '▁le', '▁temps', '▁de', '▁mettre', '▁les', '▁choses', '▁en', '▁place', ',', '▁les', '▁garçons', '▁et', '▁les', '▁filles', '▁seront', '▁sur', '▁le', '▁même', '▁pied', ',', '▁auront', '▁les', '▁mêmes', '▁droits', '▁et', '▁devoirs', ',', '▁les', '▁mêmes', '▁obligations', '.', '▁Ce', '▁sera', '▁de', '▁ce', '▁point', '▁de', '▁vue', '▁une', '▁bonne', '▁chose', ',', '▁parce', '▁qu', \"'\", 'il', '▁n', \"'\", 'y', '▁a', '▁aucune', '▁raison', '▁que', '▁la', '▁prise', '▁de', '▁conscience', '▁des', '▁problèmes', '▁de', '▁la', '▁Défense', ',', '▁notamment', '▁de', '▁la', '▁défense', '▁de', '▁nos', '▁intérêts', ',', '▁de', '▁nos', '▁valeurs', ',', '▁de', '▁la', '▁liberté', ',', '▁de', '▁la', '▁dignité', '▁de', '▁l', \"'\", 'homme', ',', '▁de', '▁l', \"'\", 'égalité', ',', '▁il', '▁n', \"'\", 'y', '▁a', '▁aucune', '▁raison', '▁qu', \"'\", 'elle', '▁ne', '▁soit', '▁pas', '▁défendu', 'e', '▁de', '▁la', '▁même', '▁façon', '▁par', '▁les', '▁filles', '▁et', '▁par', '▁les', '▁garçons', '.', 'Cette', '▁nouvelle', '▁armée', ',', '▁il', '▁faut', '▁en', '▁gros', '▁que', '▁vous', '▁sa', 'chi', 'ez', '▁ce', '▁que', '▁c', \"'\", 'est', '.', '▁Elle', '▁ne', '▁vous', '▁demande', '▁pas', '▁d', \"'\", 'obligation', ',', '▁ou', '▁très', '▁peu', '.', '▁Elle', '▁vous', '▁demande', '▁de', '▁venir', '▁passer', '▁une', '▁journée', '.', '▁C', \"'\", 'est', '▁une', '▁journée', '▁d', \"'\", 'information', ',', '▁de', '▁mobilisation', ',', '▁qui', '▁est', '▁de', '▁nature', '▁à', '▁vous', '▁permettre', '▁d', \"'\", 'apprécier', '▁le', '▁cas', '▁échéant', '▁les', '▁choses', '▁et', ',', '▁pour', '▁ceux', '▁que', '▁cela', '▁intéresse', ',', '▁de', '▁les', '▁approfondir', '.', '▁Alors', '▁elle', '▁ne', '▁vous', '▁demande', '▁pas', '▁d', \"'\", 'obligation', ',', '▁mais', '▁en', '▁revanche', '▁elle', '▁vous', '▁ouvre', '▁des', '▁possibilités', '▁importantes', ',', '▁et', '▁ne', '▁les', '▁sous', '-', 'estime', 'z', '▁pas', ',', '▁et', '▁pour', '▁en', '▁profiter', '▁il', '▁faut', '▁les', '▁connaître', '.', 'Il', '▁y', '▁en', '▁a', '▁probablement', '▁parmi', '▁vous', ',', '▁ou', '▁parmi', '▁tous', '▁ceux', '▁qui', '▁se', '▁présenter', 'ont', '▁aujourd', \"'\", 'hui', '▁pour', '▁la', '▁journée', '▁d', \"'\", 'information', ',', '▁qui', '▁voudront', '▁être', '▁militaires', '.', '▁C', \"'\", 'est', '▁un', '▁superbe', '▁métier', ',', '▁noble', '▁s', \"'\", 'il', '▁en', '▁est', '.', '▁Ceux', '-', 'là', '▁par', '▁conséquent', '▁ont', '▁besoin', '▁de', '▁savoir', '▁comment', '▁l', \"'\", 'on', '▁fait', '▁si', '▁l', \"'\", 'on', '▁veut', '▁s', \"'\", 'engager', '▁dans', '▁cette', '▁voie', '.', '▁Il', '▁y', '▁a', '▁aussi', '▁ceux', '▁qui', ',', '▁pour', '▁des', '▁raisons', '▁personnelles', ',', '▁ou', '▁parce', '▁qu', \"'\", 'ils', '▁ont', '▁le', '▁coeur', '▁à', '▁cela', ',', '▁veulent', '▁se', '▁consacrer', '▁pendant', '▁quelques', '▁temps', '▁-', '▁un', '▁an', ',', '▁un', '▁an', '▁et', '▁demi', '▁-', '▁à', '▁des', '▁tâches', ',', '▁je', '▁dirais', '▁généralement', '▁humanitaire', 's', '▁au', '▁sens', '▁large', '▁du', '▁terme', ',', '▁qu', \"'\", 'elles', '▁soient', '▁militaires', '▁ou', '▁qu', \"'\", 'elles', '▁soient', '▁civiles', ',', '▁à', '▁apporter', '▁leur', '▁concours', '▁à', '▁l', \"'\", 'éducation', '▁nationale', '▁pour', '▁apprendre', '▁à', '▁des', '▁gamin', 's', '▁dé', 'favoris', 'és', '▁à', '▁lire', '▁ou', '▁à', '▁écrire', ',', '▁pour', '▁faciliter', '▁la', '▁préservation', '▁de', '▁l', \"'\", 'environnement', ',', '▁pour', '▁lutter', '▁contre', '▁les', '▁incendie', 's', ',', '▁pour', '▁servir', '▁dans', '▁des', '▁unités', '▁chargée', 's', '▁de', '▁maintenir', '▁la', '▁sécurité', '▁des', '▁personnes', '▁et', '▁des', '▁bien', 's', ',', '▁je', '▁pense', '▁à', '▁la', '▁gendarmerie', '▁ou', '▁à', '▁d', \"'\", 'autres', '▁domaines', '.', '▁Bref', ',', '▁il', '▁y', '▁en', '▁a', '▁certainement', '▁qui', '▁seront', '▁intéressés', '▁par', '▁l', \"'\", 'idée', '▁de', '▁consacrer', '▁un', '▁an', ',', '▁un', '▁an', '▁et', '▁demi', '▁de', '▁leur', '▁coeur', ',', '▁de', '▁leur', '▁générosité', '▁à', '▁d', \"'\", 'autres', ',', '▁dans', '▁ces', '▁domaines', '▁civils', '▁ou', '▁militaires', ',', '▁qui', '▁très', '▁largement', '▁peuvent', '▁être', '▁appelés', '▁des', '▁domaines', '▁humanitaire', 's', '.', '▁J', \"'\", 'ajoute', '▁que', '▁pour', '▁eux', ',', '▁ce', '▁sera', '▁aussi', '▁la', '▁possibilité', '▁d', \"'\", 'acquérir', '▁une', '▁expérience', ',', '▁et', '▁parfois', '▁un', '▁métier', ',', '▁et', '▁de', '▁sortir', '▁de', '▁cette', '▁période', '▁avec', '▁la', '▁possibilité', '▁ou', '▁la', '▁capacité', '▁d', \"'\", 'exercer', '▁un', '▁métier', '▁qu', \"'\", 'ils', '▁auront', '▁appris', '.', '▁Il', '▁y', '▁aura', '▁ceux', '▁qui', '▁s', \"'\", 'intéressent', '▁à', '▁l', \"'\", 'armée', ',', '▁sans', '▁vouloir', '▁en', '▁faire', '▁leur', '▁métier', '.', '▁L', \"'\", 'armée', '▁aura', '▁besoin', '▁de', '▁réserv', 'istes', ',', '▁beaucoup', '▁moins', '▁qu', \"'\", 'avant', ',', '▁alors', '▁il', '▁y', '▁en', '▁a', '▁peut', '-', 'être', '▁qui', '▁veulent', '▁ou', '▁qui', '▁voudront', ',', '▁parce', '▁que', '▁cela', '▁les', '▁intéresse', ',', '▁acquérir', '▁le', '▁minimum', '▁de', '▁connaissances', '▁leur', '▁permettant', '▁d', \"'\", 'être', '▁versé', '▁dans', '▁ce', '▁que', '▁l', \"'\", 'on', '▁appelle', '▁\"', 'les', '▁réserves', '\"', '▁et', '▁le', '▁cas', '▁échéant', '▁d', \"'\", 'être', '▁réserv', 'istes', '.', 'Je', '▁ne', '▁veux', '▁pas', '▁entrer', '▁dans', '▁le', '▁détail', ',', '▁vous', '▁aurez', '▁toute', '▁la', '▁journée', '▁pour', '▁en', '▁entendre', '▁parler', ',', '▁mais', '▁l', \"'\", 'objectif', '▁de', '▁cette', '▁journée', '▁est', '▁de', '▁vous', '▁faire', '▁comprendre', '▁que', '▁l', \"'\", 'armée', '▁est', '▁l', \"'\", 'une', '▁des', '▁institutions', '▁les', '▁plus', '▁nobles', '▁et', '▁les', '▁plus', '▁nécessaires', '▁de', '▁la', '▁nation', '▁et', '▁qu', \"'\", 'il', '▁faut', '▁la', '▁connaître', '.', '▁Parfois', '▁on', '▁a', '▁un', '▁peu', '▁tendance', '▁à', '▁la', '▁critiquer', ',', '▁sans', '▁savoir', '.', '▁Il', '▁faut', '▁la', '▁connaître', '▁parce', '▁qu', \"'\", 'elle', '▁fait', '▁des', '▁choses', '▁formidable', 's', ',', '▁notamment', ',', '▁je', '▁le', '▁répète', ',', '▁dans', '▁le', '▁monde', '▁moderne', '▁sur', '▁le', '▁plan', '▁humanitaire', '▁et', '▁enfin', ',', '▁parce', '▁que', '▁c', \"'\", 'est', '▁le', '▁dernier', '▁rempart', '▁de', '▁notre', '▁sécurité', ',', '▁de', '▁nos', '▁libertés', ',', '▁de', '▁nos', '▁intérêts', ',', '▁si', '▁par', '▁hasard', '▁les', '▁choses', '▁vont', '▁mal', '.', '▁Alors', '▁elle', '▁mérite', '▁d', \"'\", 'être', '▁respectée', ',', '▁elle', '▁a', '▁droit', '▁au', '▁respect', '.', '▁Mais', '▁encore', '▁faut', '-', 'il', '▁savoir', '▁pourquoi', '▁et', '▁c', \"'\", 'est', '▁cela', '▁que', '▁l', \"'\", 'on', '▁va', '▁vous', '▁dire', '.', 'D', \"'\", 'autre', '▁part', ',', '▁on', '▁veut', '▁vous', '▁ouvrir', '▁les', '▁voies', '▁et', '▁les', '▁possibilités', '▁que', ',', '▁directement', '▁ou', '▁indirectement', ',', '▁cette', '▁armée', '▁peut', '▁vous', '▁donner', '▁et', '▁enfin', '▁vous', '▁rappeler', '▁quels', '▁sont', '▁les', '▁principes', '▁que', '▁nous', '▁avons', '▁en', '▁commun', ',', '▁les', '▁principes', '▁de', '▁solidarité', ',', '▁les', '▁principes', '▁de', '▁respect', '▁de', '▁l', \"'\", 'homme', ',', '▁de', '▁sa', '▁dignité', ',', '▁de', '▁l', \"'\", 'égalité', '▁des', '▁un', 's', '▁et', '▁des', '▁autres', ',', '▁et', '▁tout', '▁cela', '▁est', '▁également', '▁important', '.', 'Voi', 'là', '.', '▁J', \"'\", 'ai', '▁dû', '▁certainement', '▁oublier', '▁des', '▁choses', '▁importantes', ',', '▁mais', '▁on', '▁veut', '▁vous', '▁dire', '▁quels', '▁sont', '▁vos', '▁droits', '▁et', '▁aussi', '▁quels', '▁sont', '▁vos', '▁devoirs', '▁à', '▁l', \"'\", 'égard', '▁de', '▁la', '▁Nation', ',', '▁et', '▁cette', '▁journée', '▁est', '▁consacrée', '▁à', '▁cela', '.', '▁Je', '▁souhaite', '▁qu', \"'\", 'elle', '▁soit', '▁pour', '▁vous', '▁intéressante', ',', '▁qu', \"'\", 'elle', '▁vous', '▁', 'apprenne', '▁des', '▁choses', ',', '▁qu', \"'\", 'elle', '▁vous', '▁ouvre', '▁des', '▁perspectives', ',', '▁qu', \"'\", 'elle', '▁vous', '▁donne', '▁davantage', '▁conscience', '▁de', '▁vos', '▁droits', '▁mais', '▁aussi', '▁de', '▁vos', '▁devoirs', '.', 'Je', '▁termine', '▁en', '▁souhaitant', '▁surtout', ',', '▁quelle', '▁que', '▁soit', '▁la', '▁voie', '▁que', '▁vous', '▁emprunter', 'ez', '▁et', '▁les', '▁conclusions', '▁que', '▁vous', '▁tirer', 'ez', '▁de', '▁cette', '▁journée', ',', '▁je', '▁vous', '▁souhaite', '▁surtout', '▁bonne', '▁chance', '▁et', '▁bon', '▁vent']\n",
            "Token IDs:  [19923, 7, 1684, 146, 1941, 8, 39, 14778, 9, 877, 50, 701, 27, 50, 146, 1941, 8, 39, 14778, 7, 44, 49, 11, 41, 34, 28, 445, 14521, 8, 22897, 9, 100, 16, 9349, 15, 17, 11, 2337, 15, 2445, 16, 6375, 8, 13, 7682, 7, 188, 9, 5048, 7, 60, 11, 41, 24, 202, 23, 3334, 147, 51, 102, 33, 23, 126, 18, 11, 7972, 154, 50, 676, 39, 248, 590, 9, 5510, 495, 13, 272, 1010, 18, 11, 1445, 31, 45, 1438, 34, 8, 366, 2330, 9, 84, 11, 41, 28, 1141, 27, 76, 11, 73, 722, 7, 51, 102, 33, 116, 134, 7, 182, 28, 2278, 2284, 14, 23, 600, 2159, 9, 407, 66, 7, 16, 366, 2330, 60, 11, 41, 28, 3244, 2920, 945, 7, 51, 149, 40, 27, 16880, 9, 69, 102, 171, 208, 5032, 8, 1819, 15, 207, 7, 410, 13, 2966, 18, 11, 443, 28, 6841, 23960, 14, 145, 18, 11, 443, 20, 538, 11785, 68, 5892, 7, 15, 17, 11, 817, 20, 3072, 8, 17, 11, 1475, 9, 3317, 1189, 52, 11, 12944, 32, 13, 2966, 8, 3757, 29, 78, 1454, 9, 69, 102, 171, 3522, 20, 4528, 7, 51, 102, 171, 217, 8, 538, 31, 48, 21860, 46, 11, 240, 2639, 488, 23, 126, 97, 125, 7, 18, 11, 266, 31, 530, 18593, 9, 159, 51, 102, 171, 381, 320, 31, 7, 29, 127, 256, 7, 21860, 92, 16, 366, 2330, 60, 11, 41, 885, 24, 13, 13652, 945, 7, 60, 11, 41, 23, 694, 8, 328, 760, 19, 5024, 4434, 8, 6687, 14, 18, 11, 870, 10, 17414, 7, 5993, 7, 2201, 7, 579, 14, 8, 85, 22, 1055, 46, 11, 240, 48, 4688, 7, 46, 11, 240, 1053, 760, 7, 14, 44, 27, 17, 11, 88, 23308, 13, 18465, 20, 22204, 10, 149, 7538, 37, 217, 79, 23, 3228, 693, 8, 13, 13652, 945, 9, 1223, 6729, 149, 330, 8, 217, 8, 19827, 9, 74, 19827, 96, 179, 20, 8778, 5646, 7, 51, 213, 22, 1852, 287, 9, 160, 11, 369, 292, 7, 51, 115, 11, 230, 8448, 27, 66, 207, 7, 36, 729, 7, 24439, 23, 126, 15, 28, 1465, 7675, 35, 8, 13, 1923, 7, 14, 1321, 17197, 36, 1458, 32, 19, 538, 28, 9052, 3329, 14, 10930, 31, 49, 11, 230, 34, 21638, 7, 47, 31, 45, 17, 11, 230, 40, 9, 370, 11, 2677, 1558, 49, 11, 41, 3522, 34, 386, 174, 91, 171, 394, 22, 13906, 47, 18180, 7, 47, 93, 182, 7, 36, 262, 20, 9100, 8, 2341, 3190, 1385, 22, 6278, 47, 214, 9, 543, 207, 18340, 6126, 199, 27, 17, 11, 88, 507, 124, 674, 47, 118, 7, 1786, 40, 7, 8, 366, 2330, 7, 67, 610, 135, 134, 36, 479, 25, 740, 7, 118, 134, 14, 1644, 339, 13, 782, 8, 13, 775, 18, 11, 6108, 24, 420, 67, 9, 370, 11, 2010, 20, 541, 13079, 207, 22, 625, 9, 71, 11, 2010, 7, 60, 11, 230, 18, 11, 803, 17, 11, 2165, 8, 13, 1931, 22, 1532, 9, 139, 207, 33, 101, 28, 293, 3145, 7, 15, 350, 25, 262, 47, 17, 11, 88, 33, 2244, 13, 1015, 1467, 92, 63, 49, 11, 2200, 40, 101, 12319, 10, 32, 166, 5039, 9, 69, 547, 23, 125, 147, 63, 28196, 351, 19145, 23, 10230, 8, 4417, 10, 68, 4417, 10, 19912, 31, 27318, 488, 8, 52, 11, 13693, 222, 63, 7, 51, 2032, 143, 217, 18, 11, 5474, 9, 61, 1015, 1467, 33, 82, 8048, 78, 3456, 9, 170, 45, 464, 40, 12319, 10, 15, 166, 5039, 92, 91, 49, 11, 6305, 34, 17, 11, 3930, 17444, 13, 184, 7, 91, 45, 17, 11, 1106, 40, 9, 1416, 7, 22, 194, 13985, 7, 13, 3456, 52, 11, 230, 16391, 9, 69, 102, 171, 99, 7, 3522, 7, 17, 11, 15199, 31, 171, 217, 2548, 7, 19, 1054, 3788, 31, 52, 11, 2611, 5588, 14854, 10, 9, 1301, 11, 265, 7, 28, 3475, 2330, 7, 13, 775, 16, 203, 8540, 7, 24962, 20, 529, 14, 20, 389, 31, 96, 28, 95, 1118, 4956, 7, 31, 56, 15223, 10, 15, 881, 20, 1149, 4969, 131, 20656, 10, 7, 31, 56, 1366, 1339, 14, 4248, 15, 85, 1018, 2323, 7, 14, 31, 49, 11, 263, 394, 18, 11, 8855, 513, 3538, 24, 16, 85, 9, 228, 11, 41, 145, 23, 238, 460, 8, 394, 27, 63, 296, 14, 31, 45, 2174, 40, 36, 366, 2330, 861, 46, 11, 88, 16, 22984, 199, 178, 9, 84, 11, 41, 145, 28, 6841, 1050, 46, 11, 62, 63, 213, 7, 60, 11, 41, 28, 6841, 8, 2056, 7, 44, 56, 20, 6394, 941, 9, 243, 350, 8, 78, 2284, 7, 76, 11, 73, 3237, 27, 1360, 7, 4880, 27, 1053, 19, 8778, 7, 8, 696, 1039, 47, 945, 31, 10700, 488, 22, 2558, 25, 366, 2330, 7, 44, 49, 11, 230, 40, 2740, 7, 14, 76, 11, 73, 1258, 8, 16, 4446, 9, 159, 13, 5801, 25, 366, 2330, 7, 207, 45, 604, 34, 248, 13, 5801, 8, 66, 818, 128, 13, 2426, 14, 17, 11, 2677, 9, 69, 213, 27, 19, 538, 8679, 113, 44, 27, 60, 11, 41, 27, 97, 6841, 9, 69, 213, 27, 39, 16, 77, 2338, 267, 7, 398, 27, 39, 495, 30099, 10, 8, 17, 11, 2128, 9, 201, 52, 11, 6305, 27, 66, 198, 72, 7, 46, 11, 62, 49, 11, 105, 33, 40, 8, 577, 9, 877, 91, 30, 1776, 29, 19, 318, 6721, 7, 91, 33, 179, 1182, 13, 1297, 7, 13, 3543, 7, 19, 8640, 10058, 2299, 8, 13, 157, 7, 19, 1014, 7, 16, 4103, 7, 47, 18, 11, 266, 541, 7, 65, 91, 33, 34, 1182, 13, 775, 3522, 9, 4481, 726, 26, 6773, 10, 17, 11, 263, 3611, 7, 34, 39, 9, 683, 45, 39, 227, 254, 7, 14, 997, 60, 11, 41, 22, 5184, 28, 3456, 9, 61, 184, 33, 20, 2922, 29, 16, 164, 7, 109, 279, 16, 203, 8540, 19, 3773, 7, 14, 51, 213, 27, 13, 184, 1516, 2091, 77, 5109, 2330, 24, 1027, 8, 3450, 13, 1931, 323, 147, 241, 7, 47, 24, 1027, 8, 3900, 20, 157, 10, 206, 60, 11, 41, 885, 9, 1696, 13, 29045, 52, 11, 4308, 29, 28, 3475, 25934, 7, 31, 82, 20, 10355, 8, 2879, 7, 51, 213, 27, 13, 184, 7, 79, 18, 11, 266, 7, 1516, 632, 3450, 23, 126, 17, 11, 1243, 24, 1351, 19, 173, 2860, 10, 9, 84, 11, 41, 13, 93, 337, 405, 11, 265, 7, 104, 26, 177, 67, 88, 45, 900, 34, 7, 76, 11, 1612, 27, 165, 26, 29, 16, 29821, 9, 139, 66, 207, 7, 60, 11, 41, 68, 1905, 8, 17, 11, 1354, 7, 60, 11, 41, 15, 175, 8, 116, 511, 8, 222, 63, 22, 6439, 9, 370, 4605, 1358, 11, 62, 102, 33, 20, 1014, 7, 11253, 7, 22, 2971, 14, 207, 1242, 93, 86, 17, 11, 2582, 30, 32, 13, 317, 1454, 7, 51, 213, 27, 63, 25066, 13, 1088, 7, 22, 193, 511, 7, 18, 11, 1655, 3528, 20, 1455, 47, 20, 21507, 18, 11, 266, 256, 31, 56, 241, 26, 2787, 14, 31, 56, 12319, 10, 29, 97, 157, 7, 14, 46, 11, 62, 213, 17510, 95, 736, 9, 683, 400, 28, 6841, 31, 191, 165, 446, 1811, 14, 3899, 7, 65, 31, 191, 99, 3611, 7, 17686, 14, 2041, 35, 9, 160, 11, 2047, 7, 3522, 7, 17, 11, 1139, 27, 19, 538, 750, 1218, 97, 6841, 7, 750, 319, 44, 46, 11, 144, 30, 7, 14, 206, 50, 701, 19, 538, 60, 11, 41, 22, 1033, 405, 11, 265, 19, 5024, 79, 19, 1134, 9, 574, 24, 16, 262, 14, 24, 20, 1819, 21512, 7, 1054, 7, 19, 1134, 45, 56, 34, 13447, 15, 13, 93, 6178, 27, 19, 5024, 7, 65, 564, 9474, 16, 125, 8, 328, 19, 541, 22, 218, 7, 19, 5024, 14, 19, 1134, 519, 32, 16, 93, 942, 7, 2890, 19, 1952, 873, 14, 10735, 7, 19, 1952, 5641, 9, 148, 210, 8, 44, 299, 8, 477, 28, 317, 337, 7, 398, 46, 11, 62, 49, 11, 105, 33, 771, 539, 27, 13, 722, 8, 1582, 20, 1014, 8, 13, 7682, 7, 410, 8, 13, 1923, 8, 166, 2922, 7, 8, 166, 1784, 7, 8, 13, 1297, 7, 8, 13, 9352, 8, 17, 11, 698, 7, 8, 17, 11, 4293, 7, 51, 49, 11, 105, 33, 771, 539, 46, 11, 144, 45, 191, 34, 12138, 35, 8, 13, 93, 429, 37, 19, 1134, 14, 37, 19, 5024, 9, 11823, 304, 6841, 7, 51, 213, 22, 602, 27, 39, 77, 2338, 267, 44, 27, 60, 11, 41, 9, 195, 45, 39, 400, 34, 18, 11, 6604, 7, 47, 95, 126, 9, 195, 39, 400, 8, 894, 444, 28, 553, 9, 84, 11, 41, 28, 553, 18, 11, 1070, 7, 8, 6324, 7, 31, 30, 8, 696, 15, 39, 1027, 18, 11, 12882, 16, 203, 8540, 19, 541, 14, 7, 24, 320, 27, 207, 7255, 7, 8, 19, 19859, 9, 574, 109, 45, 39, 400, 34, 18, 11, 6604, 7, 65, 22, 2447, 109, 39, 3402, 20, 3349, 3830, 7, 14, 45, 19, 161, 26, 12444, 138, 34, 7, 14, 24, 22, 1153, 51, 213, 19, 1218, 9, 1799, 102, 22, 33, 2154, 865, 39, 7, 47, 865, 117, 320, 31, 48, 1442, 263, 405, 11, 265, 24, 13, 553, 18, 11, 1070, 7, 31, 30036, 98, 3788, 9, 84, 11, 41, 23, 2400, 2056, 7, 8708, 52, 11, 62, 22, 30, 9, 4499, 26, 1188, 37, 2962, 96, 394, 8, 319, 404, 17, 11, 88, 82, 86, 17, 11, 88, 604, 52, 11, 7376, 29, 78, 1454, 9, 69, 102, 33, 99, 320, 31, 7, 24, 20, 1819, 2681, 7, 47, 398, 46, 11, 240, 96, 16, 1016, 15, 207, 7, 1882, 48, 8977, 339, 193, 125, 67, 23, 674, 7, 23, 674, 14, 1644, 67, 15, 20, 4338, 7, 50, 7216, 1536, 8975, 10, 36, 437, 1071, 25, 788, 7, 46, 11, 734, 1053, 3788, 47, 46, 11, 734, 1053, 17660, 7, 15, 2091, 97, 1477, 15, 17, 11, 1997, 945, 24, 1891, 15, 20, 10090, 10, 570, 19754, 566, 15, 831, 47, 15, 2748, 7, 24, 3039, 13, 13995, 8, 17, 11, 1623, 7, 24, 4357, 192, 19, 9308, 10, 7, 24, 1950, 29, 20, 5240, 5808, 10, 8, 3450, 13, 548, 20, 242, 14, 20, 72, 10, 7, 50, 500, 15, 13, 12839, 47, 15, 18, 11, 266, 2380, 9, 2425, 7, 51, 102, 22, 33, 1975, 31, 519, 9532, 37, 17, 11, 1139, 8, 8977, 23, 674, 7, 23, 674, 14, 1644, 8, 97, 1016, 7, 8, 97, 11567, 15, 18, 11, 266, 7, 29, 119, 2380, 9356, 47, 3788, 7, 31, 95, 2170, 316, 98, 9690, 20, 2380, 8975, 10, 9, 121, 11, 10634, 27, 24, 474, 7, 44, 210, 99, 13, 1088, 18, 11, 10899, 28, 1005, 7, 14, 610, 23, 2056, 7, 14, 8, 1077, 8, 78, 782, 42, 13, 1088, 47, 13, 1381, 18, 11, 12337, 23, 2056, 46, 11, 240, 2890, 2714, 9, 69, 102, 711, 320, 31, 52, 11, 17815, 15, 17, 11, 2677, 7, 112, 2375, 22, 85, 97, 2056, 9, 71, 11, 2677, 711, 394, 8, 19566, 1350, 7, 217, 175, 46, 11, 1949, 7, 183, 51, 102, 22, 33, 104, 26, 177, 31, 1882, 47, 31, 30036, 7, 398, 27, 207, 19, 7255, 7, 8192, 16, 1858, 8, 2107, 97, 1177, 18, 11, 177, 13476, 29, 44, 27, 17, 11, 88, 2668, 87, 408, 8612, 130, 14, 16, 203, 8540, 18, 11, 177, 19566, 1350, 9, 1684, 45, 920, 34, 3305, 29, 16, 2636, 7, 39, 2222, 194, 13, 553, 24, 22, 3002, 639, 7, 65, 17, 11, 1960, 8, 78, 553, 30, 8, 39, 85, 822, 27, 17, 11, 2677, 30, 17, 11, 70, 20, 3847, 19, 40, 16833, 14, 19, 40, 1840, 8, 13, 6622, 14, 46, 11, 62, 213, 13, 1218, 9, 7242, 91, 33, 23, 126, 1367, 15, 13, 16245, 7, 112, 319, 9, 69, 213, 13, 1218, 398, 46, 11, 144, 82, 20, 541, 5823, 10, 7, 410, 7, 50, 16, 10726, 7, 29, 16, 164, 1558, 32, 16, 379, 8975, 14, 743, 7, 398, 27, 60, 11, 41, 16, 348, 18078, 8, 127, 548, 7, 8, 166, 8922, 7, 8, 166, 2922, 7, 86, 37, 2829, 19, 541, 774, 295, 9, 574, 109, 2705, 18, 11, 177, 17686, 7, 109, 33, 347, 36, 1346, 9, 159, 143, 213, 26, 62, 319, 590, 14, 60, 11, 41, 207, 27, 17, 11, 88, 198, 39, 248, 9, 342, 11, 369, 292, 7, 91, 604, 39, 2913, 19, 4723, 14, 19, 3349, 27, 7, 902, 47, 16765, 7, 78, 6841, 104, 39, 509, 14, 743, 39, 3318, 4880, 56, 19, 3189, 27, 63, 296, 22, 1330, 7, 19, 3189, 8, 3834, 7, 19, 3189, 8, 1346, 8, 17, 11, 698, 7, 8, 77, 9352, 7, 8, 17, 11, 4293, 20, 23, 10, 14, 20, 214, 7, 14, 66, 207, 30, 200, 693, 9, 17125, 1188, 9, 121, 11, 73, 1556, 1975, 2078, 20, 541, 3830, 7, 65, 91, 604, 39, 248, 4880, 56, 140, 873, 14, 99, 4880, 56, 140, 10735, 15, 17, 11, 5764, 8, 13, 15734, 7, 14, 78, 553, 30, 5670, 15, 207, 9, 100, 1282, 46, 11, 144, 191, 24, 39, 4659, 7, 46, 11, 144, 39, 21, 29732, 20, 541, 7, 46, 11, 144, 39, 3402, 20, 7699, 7, 46, 11, 144, 39, 556, 1921, 1582, 8, 140, 873, 65, 99, 8, 140, 10735, 9, 1684, 4592, 22, 10006, 381, 7, 1262, 27, 191, 13, 1454, 27, 39, 15980, 267, 14, 19, 9351, 27, 39, 2912, 267, 8, 78, 553, 7, 50, 39, 1282, 381, 317, 1073, 14, 212, 1601]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_to_feed(df,length,batch_size_value):\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  texts = df.Texte.values\n",
        "  labels = df.sexe.values\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  num_truncated_tokens =[]\n",
        "  # Apply function to our corpus\n",
        "  for text in texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # text\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = length,           # We choose for now a max length of 500.\n",
        "                          pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                          return_attention_mask = True,   # Construct attention masks\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          return_overflowing_tokens =True, # return overflowing token information\n",
        "                    )\n",
        "      \n",
        "      # Map tokens to their id in the dictionnary \n",
        "      # We add this to our list    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "  \n",
        "      #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "      \n",
        "      # 3. Attention masks\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # We convert all this into tensors in order to be able to make it work on GPU \n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Original text and transformed tensor print \n",
        "  print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "  print(\" \")\n",
        "  print('Original: ', texts[0][0:100])\n",
        "  print('IDs:', input_ids[0][0:100])\n",
        "  print('Attention masks:', attention_masks[0][0:100])\n",
        "  print('labels',labels[0])\n",
        "\n",
        "\n",
        "  # Combine all above\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Let's create a 80-20 train / validation dataset \n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "  print(\"-------------------------------------------------\")\n",
        "  print(\" \")\n",
        "  print(\"How many texts do we have in the train and validation sample ? \")\n",
        "  print(\" \")\n",
        "  print('We have {} training texts'.format(train_size))\n",
        "  print('We have {} validation texts'.format(val_size))\n",
        "  print(\" \")\n",
        "  print(\"-------------------------------------------------\")\n",
        "\n",
        "  # We set the size of the batch ( usually set around 16 or 32), we will take the lower bound because of the large text length\n",
        "  batch_size = batch_size_value\n",
        "\n",
        "  # We create data loaders for the train and validation dataset. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_set,  # The training samples.\n",
        "              sampler = RandomSampler(train_set), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  val_dataloader = DataLoader(\n",
        "              val_set, # The validation samples.\n",
        "              sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "  \n",
        "  print('Data loaders created for train [0] and val [1]')\n",
        "\n",
        "  return train_dataloader, val_dataloader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b_OFrL3mIBU",
        "colab_type": "code",
        "outputId": "ed2fed94-ea81-42c7-b663-ba59c9abf1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "print('############### Unbalanced sample ################')\n",
        "train_loader_unbalanced, val_loader_unbalanced = prepare_to_feed(df_unbalanced,length=500,batch_size_value=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############### Unbalanced sample ################\n",
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  Monsieur le Préfet, Monsieur le Président du Conseil régional, Monsieur le Président de l'Associatio\n",
            "IDs: tensor([    5,  2445,    16, 25230,     7,  2445,    16,  1850,    25,   960,\n",
            "         4124,     7,  2445,    16,  1850,     8,    17,    11,  3969,    20,\n",
            "         4678,    10,     8,   184,     7, 19923,    19,  1850,    10,     8,\n",
            "         4678,    10,     7,  4348,    13, 28499,    35,     8,    13, 19641,\n",
            "         1467,     7, 23605,    14, 15867,    19, 12680,     7, 23605,    14,\n",
            "        15867,    19,  6664,  3980,     7,  6684,    10,  9402,    80,    14,\n",
            "        15867,    19,  3454,     7, 23605,    14, 19923,     7,   228,    11,\n",
            "           41,    42,   217,     8,   593,    27,    50,   146,   405,    11,\n",
            "          265,    42,    39,    15,  5155,     7,    24,    16,  9280,     8,\n",
            "         3864,     8,    13,  4732,  1230,  8820,  1461,    20,  1751,  3980])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(1)\n",
            "-------------------------------------------------\n",
            " \n",
            "How many texts do we have in the train and validation sample ? \n",
            " \n",
            "We have 1960 training texts\n",
            "We have 490 validation texts\n",
            " \n",
            "-------------------------------------------------\n",
            "Data loaders created for train [0] and val [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "outputId": "7716756e-a587-45f4-eaa0-6d26ff6987d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "print('############### Balanced sample ################')\n",
        "train_loader_balanced, val_loader_balanced = prepare_to_feed(df_balanced,length=500,batch_size_value=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############### Balanced sample ################\n",
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  Mesdames et Messieurs les Ministres,Monsieur le Chef de l'opposition,Mesdames et Messieurs les Secré\n",
            "IDs: tensor([    5, 23605,    14, 19923,    19,  6375,    10,     7, 31609,    16,\n",
            "         6238,     8,    17,    11,  4244,     7,  6684,    10,  9402,    80,\n",
            "           14, 19923,    19, 11907,    10,  2348,     7, 31609,    16,  4860,\n",
            "           26,  7383,  4975,    10,  1114,  4681,     7,  3181,  9402,    35,\n",
            "           13,  6238,     8,  9013,     8,    13,  6854,     8,    13,  2358,\n",
            "         1467,     7, 31609,    16,  1850,    25,   960,  4124,     8,    13,\n",
            "         6873,     7,    14,  4348,  3181,  9402,    35,    14,  2445,    19,\n",
            "        15538,    26, 22778,    10,    25,   960,  4124,     7, 31609,    17,\n",
            "           11, 16336,   297,     7,  4212,  2242,  2650, 20112,  4613,    10,\n",
            "            7,  3696,    10, 12784,     7,  1684,    45,    52,  2009,    39])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(0)\n",
            "-------------------------------------------------\n",
            " \n",
            "How many texts do we have in the train and validation sample ? \n",
            " \n",
            "We have 1960 training texts\n",
            "We have 490 validation texts\n",
            " \n",
            "-------------------------------------------------\n",
            "Data loaders created for train [0] and val [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sxjN5O5mMe5",
        "colab_type": "code",
        "outputId": "030c56e7-bfc9-4aaf-c8f4-d2b43f062cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "print('############### Balanced sample split ################')\n",
        "train_loader_balanced_split, val_loader_balanced_split = prepare_to_feed(df_balanced_split,length=500,batch_size_value=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############### Balanced sample split ################\n",
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  tions un dispositif permettant l'équilibre entre des exigences de sûreté particulièrement élevées et\n",
            "IDs: tensor([    5,    21,  6279,    23,  2507,  1177,    17,    11,  4786,   128,\n",
            "           20,  3768,     8, 15850,    21,   937, 12187,    14,    19,   979,\n",
            "           20,   699,     9,   228,    11,    41,   590,     7,    50,   441,\n",
            "           27,    19, 27696,    10,    31,    56,  5989,     8,  1222,    29,\n",
            "           19,  2001,  6793, 16671,   245,     8,   242, 12117, 15910,    80,\n",
            "            9,  5076,  2170,    50,   920,    27,    17,    11,    88,   218,\n",
            "          119,   756,    36,   533,     8,    13, 15850, 14248, 13895,     9,\n",
            "          100,   555,    27,    50,   672,  1733,    32,    39,    14,    27,\n",
            "           39,   978,   138,  5205,   131,    15,   129,  7030,    24,    22,\n",
            "        15881,     7,    29,    19,  6511,   274,     7,   218,  3091,   526])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(0)\n",
            "-------------------------------------------------\n",
            " \n",
            "How many texts do we have in the train and validation sample ? \n",
            " \n",
            "We have 1980 training texts\n",
            "We have 495 validation texts\n",
            " \n",
            "-------------------------------------------------\n",
            "Data loaders created for train [0] and val [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poTTEJX1hoUK",
        "colab_type": "text"
      },
      "source": [
        "### CamemBERT Sequence Classification model tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VeJI0lDwf",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MPOVB7iRcl",
        "colab_type": "text"
      },
      "source": [
        "We will finally build up our model. We will use the  CamemBERT model for sequence classification which includes a special top layer designed for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHhHzjKgAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing from transformers\n",
        "from transformers import CamembertForSequenceClassification, CamembertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMdM-QqgAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKynoykf_9y",
        "colab_type": "code",
        "outputId": "d2c31ea3-ed8e-4af0-8061-c603f4e9eafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We run the model on the colab GPU \n",
        "gender_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWyHWg5xlBck",
        "colab_type": "text"
      },
      "source": [
        "Optimizers and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgwmHirxnEie",
        "colab_type": "text"
      },
      "source": [
        "#### Constructing the training and validation loop \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6rIz4UnLwq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDt2ZElwcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "from sklearn.metrics import roc_auc_score \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def flat_f1_score(labels,preds):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat,pred_flat,zero_division=1)\n",
        "\n",
        "def flat_roc_auc(labels,preds):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return roc_auc_score(labels_flat,pred_flat,zero_division=0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLN5mzc2ilxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUB8c4k_t1UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_gendermodel(train_loader, val_loader, epochs_val,seed_val,device,lr_value):\n",
        "\n",
        "  ############################  IMPORT MODEL ################################################\n",
        "  gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, )\n",
        "\n",
        "  model = gender_model\n",
        "  model.cuda()\n",
        "  \n",
        "  ############################## RANDOM SEED ##################################################\n",
        "\n",
        "  import random\n",
        " # Let's put a seed to make this result reproducible \n",
        "  seed=seed_val\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  ############################### LEARNING RATE SCHEDULER #######################################\n",
        "\n",
        "  # https://huggingface.co/transformers/migration.html \n",
        "  # https://pytorch.org/docs/stable/optim.html (default values)\n",
        "\n",
        "  import torch.nn as nn\n",
        "  import torch.optim as optim\n",
        "  from transformers import AdamW\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  epochs = epochs_val # In order to fine tune our model we will first set the number of epochs to 4.\n",
        "\n",
        "  # We choose Binary cross enthropy with logits loss for the loss computation. It seems to be the most adapted loss to our problem. \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  #Implements Adam algorithm with weight decay fix.\n",
        "  opti = AdamW(model.parameters(),\n",
        "                    lr =lr_value, # learning rate (default = 1e-3)\n",
        "                    eps = 1e-8 # prevents division by 0 (default = 1e-8)\n",
        "                  )\n",
        "\n",
        "  num_training_steps = len(train_loader) * epochs\n",
        "  # Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "  scheduler = get_linear_schedule_with_warmup(opti, \n",
        "                                              num_warmup_steps = 0,\n",
        "                                              num_training_steps = num_training_steps)\n",
        "    \n",
        "\n",
        "  # We want to evaluate the training phase \n",
        "  training_stats = []\n",
        "\n",
        "  for ep in range(0, epochs):\n",
        "    print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "    print('Training starts')\n",
        "\n",
        "    ################################### TRAINING ################################\n",
        "\n",
        "    #Put the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Set the train loss for the epoch to 0 \n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "      # Clear gradients \n",
        "      model.zero_grad() # (opti.zerograd ? )\n",
        "\n",
        "      # Cpy the 3 batch to GPU \n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      \n",
        "      #return loss and logits\n",
        "      loss, logits = model(b_input_ids, \n",
        "                          token_type_ids=None, \n",
        "                          attention_mask=b_input_mask, \n",
        "                          labels=b_labels) \n",
        "      \n",
        "      # Accumulate training loss for all batches \n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      #Backpropagating the gradients \n",
        "      loss.backward()\n",
        "\n",
        "      # Prevent exploding gradients problem  (forcing the gradients to be small, the parameter updates will not push the parameters too far from their previous values)\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters \n",
        "      opti.step()\n",
        "\n",
        "      # Update learning rate schedule\n",
        "      scheduler.step()\n",
        "\n",
        "    #Calculate the average training loss over all batches  \n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print('')\n",
        "    print('And now, validation STARTS')\n",
        "\n",
        "    ###################### VALIDATION #############################\n",
        "\n",
        "    # Put model in evaluation mode \n",
        "    model.eval()\n",
        "\n",
        "    # Set statistics to 0\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_eval_f1=0\n",
        "    total_roc_auc = 0 \n",
        "\n",
        "    # Confusion matrix ?\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in val_loader:\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      \n",
        "      # We don't care about gradients for eval\n",
        "\n",
        "      with torch.no_grad(): \n",
        "        (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "      total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU \n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "      pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "      labels_flat = label_ids.flatten()\n",
        "\n",
        "      # Confusion matrix ?\n",
        "      #val_batch_preds = np.argmax(logits, axis=1)\n",
        "      #val_batch_labels = label_ids\n",
        "      #predictions.extend(val_batch_preds)\n",
        "      #true_labels.extend(val_batch_labels)\n",
        "\n",
        "      # Accumulation accuracy for all batch\n",
        "      total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "      # Accumulation f1 for all batch\n",
        "      total_eval_f1 += f1_score(labels_flat,pred_flat,zero_division=1)\n",
        "\n",
        "      # Accumulation roc_auc for all batch\n",
        "      #total_roc_auc += flat_roc_auc(label_ids,logits)\n",
        "      \n",
        "      #Final accuracy on all batch\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      #Final f1 on all batch\n",
        "    avg_val_f1 = total_eval_f1 / len(val_loader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_f1))\n",
        "\n",
        "     #Final roc_auc over all batch\n",
        "    #avg_val_roc_auc = total_roc_auc / len(val_loader)\n",
        "    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "      #Final loss over all batch\n",
        "    avg_val_loss = total_eval_loss / len(val_loader)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "    # confusion matrix ? \n",
        "    pred_tags = [i for i in predictions]\n",
        "    valid_tags = [i for i in true_labels]\n",
        "\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "          {\n",
        "              'epoch': ep + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Valid. Accur.': avg_val_accuracy,\n",
        "              'Valid F1' : avg_val_f1,\n",
        "             # 'Valid ROC AUC' : avg_val_roc_auc,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Done !\")\n",
        "\n",
        "  return  training_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqNRTJME5oHs",
        "colab_type": "code",
        "outputId": "2166a7a3-4ebd-4d14-d793-f5a3de50b82a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "results_unbalanced = train_val_gendermodel(train_loader=train_loader_unbalanced, val_loader=val_loader_unbalanced, epochs_val=4,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.59\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.79\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 0.61\n",
            "===========Starting Epoch 2 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.53\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.69\n",
            "  Validation Loss: 0.64\n",
            "===========Starting Epoch 3 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.45\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.83\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.62\n",
            "===========Starting Epoch 4 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.32\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.82\n",
            "  Accuracy: 0.71\n",
            "  Validation Loss: 0.77\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK0Ohghwa8Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omPiofn4gDZc",
        "colab_type": "code",
        "outputId": "f06babfd-ad46-4725-ae60-a9cfb4cf04a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_stats = pd.DataFrame(data=results_unbalanced)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Valid F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.593989</td>\n",
              "      <td>0.608205</td>\n",
              "      <td>0.787755</td>\n",
              "      <td>0.670748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.525662</td>\n",
              "      <td>0.642845</td>\n",
              "      <td>0.812245</td>\n",
              "      <td>0.691156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.452452</td>\n",
              "      <td>0.616325</td>\n",
              "      <td>0.830612</td>\n",
              "      <td>0.717007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.318853</td>\n",
              "      <td>0.771008</td>\n",
              "      <td>0.824490</td>\n",
              "      <td>0.710204</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Valid F1\n",
              "epoch                                                     \n",
              "1           0.593989     0.608205       0.787755  0.670748\n",
              "2           0.525662     0.642845       0.812245  0.691156\n",
              "3           0.452452     0.616325       0.830612  0.717007\n",
              "4           0.318853     0.771008       0.824490  0.710204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9QNCMXR0zc",
        "colab_type": "code",
        "outputId": "b3322886-537c-45ec-a79e-54915f4e67a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "results_balanced = train_val_gendermodel(train_loader=train_loader_balanced, val_loader=val_loader_balanced, epochs_val=4,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.69\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.68\n",
            "  Accuracy: 0.59\n",
            "  Validation Loss: 0.89\n",
            "===========Starting Epoch 2 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.66\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.69\n",
            "  Validation Loss: 0.52\n",
            "===========Starting Epoch 3 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.54\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.74\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 0.74\n",
            "===========Starting Epoch 4 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.35\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.76\n",
            "  Accuracy: 0.67\n",
            "  Validation Loss: 1.10\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcHp2OKhdRPd",
        "colab_type": "code",
        "outputId": "aa7df9f8-9326-47bc-c6b2-242a07404fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_stats = pd.DataFrame(data=results_balanced)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Valid F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.690740</td>\n",
              "      <td>0.886900</td>\n",
              "      <td>0.681633</td>\n",
              "      <td>0.585034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.659529</td>\n",
              "      <td>0.520504</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.688435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.541589</td>\n",
              "      <td>0.739033</td>\n",
              "      <td>0.744898</td>\n",
              "      <td>0.672109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.353204</td>\n",
              "      <td>1.095774</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.670748</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Valid F1\n",
              "epoch                                                     \n",
              "1           0.690740     0.886900       0.681633  0.585034\n",
              "2           0.659529     0.520504       0.771429  0.688435\n",
              "3           0.541589     0.739033       0.744898  0.672109\n",
              "4           0.353204     1.095774       0.755102  0.670748"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb6Ca8qmdT74",
        "colab_type": "code",
        "outputId": "e63a7afc-a018-465a-8823-cff3594948c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "results_balanced_split = train_val_gendermodel(train_loader=train_loader_balanced_split, val_loader=val_loader_balanced_split, epochs_val=4,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.70\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.74\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.69\n",
            "===========Starting Epoch 2 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.56\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.77\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.85\n",
            "===========Starting Epoch 3 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.41\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.80\n",
            "  Accuracy: 0.74\n",
            "  Validation Loss: 0.85\n",
            "===========Starting Epoch 4 / 4 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.20\n",
            "\n",
            "And now, validation STARTS\n",
            "  Accuracy: 0.81\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 1.00\n",
            "\n",
            "Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p6WloNvdb41",
        "colab_type": "code",
        "outputId": "33e84266-6dd7-43a3-a273-506b5c857c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_stats = pd.DataFrame(data=results_balanced_split)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Valid F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.695822</td>\n",
              "      <td>0.690138</td>\n",
              "      <td>0.737903</td>\n",
              "      <td>0.677419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.556895</td>\n",
              "      <td>0.851728</td>\n",
              "      <td>0.768145</td>\n",
              "      <td>0.729839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.412280</td>\n",
              "      <td>0.850640</td>\n",
              "      <td>0.796371</td>\n",
              "      <td>0.736559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.201848</td>\n",
              "      <td>1.000318</td>\n",
              "      <td>0.808468</td>\n",
              "      <td>0.752688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur.  Valid F1\n",
              "epoch                                                     \n",
              "1           0.695822     0.690138       0.737903  0.677419\n",
              "2           0.556895     0.851728       0.768145  0.729839\n",
              "3           0.412280     0.850640       0.796371  0.736559\n",
              "4           0.201848     1.000318       0.808468  0.752688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLxa4te3gzi-",
        "colab_type": "text"
      },
      "source": [
        "Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXUkXjfg0s2",
        "colab_type": "text"
      },
      "source": [
        "#### Saving the model ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LLOEa3g2wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60t946KwdjJh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation and qualitative analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be83uYVShfmX",
        "colab_type": "text"
      },
      "source": [
        "In this section we train the model according to what we have infered from the previous section. Moreover we will evaluate our model qualitatively on a little development sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9XVYefxoUWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a6a7de3a-1a7a-4e26-b032-81950bb44c47"
      },
      "source": [
        "df_eval=balanced_splitted(df,seed_val=2020,frac_val=1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 12,641\n",
            "\n",
            "Proportions of women in the balanced splitted corpus : 51.41998259631359\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_kgUi8-Znc",
        "colab_type": "code",
        "outputId": "b56426b6-67d2-49aa-d882-7366754c8a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len_train = round(0.98*len(df_eval))\n",
        "len_dev = len(df_eval) - len_train\n",
        "len_train, len_dev"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12388, 253)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmMk8Je9-KgD",
        "colab_type": "code",
        "outputId": "01bb1876-9281-4d02-d8b4-d7f2f35cb2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df_balanced_split= df_eval[0:len_train]\n",
        "dev_balanced_split=df_eval[len_train:len(df_eval)]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this balanced splitted corpus : {:,}\\n'.format(df_balanced_split.shape[0]))\n",
        "print('Number of text in the development sample : {:,}\\n'.format(dev_balanced_split.shape[0]))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 12,388\n",
            "\n",
            "Number of text in the development sample : 253\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFNQB5W_DJg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_to_feed_train(df,length,batch_size_value):\n",
        "  from transformers import CamembertTokenizer\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  texts = df_eval.Texte.values\n",
        "  labels = df_eval.sexe.values\n",
        "  tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  num_truncated_tokens =[]\n",
        "  # Apply function to our corpus\n",
        "  for text in texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # text\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = length,           # We choose for now a max length of 500.\n",
        "                          pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                          return_attention_mask = True,   # Construct attention masks\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          return_overflowing_tokens =True, # return overflowing token information\n",
        "                    )\n",
        "      \n",
        "      # Map tokens to their id in the dictionnary \n",
        "      # We add this to our list    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "  \n",
        "      #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "      \n",
        "      # 3. Attention masks\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # We convert all this into tensors in order to be able to make it work on GPU \n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Original text and transformed tensor print \n",
        "  print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "  print(\" \")\n",
        "  print('Original: ', texts[0][0:100])\n",
        "  print('IDs:', input_ids[0][0:100])\n",
        "  print('Attention masks:', attention_masks[0][0:100])\n",
        "  print('labels',labels[0])\n",
        "\n",
        "\n",
        "  # Combine all above\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # We create data loaders for the train and validation dataset. \n",
        "  train_dataloader = DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = SequentialSampler(dataset), \n",
        "              batch_size = batch_size_value # Trains with this batch size.\n",
        "          )\n",
        "  \n",
        "  print('Data loaders created for train [0]')\n",
        "\n",
        "  return train_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9tLM3cTDeQz",
        "colab_type": "code",
        "outputId": "6ee025d0-201a-41af-e3f9-d1f352527376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "train_loader_balanced_split= prepare_to_feed_train(df_balanced_split,length=500,batch_size_value=10)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  r ces questions européennes, pour ériger l'emploi en une politique transversale ? Que pensez-vous fa\n",
            "IDs: tensor([    5,   874,   119,   756,  5165,     7,    24,    21, 30322,    17,\n",
            "           11,  1251,    22,    28,   462, 21912,   106,   799,  3463,    26,\n",
            "          315,    85,    24,    27,    13,   184,    14,    17,    11,  1906,\n",
            "         7110,   909,   573,  8304,   113,    36,   359,  2316,    23,   499,\n",
            "        13046,    20,   242,    31,    45,   104,    98,  2464,    37,    19,\n",
            "         2825,  8318,  2500,   106,   629,    67,  1301,    11,   265,     7,\n",
            "           22,   763,    18,    11,  1251,    14,     8,  1618,     7,    17,\n",
            "           11,  1354,    30,     7,    36,   334,     7, 17963,    43,    60,\n",
            "           11,    41,    13,    87, 10198,   728,  3909,     8, 17385,   517,\n",
            "           66,    16,   164,    48,   835,    18,    11,  1311,   186,    24])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(0)\n",
            "Data loaders created for train [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY9uXaDehe5v",
        "colab_type": "code",
        "outputId": "e44355d2-22c1-44f7-a692-8a51793f26b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "9f8eb756be254a179c8e075c99eace93",
            "f3a30834ae094aefb4c9475980b65ba3",
            "3c69cdc6eafa4816a4bc3545ccac13e9",
            "5362eff3d3ba4379872de2ba6b7aa509",
            "398949bda1cc4864bb2f494d54ec7e57",
            "68fef0e569244a34bd08b71107a55f81",
            "cda7972dcecb4f3baab8bbdb98cc4b3b",
            "72e46ac65ba740f5898649a52403d456",
            "a0a41efa0c9c4668a485aea81a4199e1",
            "68194341bf3049c196bed89473f7615c",
            "86615c7d61a04bbaabde2e005b5b4da5",
            "62d2d527bf814112954cb952824aa394",
            "90ebbaca818c42178737433696eb3cb2",
            "a89b56ca3cd94d768f356d45390c6d80",
            "92bec7f4ff7d4cc581d081946e689455",
            "25fffe2a7bab47adb5e2be72cf7c298a"
          ]
        }
      },
      "source": [
        "############################  IMPORT MODEL ################################################\n",
        " from transformers import CamembertForSequenceClassification\n",
        "gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "\"camembert-base\", \n",
        "num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "output_attentions = False, \n",
        "output_hidden_states = False, )\n",
        "\n",
        "gender_model.cuda()\n",
        "\n",
        "############################## RANDOM SEED ##################################################\n",
        "\n",
        "import random\n",
        "# Let's put a seed to make this result reproducible \n",
        "seed=2020\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "############################### LEARNING RATE SCHEDULER #######################################\n",
        "\n",
        "# https://huggingface.co/transformers/migration.html \n",
        "# https://pytorch.org/docs/stable/optim.html (default values)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 1  # In order to fine tune our model we will first set the number of epochs to 4.\n",
        "\n",
        "# We choose Binary cross enthropy with logits loss for the loss computation. It seems to be the most adapted loss to our problem. \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#Implements Adam algorithm with weight decay fix.\n",
        "opti = AdamW(gender_model.parameters(),\n",
        "                lr =5e-5, # learning rate (default = 1e-3)\n",
        "                eps = 1e-8 # prevents division by 0 (default = 1e-8)\n",
        "              )\n",
        "\n",
        "num_training_steps = len(train_loader_balanced_split) * epochs\n",
        "# Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "scheduler = get_linear_schedule_with_warmup(opti, \n",
        "                                          num_warmup_steps = 0,\n",
        "                                          num_training_steps = num_training_steps)\n",
        "\n",
        "# We want to evaluate the training phase \n",
        "training_stats = []\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "  print('Training starts')\n",
        "\n",
        "  ################################### TRAINING ################################\n",
        "\n",
        "  #Put the model in training mode\n",
        "  gender_model.train()\n",
        "\n",
        "  # Set the train loss for the epoch to 0 \n",
        "  total_train_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(tqdm(train_loader_balanced_split)):\n",
        "    # Clear gradients \n",
        "    gender_model.zero_grad() # (opti.zerograd ? )\n",
        "\n",
        "    # Cpy the 3 batch to GPU \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    \n",
        "    #return loss and logits\n",
        "    loss, logits = gender_model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels) \n",
        "    \n",
        "    # Accumulate training loss for all batches \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    #Backpropagating the gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Prevent exploding gradients problem  (forcing the gradients to be small, the parameter updates will not push the parameters too far from their previous values)\n",
        "    torch.nn.utils.clip_grad_norm_(gender_model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters \n",
        "    opti.step()\n",
        "\n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "#Calculate the average training loss over all batches  \n",
        "avg_train_loss = total_train_loss / len(train_loader_balanced_split)\n",
        "print(\"\")\n",
        "print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "print('')  "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f8eb756be254a179c8e075c99eace93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=637, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0a41efa0c9c4668a485aea81a4199e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=445032417, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===========Starting Epoch 1 / 1 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.47\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd2rBrsSLro",
        "colab_type": "text"
      },
      "source": [
        "Preparing development sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1GKZZKlSOZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "310220d1-9abb-4520-c45e-d29c0510b355"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import CamembertTokenizer\n",
        "\n",
        "texts = dev_balanced_split.Texte.values\n",
        "labels = dev_balanced_split.sexe.values\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right')\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "original_text =[]\n",
        "tokenized_text=[]\n",
        "\n",
        "for text in texts:\n",
        "  original_text.append(text)\n",
        "  tokenized_text.append(tokenizer.tokenize(text))\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 500,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                  )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Original text and transformed tensor print \n",
        "print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "print(\" \")\n",
        "print('Original: ', texts[0][0:100])\n",
        "print('Tokenized: ', tokenized_text[0])\n",
        "print('IDs:', input_ids[0][0:100])\n",
        "print('Attention masks:', attention_masks[0][0:100])\n",
        "print('labels',labels[0])\n",
        "\n",
        "# Combine all above\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "batch_size = 1\n",
        "# We create data loaders for the train and validation dataset. \n",
        "dev_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(dataset), # We set to sequential and we keep track \n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  M. le président. L'ordre du jour appelle la discussion du projet de loi ratifiant l'ordonnance n° 20\n",
            "Tokenized:  ['▁M', '.', '▁le', '▁président', '.', '▁L', \"'\", 'ordre', '▁du', '▁jour', '▁appelle', '▁la', '▁discussion', '▁du', '▁projet', '▁de', '▁loi', '▁rat', 'ifiant', '▁l', \"'\", 'ordonnance', '▁n', '°', '▁2017', '31', '▁du', '▁12', '▁janvier', '▁2017', '▁de', '▁mise', '▁en', '▁cohérence', '▁des', '▁textes', '▁au', '▁regard', '▁des', '▁dispositions', '▁de', '▁la', '▁loi', '▁n', '°', '▁2016', '-', '41', '▁du', '▁26', '▁janvier', '▁2016', '▁de', '▁modernisation', '▁de', '▁notre', '▁système', '▁de', '▁santé', '▁(', 'projet', '▁n', '°', '▁6', '70', '▁[', '2016', '-', '2017', ']', ',', '▁texte', '▁de', '▁la', '▁commission', '▁n', '°', '▁12', ',', '▁rapport', '▁n', '°', '▁10', '),', '▁du', '▁projet', '▁de', '▁loi', '▁rat', 'ifiant', '▁l', \"'\", 'ordonnance', '▁n', '°', '▁2017', '-', '48', '▁du', '▁19', '▁janvier', '▁2017', '▁relative', '▁à', '▁la', '▁profession', '▁de', '▁', 'physicien', '▁médical', '▁et', '▁l', \"'\", 'ordonnance', '▁n', '°', '▁2017', '-', '50', '▁du', '▁19', '▁janvier', '▁2017', '▁relative', '▁à', '▁la', '▁reconnaissance', '▁des', '▁qualifications', '▁professionnelle', 's', '▁dans', '▁le', '▁domaine', '▁de', '▁la', '▁santé', '▁(', 'projet', '▁n', '°', '▁6', '70', '▁[', '2016', '-', '2017', ']', ',', '▁texte', '▁de', '▁la', '▁commission', '▁n', '°', '▁11', ',', '▁rapport', '▁n', '°', '▁10', ')', '▁et', '▁du', '▁projet', '▁de', '▁loi', '▁rat', 'ifiant', '▁l', \"'\", 'ordonnance', '▁n', '°', '▁2017', '-6', '44', '▁du', '▁27', '▁avril', '▁2017', '▁relative', '▁à', '▁l', \"'\", 'adaptation', '▁des', '▁dispositions', '▁législatives', '▁relatives', '▁au', '▁fonctionnement', '▁des', '▁ordres', '▁des', '▁professions', '▁de', '▁santé', '▁(', 'projet', '▁n', '°', '▁6', '70', '▁[', '2016', '-', '2017', ']', ',', '▁texte', '▁de', '▁la', '▁commission', '▁n', '°', '▁13', ',', '▁rapport', '▁n', '°', '▁10', ').', 'La', '▁procédure', '▁accéléré', 'e', '▁a', '▁été', '▁engagée', '▁sur', '▁ces', '▁trois', '▁textes', '▁adopté', 's', '▁par', '▁l', \"'\", 'Assemblée', '▁nationale', '.', 'Dans', '▁la', '▁discussion', '▁générale', '▁commune', ',', '▁la', '▁parole', '▁est', '▁à', '▁Mme', '▁la', '▁secrétaire', '▁d', \"'\", 'État', '.', 'M', 'me', '▁Mar', 'lène', '▁S', 'chi', 'app', 'a', ',', '▁secrétaire', '▁d', \"'\", 'État', '▁auprès', '▁du', '▁Premier', '▁ministre', ',', '▁chargée', '▁de', '▁l', \"'\", 'égalité', '▁entre', '▁les', '▁femmes', '▁et', '▁les', '▁hommes', '.', '▁Monsieur', '▁le', '▁président', ',', '▁monsieur', '▁le', '▁président', '▁de', '▁la', '▁commission', '▁des', '▁affaires', '▁sociales', ',', '▁madame', '▁la', '▁rapporteur', 'e', ',', '▁mesdames', '▁les', '▁sénat', 'rice', 's', ',', '▁messieurs', '▁les', '▁sénateur', 's', ',', '▁je', '▁viens', '▁présenter', '▁aujourd', \"'\", 'hui', '▁devant', '▁votre', '▁assemblée', ',', '▁au', '▁nom', '▁de', '▁Mme', '▁la', '▁ministre', '▁des', '▁solidarité', 's', '▁et', '▁de', '▁la', '▁santé', ',', '▁Agnès', '▁Bu', 'z', 'yn', ',', '▁trois', '▁projets', '▁de', '▁loi', '▁de', '▁ratification', '▁d', \"'\", 'ordonnance', 's', '▁rédigée', 's', '▁en', '▁application', '▁de', '▁la', '▁loi', '▁du', '▁26', '▁janvier', '▁2016', '▁de', '▁modernisation', '▁de', '▁notre', '▁système', '▁de', '▁santé', '.', 'Con', 'tra', 'ire', 'ment', '▁à', '▁une', '▁majorité', '▁d', \"'\", 'entre', '▁vous', ',', '▁je', '▁n', \"'\", 'ai', '▁pas', '▁été', '▁directement', '▁impliqué', 'e', '▁dans', '▁les', '▁débats', '▁parlementaires', '▁ayant', '▁conduit', '▁à', '▁', 'habilit', 'er', '▁le', '▁précédent', '▁gouvernement', '▁à', '▁agir', '▁par', '▁voie', '▁d', \"'\", 'ordonnance', 's', '▁dans', '▁des', '▁délais', '▁limités', '▁et', '▁dans', '▁les', '▁domaines', '▁qui', '▁nous', '▁occupent', '▁aujourd', \"'\", 'hui', '.', '▁Il', '▁me', '▁paraît', '▁néanmoins', '▁important', ',', '▁afin', '▁d', \"'\", 'éclairer', '▁nos', '▁débats', ',', '▁de', '▁re', 'place', 'r', '▁dans', '▁leur', '▁contexte', '▁chacune', '▁des', '▁ordonnance', 's', '▁faisant', '▁l', \"'\", 'objet', '▁d', \"'\", 'un', '▁projet', '▁de', '▁loi', '▁de', '▁ratification', '.', 'Je', '▁m', \"'\", 'efforcer', 'ai', '▁d', \"'\", 'être', '▁aussi', '▁claire', '▁que', '▁possible', '▁dans', '▁la', '▁présentation', '▁de', '▁chacun', '▁de', '▁ces', '▁textes', ',', '▁dans', '▁la', '▁description', '▁des', '▁objectifs', '▁qui', '▁leur', '▁sont', '▁associés', ',', '▁des', '▁conditions', '▁dans', '▁lesquelles', '▁ils', '▁ont', '▁été', '▁élaboré', 's', ',', '▁ainsi', '▁que', '▁de', '▁leur', '▁contenu', '▁et', '▁de', '▁leurs', '▁enjeux', '▁propres', '.', 'L', \"'\", 'ordonnance', '▁relative', '▁à', '▁la', '▁profession', '▁de', '▁', 'physicien', '▁médical', '▁reconnaît', '▁cette', '▁dernière', '▁comme', '▁profession', '▁de', '▁santé', ',', '▁conformément', '▁à', '▁l', \"'\", 'un', '▁des', '▁objectifs', '▁du', '▁pl']\n",
            "IDs: tensor([    5,   188,     9,    16,   668,     9,    71,    11,  1243,    25,\n",
            "          209,  2668,    13,  3651,    25,   327,     8,   589,  7098,  7986,\n",
            "           17,    11, 15050,    49,   571,   755,  3018,    25,   409,   695,\n",
            "          755,     8,   375,    22,  9157,    20,  1726,    36,   897,    20,\n",
            "         2756,     8,    13,   589,    49,   571,   983,    26,  3967,    25,\n",
            "         1157,   695,   983,     8, 15388,     8,   127,   439,     8,   561,\n",
            "           38, 19940,    49,   571,   260,  3080,   403,  8946,    26,  6595,\n",
            "          374,     7,   930,     8,    13,  2585,    49,   571,   409,     7,\n",
            "          459,    49,   571,   239,   142,    25,   327,     8,   589,  7098,\n",
            "         7986,    17,    11, 15050,    49,   571,   755,    26,  3469,    25])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXVKl1flXbkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dev_eval(model,dev_dataloader):\n",
        "  # Put model in evaluation mode \n",
        "  model.eval()\n",
        "\n",
        "  total_pred,total_labels,total_logits=[],[], []\n",
        "  for batch in dev_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    \n",
        "    # We don't care about gradients for eval\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      (loss, logits) = model(b_input_ids, \n",
        "                                  token_type_ids=None, \n",
        "                                  attention_mask=b_input_mask,\n",
        "                                  labels=b_labels)\n",
        "    total_eval_loss.append(loss.item())\n",
        "\n",
        "      # Move logits and labels to CPU \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    #score_max = np.amax(logits, axis=1).flatten().item()\n",
        "    #score_min = np.amin(logits, axis=1).flatten().item()\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten().item()\n",
        "    labels_flat = label_ids.flatten().item()\n",
        "    logits_flat = logits.flatten()\n",
        "\n",
        "    total_logits.append(logits_flat)\n",
        "    total_pred.append(pred_flat)\n",
        "    total_labels.append(labels_flat)\n",
        "    #scores_max.append(score_max)\n",
        "    #scores_min.append(score_min)\n",
        "\n",
        "  return total_labels,total_logits,total_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrBiNLt4MLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_labels,total_logits,total_logits = dev_eval(gender_model,dev_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgnb82fazOkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Compute the mean of logits \n",
        "means = [np.mean([el for el in sublist]) for sublist in total_logits]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_setJg90Cck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the score for label 1 \n",
        "one_score = [el[1] for el in total_logits]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvgZXct7YSpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put everything inside a dataframe\n",
        "results_dev=pd.DataFrame([total_labels,total_pred,one_score]).transpose()\n",
        "results_dev.columns=['returned_labels','model_pred','one_score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBhJ624rzOWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f814ee53-9fa2-4634-cb99-7d3b964c447e"
      },
      "source": [
        "results_dev"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.960449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.978529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.979520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.355703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.962091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.981431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.920880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.676623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.960540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.354800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>253 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     returned_labels  model_pred  one_score\n",
              "0                1.0         1.0   1.960449\n",
              "1                1.0         1.0   1.978529\n",
              "2                1.0         1.0   1.979520\n",
              "3                0.0         0.0  -1.355703\n",
              "4                1.0         1.0   0.962091\n",
              "..               ...         ...        ...\n",
              "248              1.0         1.0   1.981431\n",
              "249              1.0         1.0   1.920880\n",
              "250              0.0         0.0  -1.676623\n",
              "251              1.0         1.0   1.960540\n",
              "252              0.0         0.0  -1.354800\n",
              "\n",
              "[253 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8hE01QQcz8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge back with the text\n",
        "frames = [dev_balanced_split[['Texte','sexe','index_df']].reset_index(), results_dev]\n",
        "result = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psePogRcfctx",
        "colab_type": "code",
        "outputId": "b36fa401-54c3-4d11-efbc-e2f08f2a5e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "result.head(2)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12388</td>\n",
              "      <td>M. le président. L'ordre du jour appelle la di...</td>\n",
              "      <td>1</td>\n",
              "      <td>203962</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.960449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12389</td>\n",
              "      <td>relle en Jordanie, qui se traduisent notamment...</td>\n",
              "      <td>1</td>\n",
              "      <td>164041</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.978529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... one_score\n",
              "0  12388  ...  1.960449\n",
              "1  12389  ...  1.978529\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLF9aS31dH7M",
        "colab_type": "code",
        "outputId": "21d034b1-e90f-42cc-990c-d7c340b99526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "#Which texts failed ? \n",
        "result[result.model_pred!=result.sexe].head(2)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12396</td>\n",
              "      <td>e Nice Sophia Antipolis, que dans des Ecoles d...</td>\n",
              "      <td>1</td>\n",
              "      <td>198508</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.351301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12398</td>\n",
              "      <td>core le service du Museum d'histoire naturelle...</td>\n",
              "      <td>0</td>\n",
              "      <td>190515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.195292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ... one_score\n",
              "8   12396  ... -0.351301\n",
              "10  12398  ...  1.195292\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHTuqPbu1nti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We merge this dataframe to the first dataframe \n",
        "merged_results=result.merge(df,how='left',left_on='index_df',right_on='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74zbZ1xZ2DZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "fdbffbf3-16a2-4c7c-a943-7fe070095e46"
      },
      "source": [
        "#Recover characteristics of speeches wrongly classified\n",
        "merged_results[merged_results.returned_labels!=merged_results.model_pred].head()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte_x</th>\n",
              "      <th>sexe_x</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>Titre</th>\n",
              "      <th>Type</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Prenom</th>\n",
              "      <th>Nom</th>\n",
              "      <th>Fonction</th>\n",
              "      <th>Date</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Texte_y</th>\n",
              "      <th>Lien</th>\n",
              "      <th>PRENOM</th>\n",
              "      <th>preusuel</th>\n",
              "      <th>sexe_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12396</td>\n",
              "      <td>e Nice Sophia Antipolis, que dans des Ecoles d...</td>\n",
              "      <td>1</td>\n",
              "      <td>198508</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.351301</td>\n",
              "      <td>16675</td>\n",
              "      <td>198508</td>\n",
              "      <td>Déclaration de Mme Najat Vallaud-Belkacem, min...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Najat</td>\n",
              "      <td>Vallaud-Belkacem</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-03-25T12:00:00Z</td>\n",
              "      <td>Education - Recherche,Innovation</td>\n",
              "      <td>1. Les temps que nous vivons, sont des temps d...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/198508-de...</td>\n",
              "      <td>NAJAT</td>\n",
              "      <td>NAJAT</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12398</td>\n",
              "      <td>core le service du Museum d'histoire naturelle...</td>\n",
              "      <td>0</td>\n",
              "      <td>190515</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.195292</td>\n",
              "      <td>13376</td>\n",
              "      <td>190515</td>\n",
              "      <td>Déclaration de M. Jean-Marc Ayrault, Premier m...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Jean-Marc</td>\n",
              "      <td>Ayrault</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2014-02-20T12:00:00Z</td>\n",
              "      <td>Environnement,Politique de l'environnement,Pro...</td>\n",
              "      <td>Merci Monsieur le président,Mesdames et Messie...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/190515-de...</td>\n",
              "      <td>JEAN-MARC</td>\n",
              "      <td>JEAN-MARC</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12400</td>\n",
              "      <td>ure. Toutefois, le soutien à la croissance et ...</td>\n",
              "      <td>0</td>\n",
              "      <td>201836</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022327</td>\n",
              "      <td>18538</td>\n",
              "      <td>201836</td>\n",
              "      <td>Déclaration de M. Harlem Désir, secrétaire d'E...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>International</td>\n",
              "      <td>Harlem</td>\n",
              "      <td>Désir</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-01-12T12:00:00Z</td>\n",
              "      <td>Union européenne,Zone euro</td>\n",
              "      <td>Mme la présidente. L'ordre du jour appelle le ...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/201836-de...</td>\n",
              "      <td>HARLEM</td>\n",
              "      <td>HARLEM</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>12402</td>\n",
              "      <td>égalités reste un combat permanent, que décidé...</td>\n",
              "      <td>0</td>\n",
              "      <td>130176</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.976747</td>\n",
              "      <td>8802</td>\n",
              "      <td>130176</td>\n",
              "      <td>Déclaration de M. François Chérèque, secrétair...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Economie</td>\n",
              "      <td>François</td>\n",
              "      <td>Chérèque</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2002-05-31T12:00:00Z</td>\n",
              "      <td>Emploi - Travail,Syndicat</td>\n",
              "      <td>Voici venu le moment de clore les travaux de n...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/130176-de...</td>\n",
              "      <td>FRANÇOIS</td>\n",
              "      <td>FRANÇOIS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>12409</td>\n",
              "      <td>qui sont un problème pour toutes les communaut...</td>\n",
              "      <td>0</td>\n",
              "      <td>182871</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.843098</td>\n",
              "      <td>11404</td>\n",
              "      <td>182871</td>\n",
              "      <td>Déclaration de M. Nicolas Sarkozy, Président d...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Nicolas</td>\n",
              "      <td>Sarkozy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011-08-28T12:00:00Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mes chers compatriotes de Nouvelle-Calédonie e...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/182871-de...</td>\n",
              "      <td>NICOLAS</td>\n",
              "      <td>NICOLAS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index                                            Texte_x  ...   preusuel sexe_y\n",
              "8   12396  e Nice Sophia Antipolis, que dans des Ecoles d...  ...      NAJAT      1\n",
              "10  12398  core le service du Museum d'histoire naturelle...  ...  JEAN-MARC      0\n",
              "12  12400  ure. Toutefois, le soutien à la croissance et ...  ...     HARLEM      0\n",
              "14  12402  égalités reste un combat permanent, que décidé...  ...   FRANÇOIS      0\n",
              "21  12409  qui sont un problème pour toutes les communaut...  ...    NICOLAS      0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjJlGCVmhlsW",
        "colab_type": "text"
      },
      "source": [
        "We want to dive a bit into the model and see how it makes a choice and why it fails on thos 38 sentences. Let's take one of them. We will redo point 4 of TD4 to see the score reached by each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22UuylghdiGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_to_analyse = result[result.model_pred!=result.sexe][0:1]\n",
        "sentence_to_analyse = sentence_to_analyse[['Texte','sexe']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70o0twGrkVG7",
        "colab_type": "code",
        "outputId": "1e4a75b0-a78d-432c-8d60-0072f76eedf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sentence_to_analyse['Texte']"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8    e Nice Sophia Antipolis, que dans des Ecoles d...\n",
              "Name: Texte, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XKOwjLLh7u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in sentence_to_analyse['Texte']:\n",
        "  tokens = text.split('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8P-mscmiWp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_to_analyse =pd.DataFrame(tokens)\n",
        "sentences_to_analyse=sentences_to_analyse.assign(sexe=1)\n",
        "sentences_to_analyse.columns=['Texte','sexe']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhu2c2MSkOmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "texts = sentences_to_analyse.Texte.values\n",
        "labels = sentences_to_analyse.sexe.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Zlgz0hm5qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnbWnbyrleBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "original_text =[]\n",
        "tokenized_text=[]\n",
        "\n",
        "for text in texts:\n",
        "  original_text.append(text)\n",
        "  tokenized_text.append(tokenizer.tokenize(text))\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 50,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                  )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "  attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSARDhWAl5zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPMyriA4leiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine all above\n",
        "batch_size = 1\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "# We create data loaders for the train and validation dataset. \n",
        "dev_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(dataset), # We set to sequential and we keep track \n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzsoooGrlpdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put model in evaluation mode \n",
        "gender_model.eval()\n",
        "\n",
        "total_pred,total_labels, total_eval_loss,scores_max,scores_min=[],[], [],[],[]\n",
        "for batch in dev_dataloader:\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_input_mask = batch[1].to(device)\n",
        "  b_labels = batch[2].to(device)\n",
        "  \n",
        "  # We don't care about gradients for eval\n",
        "\n",
        "  with torch.no_grad(): \n",
        "    (loss, logits) = gender_model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "  total_eval_loss.append(loss.item())\n",
        "\n",
        "    # Move logits and labels to CPU \n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  score_max = np.amax(logits, axis=1).flatten().item()\n",
        "  score_min = np.amin(logits, axis=1).flatten().item()\n",
        "  pred_flat = np.argmax(logits, axis=1).flatten().item()\n",
        "  labels_flat = label_ids.flatten().item()\n",
        "\n",
        "  total_pred.append(pred_flat)\n",
        "  total_labels.append(labels_flat)\n",
        "  scores_max.append(score_max)\n",
        "  scores_min.append(score_min)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaWq1yNgnHIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_dev=pd.DataFrame([total_labels,total_pred,scores_max,scores_min]).transpose()\n",
        "results_dev.columns=['returned_labels','model_pred','scores_max','score_min']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4-NN0erngfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = [sentences_to_analyse.reset_index(), results_dev]\n",
        "result = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5MI09GF4yuR",
        "colab_type": "code",
        "outputId": "cdb79f61-e5df-4eb2-a80d-2c165015c133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "result[result.sexe!=result.model_pred]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>scores_max</th>\n",
              "      <th>score_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>Enfin, je ne voudrais pas conclure sans vous r...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102376</td>\n",
              "      <td>-0.247951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Cette mobilisation, pour naturelle qu'elle pu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.494533</td>\n",
              "      <td>-0.645300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Vous avez mon soutien, vous pouvez en être as...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.002535</td>\n",
              "      <td>-0.035471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>(</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045763</td>\n",
              "      <td>-0.205246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ... score_min\n",
              "5       5  ... -0.247951\n",
              "6       6  ... -0.645300\n",
              "9       9  ... -0.035471\n",
              "10     10  ... -0.205246\n",
              "\n",
              "[4 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mis9R5RpZsv_",
        "colab_type": "code",
        "outputId": "fe8664f1-5aee-4d36-b51e-d7a788d0eab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "result[6:7]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>scores_max</th>\n",
              "      <th>score_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Cette mobilisation, pour naturelle qu'elle pu...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.494533</td>\n",
              "      <td>-0.6453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... score_min\n",
              "6      6  ...   -0.6453\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyCUByqlnJ-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result['avg'] = result.iloc[:,5:7].mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlp9DZGlZ_Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_to_analyse = [i for i in result[5:6].Texte.str.split(' ')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBYBn5Ekay5F",
        "colab_type": "code",
        "outputId": "757b23af-0ecc-4b62-ee01-c03f24ae839c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "words_to_analyse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Enfin,',\n",
              "  'je',\n",
              "  'ne',\n",
              "  'voudrais',\n",
              "  'pas',\n",
              "  'conclure',\n",
              "  'sans',\n",
              "  'vous',\n",
              "  'remercier',\n",
              "  'pour',\n",
              "  \"l'engagement\",\n",
              "  'qui',\n",
              "  'a',\n",
              "  'été',\n",
              "  'le',\n",
              "  'vôtre,',\n",
              "  'lors',\n",
              "  'du',\n",
              "  'passage',\n",
              "  'de',\n",
              "  'la',\n",
              "  'tempête,',\n",
              "  'qui',\n",
              "  'a',\n",
              "  'malheureusement',\n",
              "  'affecté',\n",
              "  'certains',\n",
              "  'établissements',\n",
              "  'de',\n",
              "  'santé',\n",
              "  'de',\n",
              "  'notre',\n",
              "  'région']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUYIaXwPWJCp",
        "colab_type": "text"
      },
      "source": [
        "Now let's put in the model just one sentence and see how it behaves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH8JStdynLDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_to_analyse=pd.DataFrame(words_to_analyse).transpose()\n",
        "words_to_analyse=words_to_analyse.assign(sexe=1)\n",
        "words_to_analyse.columns=['Texte','sexe']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQ6fWsJb9x1",
        "colab_type": "code",
        "outputId": "216e6ff4-cd98-4161-841e-3676a3fecca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "words_to_analyse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Enfin,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>je</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ne</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>voudrais</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pas</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>conclure</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sans</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>vous</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>remercier</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>pour</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>l'engagement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>qui</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>été</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>le</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>vôtre,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lors</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>du</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>passage</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>la</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>tempête,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>qui</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>a</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>malheureusement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>affecté</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>certains</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>établissements</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>santé</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>notre</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>région</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Texte  sexe\n",
              "0            Enfin,     1\n",
              "1                je     1\n",
              "2                ne     1\n",
              "3          voudrais     1\n",
              "4               pas     1\n",
              "5          conclure     1\n",
              "6              sans     1\n",
              "7              vous     1\n",
              "8         remercier     1\n",
              "9              pour     1\n",
              "10     l'engagement     1\n",
              "11              qui     1\n",
              "12                a     1\n",
              "13              été     1\n",
              "14               le     1\n",
              "15           vôtre,     1\n",
              "16             lors     1\n",
              "17               du     1\n",
              "18          passage     1\n",
              "19               de     1\n",
              "20               la     1\n",
              "21         tempête,     1\n",
              "22              qui     1\n",
              "23                a     1\n",
              "24  malheureusement     1\n",
              "25          affecté     1\n",
              "26         certains     1\n",
              "27   établissements     1\n",
              "28               de     1\n",
              "29            santé     1\n",
              "30               de     1\n",
              "31            notre     1\n",
              "32           région     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk00zTm0nuiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "texts = words_to_analyse.Texte.values\n",
        "labels = words_to_analyse.sexe.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "num_truncated_tokens =[]\n",
        "original_text =[]\n",
        "tokenized_text=[]\n",
        "\n",
        "for text in texts:\n",
        "  original_text.append(text)\n",
        "  tokenized_text.append(tokenizer.tokenize(text))\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # text\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 5,           # We choose for now a max length of 500.\n",
        "                        pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                        return_attention_mask = True,   # Construct attention masks\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                        return_overflowing_tokens =True, # return overflowing token information\n",
        "                  )\n",
        "    \n",
        "    # Map tokens to their id in the dictionnary \n",
        "    # We add this to our list    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "    \n",
        "    # 3. Attention masks\n",
        "  attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JebL2SIZas99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vXr-JqnKbzJh",
        "colab": {}
      },
      "source": [
        "# We convert all this into tensors in order to be able to make it work on GPU \n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRQ9sN08bzJw",
        "colab": {}
      },
      "source": [
        "# Combine all above\n",
        "batch_size = 1\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "# We create data loaders for the train and validation dataset. \n",
        "dev_dataloader = DataLoader(\n",
        "            dataset,  # The training samples.\n",
        "            sampler = SequentialSampler(dataset), # We set to sequential and we keep track \n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYSzRARgbzJz",
        "colab": {}
      },
      "source": [
        "# Put model in evaluation mode \n",
        "gender_model.eval()\n",
        "\n",
        "total_pred,total_labels, total_eval_loss,scores_max,scores_min=[],[], [],[],[]\n",
        "for batch in dev_dataloader:\n",
        "  b_input_ids = batch[0].to(device)\n",
        "  b_input_mask = batch[1].to(device)\n",
        "  b_labels = batch[2].to(device)\n",
        "  \n",
        "  # We don't care about gradients for eval\n",
        "\n",
        "  with torch.no_grad(): \n",
        "    (loss, logits) = gender_model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "  total_eval_loss.append(loss.item())\n",
        "\n",
        "    # Move logits and labels to CPU \n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  score_max = np.amax(logits, axis=1).flatten().item()\n",
        "  score_min = np.amin(logits, axis=1).flatten().item()\n",
        "  pred_flat = np.argmax(logits, axis=1).flatten().item()\n",
        "  labels_flat = label_ids.flatten().item()\n",
        "\n",
        "  total_pred.append(pred_flat)\n",
        "  total_labels.append(labels_flat)\n",
        "  scores_max.append(score_max)\n",
        "  scores_min.append(score_min)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxSYfGDVauQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_dev_word=pd.DataFrame([total_labels,total_pred,scores_max,scores_min]).transpose()\n",
        "results_dev_word.columns=['returned_labels','model_pred','scores_max','score_min']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK64BxJBcIuy",
        "colab_type": "code",
        "outputId": "106a7a3a-1cad-4704-bf9f-dcd1dda6b85e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results_dev_word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>scores_max</th>\n",
              "      <th>score_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.032114</td>\n",
              "      <td>-0.111916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062017</td>\n",
              "      <td>-0.163058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166514</td>\n",
              "      <td>-0.276842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069258</td>\n",
              "      <td>-0.167983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.279248</td>\n",
              "      <td>-0.418742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069483</td>\n",
              "      <td>-0.167864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020976</td>\n",
              "      <td>-0.149210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015067</td>\n",
              "      <td>-0.069046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.065968</td>\n",
              "      <td>-0.164490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.051441</td>\n",
              "      <td>-0.211815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083952</td>\n",
              "      <td>-0.145390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085384</td>\n",
              "      <td>-0.219316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.279145</td>\n",
              "      <td>-0.413091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057478</td>\n",
              "      <td>-0.178586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166390</td>\n",
              "      <td>-0.302509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.093130</td>\n",
              "      <td>-0.190890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115808</td>\n",
              "      <td>-0.229120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102862</td>\n",
              "      <td>-0.224387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.112644</td>\n",
              "      <td>-0.221151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.246492</td>\n",
              "      <td>-0.322053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.202688</td>\n",
              "      <td>-0.317411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.234396</td>\n",
              "      <td>-0.354669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085384</td>\n",
              "      <td>-0.219316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.279145</td>\n",
              "      <td>-0.413091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.017742</td>\n",
              "      <td>-0.087174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.039858</td>\n",
              "      <td>-0.142743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.058719</td>\n",
              "      <td>-0.182367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053276</td>\n",
              "      <td>-0.155871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.246492</td>\n",
              "      <td>-0.322053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136135</td>\n",
              "      <td>-0.238461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.246492</td>\n",
              "      <td>-0.322053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.094192</td>\n",
              "      <td>-0.207393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.254257</td>\n",
              "      <td>-0.394947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    returned_labels  model_pred  scores_max  score_min\n",
              "0               1.0         0.0   -0.032114  -0.111916\n",
              "1               1.0         0.0    0.062017  -0.163058\n",
              "2               1.0         0.0    0.166514  -0.276842\n",
              "3               1.0         0.0    0.069258  -0.167983\n",
              "4               1.0         0.0    0.279248  -0.418742\n",
              "5               1.0         0.0    0.069483  -0.167864\n",
              "6               1.0         0.0    0.020976  -0.149210\n",
              "7               1.0         0.0    0.015067  -0.069046\n",
              "8               1.0         0.0    0.065968  -0.164490\n",
              "9               1.0         0.0    0.051441  -0.211815\n",
              "10              1.0         0.0    0.083952  -0.145390\n",
              "11              1.0         0.0    0.085384  -0.219316\n",
              "12              1.0         0.0    0.279145  -0.413091\n",
              "13              1.0         0.0    0.057478  -0.178586\n",
              "14              1.0         0.0    0.166390  -0.302509\n",
              "15              1.0         0.0    0.093130  -0.190890\n",
              "16              1.0         0.0    0.115808  -0.229120\n",
              "17              1.0         0.0    0.102862  -0.224387\n",
              "18              1.0         0.0    0.112644  -0.221151\n",
              "19              1.0         0.0    0.246492  -0.322053\n",
              "20              1.0         0.0    0.202688  -0.317411\n",
              "21              1.0         0.0    0.234396  -0.354669\n",
              "22              1.0         0.0    0.085384  -0.219316\n",
              "23              1.0         0.0    0.279145  -0.413091\n",
              "24              1.0         0.0   -0.017742  -0.087174\n",
              "25              1.0         0.0    0.039858  -0.142743\n",
              "26              1.0         0.0    0.058719  -0.182367\n",
              "27              1.0         0.0    0.053276  -0.155871\n",
              "28              1.0         0.0    0.246492  -0.322053\n",
              "29              1.0         1.0    0.136135  -0.238461\n",
              "30              1.0         0.0    0.246492  -0.322053\n",
              "31              1.0         0.0    0.094192  -0.207393\n",
              "32              1.0         0.0    0.254257  -0.394947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la9fCVARcJsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = [words_to_analyse.reset_index(), results_dev_word]\n",
        "results_dev_word = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsKS2lHDcOtT",
        "colab_type": "code",
        "outputId": "c8ceb7c7-2f1e-4737-fcfa-108c1390e071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "results_dev_word.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>scores_max</th>\n",
              "      <th>score_min</th>\n",
              "      <th>avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Enfin,</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.032114</td>\n",
              "      <td>-0.111916</td>\n",
              "      <td>-0.072015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>je</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062017</td>\n",
              "      <td>-0.163058</td>\n",
              "      <td>-0.050521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ne</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166514</td>\n",
              "      <td>-0.276842</td>\n",
              "      <td>-0.055164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>voudrais</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069258</td>\n",
              "      <td>-0.167983</td>\n",
              "      <td>-0.049362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>pas</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.279248</td>\n",
              "      <td>-0.418742</td>\n",
              "      <td>-0.069747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index     Texte  sexe  ...  scores_max  score_min       avg\n",
              "0      0    Enfin,     1  ...   -0.032114  -0.111916 -0.072015\n",
              "1      1        je     1  ...    0.062017  -0.163058 -0.050521\n",
              "2      2        ne     1  ...    0.166514  -0.276842 -0.055164\n",
              "3      3  voudrais     1  ...    0.069258  -0.167983 -0.049362\n",
              "4      4       pas     1  ...    0.279248  -0.418742 -0.069747\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 317
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WF8SiShcPqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_dev_word['avg'] = results_dev_word.iloc[:,5:7].mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJC0IC54hoxg",
        "colab_type": "code",
        "outputId": "9a64a7ed-4cb8-4696-b83b-5bcc5bd73ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "results_dev_word.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>scores_max</th>\n",
              "      <th>score_min</th>\n",
              "      <th>avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Enfin,</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.032114</td>\n",
              "      <td>-0.111916</td>\n",
              "      <td>-0.072015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>je</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062017</td>\n",
              "      <td>-0.163058</td>\n",
              "      <td>-0.050521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ne</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166514</td>\n",
              "      <td>-0.276842</td>\n",
              "      <td>-0.055164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>voudrais</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069258</td>\n",
              "      <td>-0.167983</td>\n",
              "      <td>-0.049362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>pas</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.279248</td>\n",
              "      <td>-0.418742</td>\n",
              "      <td>-0.069747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index     Texte  sexe  ...  scores_max  score_min       avg\n",
              "0      0    Enfin,     1  ...   -0.032114  -0.111916 -0.072015\n",
              "1      1        je     1  ...    0.062017  -0.163058 -0.050521\n",
              "2      2        ne     1  ...    0.166514  -0.276842 -0.055164\n",
              "3      3  voudrais     1  ...    0.069258  -0.167983 -0.049362\n",
              "4      4       pas     1  ...    0.279248  -0.418742 -0.069747\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFVzNZ63jWkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = [i for i in results_dev_word['scores_max']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeS_HMSBlQQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot= results_dev_word[['scores_max']].transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dbId7RmuG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot.columns=list(results_dev_word.Texte)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilmXHJJcloMK",
        "colab_type": "code",
        "outputId": "8e162f8d-4553-4213-f03a-b26b558813a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        " \n",
        "# Default heatmap: just a visualization of this square matrix\n",
        " sns.heatmap(df_plot)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAFKCAYAAAAqvSVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIUdENEQSckREQyQhR0Q0RBJyRERD/H/2YDcFkmzrSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VXzb-ayoB-o",
        "colab_type": "text"
      },
      "source": [
        "Faire des annotations pour la fin du doc et nettoyer mettre en forme\n",
        "Faire des fonctions pour la fin \n",
        "Amélioer le graphique . Est ce que c'est intelligent par mot ? \n",
        "Remplacer score max par la moyenne des logits à chercher à la fin du train \n",
        "Analyser un peu et mettre au propre sur la feuille\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlwRs8Jgn8DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}