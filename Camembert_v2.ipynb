{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Camembert_v2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9364e133f31349a29514b01406220077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c4e8b3c76664e27b0751b2ba259bbc0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85ebe676c7c4491aa93c79c9e21a9d02",
              "IPY_MODEL_77a0291fd1494a1f917dbbaad6c81dad"
            ]
          }
        },
        "7c4e8b3c76664e27b0751b2ba259bbc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85ebe676c7c4491aa93c79c9e21a9d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_abe69e966d0e47cc9c25f65403c0aafe",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 810912,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 810912,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cef797e99f054879b11258796fea3454"
          }
        },
        "77a0291fd1494a1f917dbbaad6c81dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e3ea869fb6eb4b1b83507abf91579364",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 811k/811k [00:01&lt;00:00, 704kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b787d7f848104c718841f2a5093d4c68"
          }
        },
        "abe69e966d0e47cc9c25f65403c0aafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cef797e99f054879b11258796fea3454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3ea869fb6eb4b1b83507abf91579364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b787d7f848104c718841f2a5093d4c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerezamo/NLP_brouillon/blob/master/Camembert_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxLW3uyHTSN",
        "colab_type": "text"
      },
      "source": [
        "# CamemBERT classification model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JD-Tb0HdvN",
        "colab_type": "code",
        "outputId": "8346ccb8-1a06-4607-a8c9-cd7180d254bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os \n",
        "os.getcwd()\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bxA1IEgH-GI",
        "colab_type": "text"
      },
      "source": [
        "### Set up Colab GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mF6Hs6yH2A5",
        "colab_type": "code",
        "outputId": "1897ff0f-a896-4761-f9c9-c5d9617fcb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%tensorflow_version 1.x\n",
        "# First you should go in 'Edit' -> 'Notebook settings' -> Add device GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_F6NV3IaCY",
        "colab_type": "text"
      },
      "source": [
        "Let's now tell torch that one GPU is available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4fjemjIVoQ",
        "colab_type": "code",
        "outputId": "4d442528-6309-499e-92a7-ed00551f9db0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "        \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LcRSHffJyZm",
        "colab_type": "text"
      },
      "source": [
        "Please check GPU capacity that you were given. You might want to reduce the batch size further in the code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGwjFzizIsMI",
        "colab_type": "text"
      },
      "source": [
        "Let's install the Hugging Face Library transformer package "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--g7cokfIrpT",
        "colab_type": "code",
        "outputId": "4434f551-472e-4f6d-af00-2bdc8cd23316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "! pip install transformers "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\r\u001b[K     |▋                               | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 34.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 39.9MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 22.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 21.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 143kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 163kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 184kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 194kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 215kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 235kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 245kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 256kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 266kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 276kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 286kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 307kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 317kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 327kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 337kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 348kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 358kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 368kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 378kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 389kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 399kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 409kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 419kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 430kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 440kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 450kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 460kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 471kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 481kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 491kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 501kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 512kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 522kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 532kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 552kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 49.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=8b652a3b9fb0e7b87f5038f78c601da0baadaec22639e9b8c386d228b14e0092\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4OKq8Z4JId9",
        "colab_type": "text"
      },
      "source": [
        "### Loading our corpus and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCVLtje9_Rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "29b6a84b-bf14-4aa6-991a-b6199911e765"
      },
      "source": [
        "# Import medium_df_desq in \"files\" (on the left) => ICI prendre du github ??\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df=pd.read_csv('fullDF.csv',encoding='utf-8')\n",
        "df=df[~df.Texte.str.startswith(\"Q-\")]\n",
        "df=df[~df.Texte.str.startswith(\"R-\")]\n",
        "\n",
        "# We replace the labels in a more normalized way : 0=men, 1=women \n",
        "df.sexe=df.sexe.replace(1,0)\n",
        "df.sexe=df.sexe.replace(2,1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-48d7c6e7d332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset into a pandas dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullDF.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTexte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTexte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"R-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fullDF.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn1jaLDV6VIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "6786fb1a-5181-4534-d735-890a64f41b96"
      },
      "source": [
        "df=df[['Id','Titre','Theme','Prenom','Nom','Date','Tags','Texte','sexe']]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-441c6a5b56d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Titre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Theme'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Prenom'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Nom'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tags'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Texte'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sexe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48nemL1W3XKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "fb63df48-7be7-418c-f7ec-3a9844814128"
      },
      "source": [
        "# For this part we will just remove urls\n",
        "import re\n",
        "def remove_urls (text):\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
        "    return text \n",
        "\n",
        "df['Texte']=df.Texte.apply(remove_urls)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-59efa299ff88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Texte'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTexte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgllpcqJevp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make results reproducible \n",
        "seed_val = 2003"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8ICVGvUBLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unbalanced_preprocess(df,seed_val,frac_val):\n",
        "\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #Shuffle the data \n",
        "  df_unbalanced=df.sample(frac=frac_val).reset_index()\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in the unbalanced corpus : {:,}\\n'.format(df_unbalanced.shape[0]))\n",
        "  prop = (len(df_unbalanced[df_unbalanced.sexe==1])/len(df_unbalanced))*100\n",
        "  print('Proportions of women in the unbalanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_unbalanced"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IdYJSSRUdD0",
        "colab_type": "code",
        "outputId": "622517a2-cc87-49e4-b6da-3434e3f82ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "df_unbalanced = unbalanced_preprocess(df,seed_val,frac_val=0.5) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-66b71b0da221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_unbalanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munbalanced_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrac_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd4SCY6LUuwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def balanced_preprocess(df,seed_val,frac_val):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Let's take a balanced sample \n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  #Shuffle the data and taking half of the sample in order not to have to many data compared to the other samples \n",
        "  df_balanced=df.sample(frac=frac_val,random_state=seed_val).reset_index()\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this corpus : {:,}\\n'.format(df_balanced.shape[0]))\n",
        "  prop = (len(df_balanced[df_balanced.sexe==1])/len(df_balanced))*100\n",
        "  print('Proportions of women in the balanced corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_balanced\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeQif_N0X4uW",
        "colab_type": "code",
        "outputId": "2b6ea87f-280e-4eef-dc65-603873ae3515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "df_balanced = balanced_preprocess(df,seed_val,frac_val=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-394a37bdbd12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalanced_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrac_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'balanced_preprocess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgxCphDnCiPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "def sent_detector_mano(x):\n",
        "    \"\"\"\n",
        "        Détection de phrase à la main.\n",
        "        Input : document\n",
        "        Output : liste de phrases\n",
        "        Problème avec les phrases finissant par : entrainant souvent une liste. \n",
        "        De même avec ;. Tentative réalisée\n",
        "        \n",
        "    \"\"\"\n",
        "    lst =[]\n",
        "    phrase = []\n",
        "    i = 0\n",
        "    for caractere in x: \n",
        "        if not (caractere == ' ' and len(phrase) == 0) :\n",
        "            phrase.append(caractere)\n",
        "        if caractere in '?!.:;':\n",
        "            if caractere == ':':\n",
        "                if x[i+1].isupper() or x[i+2].isupper() or x[i+1] == '-' or x[i+2] == '-':\n",
        "                    lst.append(''.join(phrase))\n",
        "                    phrase = []\n",
        "            elif caractere == ';':\n",
        "                if x[i+1].isupper() or x[i+2].isupper() or x[i+1] == '-' or x[i+2] == '-':\n",
        "                    lst.append(''.join(phrase))\n",
        "                    phrase = []\n",
        "            elif phrase != '.' or phrase != '?' or phrase != '!':\n",
        "                lst.append(''.join(phrase))\n",
        "                phrase = []\n",
        "        i+=1\n",
        "    return lst\n",
        "def split_document_to_limit(MAX_TOKENS,df):\n",
        "  lst= []\n",
        "  for index,row in df.iterrows():\n",
        "    identifiant = row.Id\n",
        "    label = row.sexe\n",
        "    phrase = []\n",
        "    for token in row.Texte.split(' '):\n",
        "      if len(phrase) < MAX_TOKENS:\n",
        "        phrase.append(token)\n",
        "      else:\n",
        "        lst += [(identifiant,label,' '.join(phrase),len(phrase))]\n",
        "        phrase = []\n",
        "    if len(phrase)>1:\n",
        "      lst += [(identifiant,label,' '.join(phrase),len(phrase))]\n",
        "  return pd.DataFrame(lst,columns=['index_df','sexe','Texte','Length'])\n",
        "def split_document_to_limit_phrases(MAX_TOKENS,df):\n",
        "  lst= []\n",
        "  for index,row in df.iterrows():\n",
        "    identifiant = row.Id\n",
        "    label = row.sexe\n",
        "    phrase = ''\n",
        "    for phrases in sent_detector_mano(row.Texte):\n",
        "      if len(phrase.split(' ')) + len(phrases.split(' ')) < MAX_TOKENS:\n",
        "        phrase+= \" \" + phrases\n",
        "      else:\n",
        "        lst += [(identifiant,label,phrase,len(phrase.split(' ')))]\n",
        "        phrase = ''\n",
        "    lst += [(identifiant,label,phrase,len(phrase.split(' ')))]\n",
        "    phrase = ''\n",
        "  return pd.DataFrame(lst,columns=['index_df','sexe','Texte','Length'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3qnfTDXCwF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def  balanced_splitted(df,seed_val,frac_val,max_tokens):\n",
        "  # Let's take a balanced sample \n",
        "  df_m = df.loc[df['sexe'] == 0]\n",
        "  df_f = df.loc[df['sexe'] == 1] \n",
        "  df_m = df_m[0:len(df_f)]\n",
        "  df = df_f.append(df_m)\n",
        "\n",
        "  df=split_document_to_limit_phrases(max_tokens,df)\n",
        "\n",
        "  df_balanced_split=df.sample(frac=frac_val,random_state=seed_val).reset_index()\n",
        "\n",
        "  # Report the number of speeches in the corpus.\n",
        "  print('Number of text in this balanced splitted corpus : {:,}\\n'.format(df_balanced_split.shape[0]))\n",
        "  prop = (len(df_balanced_split[df_balanced_split.sexe==1])/len(df_balanced_split))*100\n",
        "  print('Proportions of women in the balanced splitted corpus : {}\\n'.format(prop))\n",
        "\n",
        "  return df_balanced_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEpbT1JYf5-",
        "colab_type": "code",
        "outputId": "b2b20b82-eec1-432c-b8d6-0b2b77baa623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "df_balanced_split = balanced_splitted(df,seed_val,frac_val=0.5,max_tokens=512)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-92c08eaef5d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_balanced_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbalanced_splitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrac_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNtHK4DXGCJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e0119adf-71c0-40ab-e5a7-0a77417cceb0"
      },
      "source": [
        "df_balanced_split"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b5302dba82a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_balanced_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_balanced_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTKQ0QnAZ_wu",
        "colab_type": "text"
      },
      "source": [
        "**We propose 3 samples to train our model :**\n",
        "\n",
        "\n",
        "1.   **Unbalanced sample**\n",
        "\n",
        "We take the raw data without any further treatment.\n",
        "\n",
        "2.   **Balanced sample**\n",
        "\n",
        "The second option consist in deleting randomly part of male speeches in order to get a balanced sample. Indeed, in the case of unbalanced sample our model could decide to classify all speakers in the male category which would lead to a 0.75 accuracy in our case study. In order to avoid this we feed the model with the same proportions of male and female speakers. Other kind of treatments exist to deal with unbalanced sample. This one is the simpliest one and we could argue that there is a possibility that the deleted sample contains important information that we therefore miss. However we believe that in our case this is not a big issue. Our unbalanced sample is quite large for both female and male.\n",
        "\n",
        "3. **Balanced and splitted sample**\n",
        "\n",
        "The third option is a response to the max length constraint of BERT models. Our text samples are big and contain much more tokens than the 512 limit. In the first two options we decide to just feed the model with the 512 first tokens and thus delete the rest of them. In this third option we cut the text into x parts containing 500 tokens each. All parts of the speech will serve to feed the model. By this technique we do not loose potential important informations at the end of the text. A lot of other techniques have been employed (see ref !!! PUT). We decide to stick to this one in this project. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ressz8h6OHxn",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization of our text and preparing to feed CamemBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntpzo9X5SSjA",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the Camembert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mggkz5R9g8dD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9364e133f31349a29514b01406220077",
            "7c4e8b3c76664e27b0751b2ba259bbc0",
            "85ebe676c7c4491aa93c79c9e21a9d02",
            "77a0291fd1494a1f917dbbaad6c81dad",
            "abe69e966d0e47cc9c25f65403c0aafe",
            "cef797e99f054879b11258796fea3454",
            "e3ea869fb6eb4b1b83507abf91579364",
            "b787d7f848104c718841f2a5093d4c68"
          ]
        },
        "outputId": "34119f2c-9d7b-47e0-a6d6-296543518536"
      },
      "source": [
        "# Import Camembert tokenizer\n",
        "from transformers import CamembertTokenizer\n",
        "# We choose a right padding side for the moment and we will test for a left padding side on a second stage\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right') #left"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9364e133f31349a29514b01406220077",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=810912, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoaZUUBChM_J",
        "colab_type": "code",
        "outputId": "fcd6a5d6-11d1-4e81-d193-cea461d2c4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Print the original text.\n",
        "print(' Original: ', df.Texte[0])\n",
        "\n",
        "# Print the text split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(df.Texte[0]))\n",
        "\n",
        "# Print the text mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.Texte[0])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9cc11c924803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Original: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTexte\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Print the text split into tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tokenized: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTexte\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yAtMsdR9HB",
        "colab_type": "text"
      },
      "source": [
        "#### Adding special tokens to the start and end of the text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXlKcUdlYetx",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing steps : \n",
        "\n",
        "\n",
        "1.   **Add special tokens [CLS] [SEP]** \n",
        "\n",
        "According to the documentation we need to add special tokens to the start and end of the text Moreover, for camembert we should add a space between CLS and the first token (not sure here, we have to ask benjamin). \n",
        "\n",
        "2.   **Pad and truncate all texts to a single number**\n",
        "\n",
        "Pretrained transformes like Camembert only accept input of the same length. Our corpus contains large texts and we have to pad them in order to be able to feed Camembert. We will set the max length to a large number in order to get all information possible in the text. We choose a max length of 500 which is almost the maximum (512) \"sentence\" length  accepted. We are aware that this choice will impact a lot training speed.\n",
        "\n",
        "3.   **Construct an attention mask**\n",
        "\n",
        "Attention masks are just set to 1 when the token have to be analyzed and 0 otherwise (padded tokens). All our attention mask should be 1 with this corpus. \n",
        "\n",
        "\n",
        "\n",
        "For sake of simplicity and to avoid errors we will use the function encode_plus of the library which is really convenient. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XKNZMJvSb2w",
        "colab_type": "text"
      },
      "source": [
        "#### Length and attention mask "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HF89V-xSgGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_to_feed(df,length,batch_size_value,length_train):\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  texts = df.Texte.values\n",
        "  labels = df.sexe.values\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  num_truncated_tokens =[]\n",
        "  # Apply function to our corpus\n",
        "  for text in texts:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # text\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = length,           # We choose for now a max length of 500.\n",
        "                          pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                          return_attention_mask = True,   # Construct attention masks\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          return_overflowing_tokens =True, # return overflowing token information\n",
        "                    )\n",
        "      \n",
        "      # Map tokens to their id in the dictionnary \n",
        "      # We add this to our list    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "  \n",
        "      #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "      \n",
        "      # 3. Attention masks\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # We convert all this into tensors in order to be able to make it work on GPU \n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Original text and transformed tensor print \n",
        "  print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "  print(\" \")\n",
        "  print('Original: ', texts[0][0:100])\n",
        "  print('IDs:', input_ids[0][0:100])\n",
        "  print('Attention masks:', attention_masks[0][0:100])\n",
        "  print('labels',labels[0])\n",
        "\n",
        "\n",
        "  # Combine all above\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Let's create a 80-20 train / validation dataset \n",
        "  train_size = int(length_train * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "  print(\"-------------------------------------------------\")\n",
        "  print(\" \")\n",
        "  print(\"How many texts do we have in the train and validation sample ? \")\n",
        "  print(\" \")\n",
        "  print('We have {} training texts'.format(train_size))\n",
        "  print('We have {} validation texts'.format(val_size))\n",
        "  print(\" \")\n",
        "  print(\"-------------------------------------------------\")\n",
        "\n",
        "  # We set the size of the batch ( usually set around 16 or 32), we will take the lower bound because of the large text length\n",
        "  batch_size = batch_size_value\n",
        "\n",
        "  # We create data loaders for the train and validation dataset. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_set,  # The training samples.\n",
        "              sampler = RandomSampler(train_set), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  val_dataloader = DataLoader(\n",
        "              val_set, # The validation samples.\n",
        "              sampler = SequentialSampler(val_set), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )\n",
        "  \n",
        "  print('Data loaders created for train [0] and val [1]')\n",
        "\n",
        "  return train_dataloader, val_dataloader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b_OFrL3mIBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('############### Unbalanced sample ################')\n",
        "train_loader_unbalanced, val_loader_unbalanced = prepare_to_feed(df_unbalanced,length=500,batch_size_value=2,length_train=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIQMcNAgekhx",
        "colab_type": "code",
        "outputId": "fded0ec9-be29-4045-cefa-fd3e19ec940d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print('############### Balanced sample ################')\n",
        "train_loader_balanced, val_loader_balanced = prepare_to_feed(df_balanced,length=500,batch_size_value=2,length_train=0.8)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############### Balanced sample ################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-969d04d90cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'############### Balanced sample ################'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loader_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_to_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_balanced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prepare_to_feed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sxjN5O5mMe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('############### Balanced sample split ################')\n",
        "train_loader_balanced_split, val_loader_balanced_split = prepare_to_feed(df_balanced_split,length=500,batch_size_value=2,length_train=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6YDmQsgljf",
        "colab_type": "text"
      },
      "source": [
        "5 and 6 seem to be the [CLS] and [SEP] special tokens \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poTTEJX1hoUK",
        "colab_type": "text"
      },
      "source": [
        "### CamemBERT Sequence Classification model tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1VeJI0lDwf",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MPOVB7iRcl",
        "colab_type": "text"
      },
      "source": [
        "We will finally build up our model. We will use the  CamemBERT model for sequence classification which includes a special top layer designed for this task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRHhHzjKgAC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing from transformers\n",
        "from transformers import CamembertForSequenceClassification, CamembertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMdM-QqgAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the model\n",
        "gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUKynoykf_9y",
        "colab_type": "code",
        "outputId": "89009a9e-f0d6-45de-d13a-1becbcfd30f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We run the model on the colab GPU \n",
        "gender_model.cuda()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CamembertForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWyHWg5xlBck",
        "colab_type": "text"
      },
      "source": [
        "Optimizers and Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgwmHirxnEie",
        "colab_type": "text"
      },
      "source": [
        "#### Constructing the training and validation loop \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZDt2ZElwcJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score \n",
        "\n",
        "def create_report(labels,preds) : \n",
        "  pred_flat= np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  F1_score = f1_score(labels_flat,pred_flat,zero_division=1)\n",
        "  Accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "  return F1_score, Accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROTX-yvn7kaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUB8c4k_t1UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_val_gendermodel(train_loader, val_loader, epochs_val,seed_val,device,lr_value):\n",
        "\n",
        "  ############################  IMPORT MODEL ################################################\n",
        "  from transformers import CamembertForSequenceClassification\n",
        "  gender_model = CamembertForSequenceClassification.from_pretrained(\n",
        "    \"camembert-base\", \n",
        "    num_labels = 2, # We have two different labels Women = 1 and Men =0   \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, )\n",
        "\n",
        "  model = gender_model\n",
        "  model.cuda()\n",
        "  \n",
        "  ############################## RANDOM SEED ##################################################\n",
        "\n",
        "  import random\n",
        " # Let's put a seed to make this result reproducible \n",
        "  seed=seed_val\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  ############################### LEARNING RATE SCHEDULER #######################################\n",
        "\n",
        "  # https://huggingface.co/transformers/migration.html \n",
        "  # https://pytorch.org/docs/stable/optim.html (default values)\n",
        "\n",
        "  import torch.nn as nn\n",
        "  import torch.optim as optim\n",
        "  from transformers import AdamW\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  epochs = epochs_val # In order to fine tune our model we will first set the number of epochs to 4.\n",
        "\n",
        "  # We choose Binary cross enthropy with logits loss for the loss computation. It seems to be the most adapted loss to our problem. \n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  #Implements Adam algorithm with weight decay fix.\n",
        "  opti = AdamW(model.parameters(),\n",
        "                    lr =lr_value, # learning rate (default = 1e-3)\n",
        "                    eps = 1e-8 # prevents division by 0 (default = 1e-8)\n",
        "                  )\n",
        "\n",
        "  num_training_steps = len(train_loader) * epochs\n",
        "  # Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "  scheduler = get_linear_schedule_with_warmup(opti, \n",
        "                                              num_warmup_steps = 0,\n",
        "                                              num_training_steps = num_training_steps)\n",
        "  \n",
        "  \n",
        "  # We want to evaluate the training phase \n",
        "  training_stats = []\n",
        "\n",
        "  for ep in range(0, epochs):\n",
        "    print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "    print('Training starts')\n",
        "\n",
        "    ################################### TRAINING ################################\n",
        "\n",
        "    #Put the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Set the train loss for the epoch to 0 \n",
        "    total_train_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "      # Clear gradients \n",
        "      model.zero_grad() # (opti.zerograd ? )\n",
        "\n",
        "      # Cpy the 3 batch to GPU \n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      \n",
        "      #return loss and logits\n",
        "      loss, logits = model(b_input_ids, \n",
        "                          token_type_ids=None, \n",
        "                          attention_mask=b_input_mask, \n",
        "                          labels=b_labels) \n",
        "      \n",
        "      # Accumulate training loss for all batches \n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      #Backpropagating the gradients \n",
        "      loss.backward()\n",
        "\n",
        "      # Prevent exploding gradients problem  (forcing the gradients to be small, the parameter updates will not push the parameters too far from their previous values)\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      # Update parameters \n",
        "      opti.step()\n",
        "\n",
        "      # Update learning rate schedule\n",
        "      scheduler.step()\n",
        "\n",
        "    #Calculate the average training loss over all batches  \n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print('')\n",
        "    print('Validation starts')\n",
        "\n",
        "    ###################### VALIDATION #############################\n",
        "\n",
        "    # Put model in evaluation mode \n",
        "    model.eval()\n",
        "\n",
        "    # Set statistics to 0\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    total_eval_f1=0\n",
        "    total_roc_auc = 0 \n",
        "\n",
        "    # Confusion matrix ?\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in val_loader:\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "      \n",
        "      # We don't care about gradients for eval\n",
        "\n",
        "      with torch.no_grad(): \n",
        "        (loss, logits) = model(b_input_ids, \n",
        "                                  token_type_ids=None, \n",
        "                                  attention_mask=b_input_mask,\n",
        "                                  labels=b_labels)\n",
        "      total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU \n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "      F1_score, Accuracy = create_report(label_ids,logits)\n",
        "\n",
        "      # Accumulation accuracy for all batch\n",
        "      total_eval_accuracy += Accuracy\n",
        "\n",
        "      # Accumulation f1 for all batch\n",
        "      total_eval_f1 += F1_score\n",
        "      \n",
        "      #Final accuracy on all batch\n",
        "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      #Final f1 on all batch\n",
        "    avg_val_f1 = total_eval_f1 / len(val_loader)\n",
        "    print(\"  F1_score: {0:.2f}\".format(avg_val_f1))\n",
        "\n",
        "      #Final loss over all batch\n",
        "    avg_val_loss = total_eval_loss / len(val_loader)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "    training_stats.append(\n",
        "          {\n",
        "              'epoch': ep + 1,\n",
        "              'Train Loss': avg_train_loss,\n",
        "              'Val Loss': avg_val_loss,\n",
        "              'Val Accur.': avg_val_accuracy,\n",
        "              'Val F1' : avg_val_f1,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Done !\")\n",
        "\n",
        "  return  training_stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anria4-x6FHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_model_1(results):\n",
        "  '''\n",
        "  Input : statistics of the model \n",
        "  Output : training and valid loss \n",
        "  ''' \n",
        "  df_stats = pd.DataFrame(data=results)\n",
        "  df_stats = df_stats.set_index('epoch')\n",
        "  print(df_stats)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  % matplotlib inline\n",
        "  import seaborn as sns\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "  # Plot the learning curve.\n",
        "  plt.plot(df_stats['Train Loss'], 'b-o', label=\"Training\")\n",
        "  plt.plot(df_stats['Val Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "  # Label the plot.\n",
        "  plt.title(\"Training & Validation Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.xticks([1, 2, 3, 4, 5])\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqNRTJME5oHs",
        "colab_type": "code",
        "outputId": "27472e60-287d-4445-ba6b-76d4067f4cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "results_unbalanced = train_val_gendermodel(train_loader=train_loader_unbalanced, val_loader=val_loader_unbalanced, epochs_val=3,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 3 ==============\n",
            "Training starts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-378920b02196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_unbalanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_gendermodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader_unbalanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader_unbalanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-571ccddbdd22>\u001b[0m in \u001b[0;36mtrain_val_gendermodel\u001b[0;34m(train_loader, val_loader, epochs_val, seed_val, device, lr_value)\u001b[0m\n\u001b[1;32m     77\u001b[0m                           \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                           \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                           labels=b_labels) \n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;31m# Accumulate training loss for all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Double for argument #2 'target' in call to _thnn_nll_loss_forward"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKu-O0c653tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_model_1(results_unbalanced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH9QNCMXR0zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_balanced = train_val_gendermodel(train_loader=train_loader_balanced, val_loader=val_loader_balanced, epochs_val=3,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Pti-Dk59DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_model_1(results_balanced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb6Ca8qmdT74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_balanced_split = train_val_gendermodel(train_loader=train_loader_balanced_split, val_loader=val_loader_balanced_split, epochs_val=3,seed_val=2020,device=device,lr_value=5e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJSzx-x35-jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_model_1(results_balanced_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLxa4te3gzi-",
        "colab_type": "text"
      },
      "source": [
        "Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXUkXjfg0s2",
        "colab_type": "text"
      },
      "source": [
        "#### Training the optimal model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LLOEa3g2wL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bcb5ae11-4d68-40f3-f133-bbc4ebebf4fe"
      },
      "source": [
        "df_eval= balanced_splitted(df,seed_val,frac_val=1,max_tokens=500)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 10,121\n",
            "\n",
            "Proportions of women in the balanced splitted corpus : 51.09178934887857\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa_3vh-698n6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We prepare another sample which will be dedicated to further qualitative analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL1SsPWU98tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3883c70c-0ee7-4081-86fc-dc75520aaee3"
      },
      "source": [
        "len_train = round(0.97*len(df_eval))\n",
        "df_balanced_split= df_eval[0:len_train]\n",
        "dev_balanced_split=df_eval[len_train:len(df_eval)]\n",
        "\n",
        "# Report the number of speeches in the corpus.\n",
        "print('Number of text in this balanced splitted corpus : {:,}\\n'.format(df_balanced_split.shape[0]))\n",
        "print('Number of text in the development sample : {:,}\\n'.format(dev_balanced_split.shape[0]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of text in this balanced splitted corpus : 9,817\n",
            "\n",
            "Number of text in the development sample : 304\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2CdFou898x8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "8b414af2-35ea-4665-9b07-d8615e20e4c9"
      },
      "source": [
        "train_loader_balanced_split, val_loader_balanced_split = prepare_to_feed(df_balanced_split,length=500,batch_size_value=16,length_train=0.9)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:   Le protocole du 19 décembre 2018 a permis de décider d'une augmentation de salaire de plus de 100 e\n",
            "IDs: tensor([    5,    54,  5996,    25,   653,   753,   552,    33,   994,     8,\n",
            "         4708,    18,    11,    70,  3708,     8,  4360,     8,    40,     8,\n",
            "          779,   982,  2607,    10,    37,   250,    24,   166,  6436,     9,\n",
            "         2335,    17,    11,  1629,     8,    13,  8776,     7,    63,   296,\n",
            "         6267,  6132,    20,  1964, 11805,     8,  3508,    22,  3100,    25,\n",
            "         1625,    25,  2011,    14,    20,   643,     8,   225,     8,   166,\n",
            "         6436,     7,    22,   770,    52,    11, 12522,     8,    17,    11,\n",
            "         1563,    25,   125,     8,   225,     9, 16078,    68,  3708,    10,\n",
            "        16054,    10,     7,    63,   296,    19,  1149,    18,    11,  7877,\n",
            "           17,    11,  1960,    27,    63,    63,   464,  5499,     8, 14901])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1])\n",
            "labels tensor(0)\n",
            "-------------------------------------------------\n",
            " \n",
            "How many texts do we have in the train and validation sample ? \n",
            " \n",
            "We have 8835 training texts\n",
            "We have 982 validation texts\n",
            " \n",
            "-------------------------------------------------\n",
            "Data loaders created for train [0] and val [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-TPR1CAOlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "31c9bfab-cdea-47e1-897c-8abdbe0a5f20"
      },
      "source": [
        "############################  IMPORT MODEL ################################################\n",
        "from transformers import CamembertForSequenceClassification\n",
        "gender_model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", \n",
        "                                                                  num_labels = 2, \n",
        "                                                                  output_attentions = False, \n",
        "                                                                  output_hidden_states = False, )\n",
        "\n",
        "gender_model.cuda()\n",
        "############################## RANDOM SEED ##################################################\n",
        "\n",
        "import random\n",
        "seed=seed_val\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "############################### LEARNING RATE SCHEDULER #######################################\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3 \n",
        "\n",
        "#Implements Adam algorithm with weight decay fix.\n",
        "opti = AdamW(gender_model.parameters(),\n",
        "              lr =5e-5, # learning rate (default = 1e-3)\n",
        "              eps = 1e-8 # prevents division by 0 (default = 1e-8)\n",
        "            )\n",
        "\n",
        "num_training_steps = len(train_loader_balanced_split) * epochs\n",
        "\n",
        "# Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period (0 here)\n",
        "scheduler = get_linear_schedule_with_warmup(opti, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = num_training_steps)\n",
        "\n",
        "\n",
        "for ep in range(0, epochs):\n",
        "  print('===========Starting Epoch {} / {} =============='.format(ep+1,epochs))\n",
        "  print('Training starts')\n",
        "\n",
        "  ################################### TRAINING ################################\n",
        "\n",
        "  #Put the model in training mode\n",
        "  gender_model.train()\n",
        "\n",
        "  # Set the train loss for the epoch to 0 \n",
        "  total_train_loss = 0\n",
        "\n",
        "  for step, batch in enumerate(train_loader_balanced_split):\n",
        "    # Clear gradients \n",
        "    gender_model.zero_grad() # (opti.zerograd ? )\n",
        "\n",
        "    # Cpy the 3 batch to GPU \n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    \n",
        "    #return loss and logits\n",
        "    loss, logits = gender_model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask, \n",
        "                                labels=b_labels) \n",
        "    \n",
        "    # Accumulate training loss for all batches \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    #Backpropagating the gradients \n",
        "    loss.backward()\n",
        "\n",
        "    # Prevent exploding gradients problem  (forcing the gradients to be small, the parameter updates will not push the parameters too far from their previous values)\n",
        "    torch.nn.utils.clip_grad_norm_(gender_model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters \n",
        "    opti.step()\n",
        "\n",
        "    # Update learning rate schedule\n",
        "    scheduler.step()\n",
        "\n",
        "  #Calculate the average training loss over all batches  \n",
        "  avg_train_loss = total_train_loss / len(train_loader_balanced_split)\n",
        "  print(\"\")\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========Starting Epoch 1 / 3 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.51\n",
            "===========Starting Epoch 2 / 3 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.35\n",
            "===========Starting Epoch 3 / 3 ==============\n",
            "Training starts\n",
            "\n",
            "  Average training loss: 0.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFpRse9-DAAy",
        "colab_type": "text"
      },
      "source": [
        "Let's compute some statistics on the performance of this final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMAECbOYC_Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def evaluation_loop(model,eval_loader): \n",
        "  # Put model in evaluation mode \n",
        "  model.eval()\n",
        "  total_eval_loss,total_pred,total_label,total_logits=[],[],[],[]\n",
        "\n",
        "  for batch in eval_loader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      loss, logits = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "    #total_eval_loss += loss.item()\n",
        "\n",
        "      # Move logits and labels to CPU \n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      pred= np.argmax(logits, axis=1).flatten()\n",
        "      labels_flat = label_ids.flatten()\n",
        "\n",
        "    # Accumulation accuracy for all batch\n",
        "      total_pred += pred.tolist()\n",
        "\n",
        "    # Accumulation f1 for all batch\n",
        "      total_label += labels_flat.tolist()\n",
        "\n",
        "      # Logits score on positive \n",
        "      total_logits += logits.tolist()\n",
        "\n",
        "  return total_pred,total_label,total_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLPLUSq6mWgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_pred,total_label,total_logits =evaluation_loop(gender_model,val_loader_balanced_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_IfZYuEyJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_report(pred,label,logits):\n",
        "    \"\"\"\n",
        "        Input :\n",
        "            model : Algorithme de sklearn avec les paramètres choisit ou par défaut\n",
        "            X_train,X_test,y_train,y_test : dataset découpé à l'aide de train_test_split\n",
        "        Output : \n",
        "            Classification_report + Confusion_matrix + ROC_curve + (si possible feature importance)\n",
        "    \"\"\"\n",
        "    #from sklearn\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "    logits = [el[1] for el in total_logits]\n",
        "    pred = [i for i in total_pred]\n",
        "    label = [i for i in total_label]\n",
        "    print (\"Classification report :\")\n",
        "    print(classification_report(label,pred))\n",
        "    print (\"Accuracy : \",accuracy_score(label,pred))\n",
        "    cm = confusion_matrix(label,pred)\n",
        "    ROC = roc_auc_score(label,pred) \n",
        "    print (\"AUC : \",ROC)\n",
        "    fpr,tpr,thresholds = roc_curve(label,logits)\n",
        "    plt.figure(figsize=(12,10))\n",
        "    plt.subplot(221)\n",
        "    sns.heatmap(cm/np.sum(cm), annot=True, \n",
        "            fmt='.2%', cmap='Blues').set_title('Matrice de confusion')\n",
        "    plt.subplot(222)\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % ROC)\n",
        "    plt.plot([0,1],[0,1],color='red')\n",
        "    plt.title('Courbe ROC')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOvGlNbxOSPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "978d09f0-aa89-42b3-970b-d5e8e1084d61"
      },
      "source": [
        "model_report(total_pred,total_label,logits)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80       469\n",
            "           1       0.81      0.84      0.82       513\n",
            "\n",
            "    accuracy                           0.81       982\n",
            "   macro avg       0.81      0.81      0.81       982\n",
            "weighted avg       0.81      0.81      0.81       982\n",
            "\n",
            "Accuracy :  0.8126272912423625\n",
            "AUC :  0.8113359684451593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAE0CAYAAADAGJ4EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd1xV9RvA8c+9bIELKCDujbjAXWaZ\nM0fuvcVtjhxlavWzMlMzc6TmSlPRUlEMR860zJ0T91ZEVJC91z2/P5CbdAEBxXuR592L16t7zvd7\nznMueO5zv+c536NSFEVBCCGEEEIIgdrQAQghhBBCCGEsJDkWQgghhBDiKUmOhRBCCCGEeEqSYyGE\nEEIIIZ6S5FgIIYQQQoinJDkWQgghhBDiKUmOjVxAQACVK1dm4cKFhg5FT9OmTenXr5+hw8gT69ev\np1WrVlSvXp3KlSsTEBCQJ/vp168fTZs2zZNtCyGEsalcuTKTJ082dBhCZMnU0AEY0okTJ+jfvz8A\nffr0YerUqXptQkJCePfdd0lKSqJ+/fp4eXnlal8+Pj5ERkbi6en5IiGLV+D48eNMmzaNZs2aMXTo\nUExNTSlcuLChwxJCiCzFxcWxceNG9u7dy82bN4mJicHOzo5q1arRunVr2rdvj6lpwfrYX7hwIYsW\nLdK9VqlUaDQaqlSpQv/+/WnWrFmG/bRaLb6+vmzdupVr164RExODo6Mj9erVY9CgQVSpUiXTfR47\ndowNGzZw7tw5QkJCMDMzo1y5crzzzjv06tULFxeXl36c4uUqWP9KMmFhYcGOHTuYPHky5ubm6db5\n+vqiKMoLn1C2bt3KgwcPcpwclyhRAj8/P0xMTF5o/yL7jh49CsCMGTOwt7fP032tXLkyT7cvhCgY\n7t27x7Bhw7h79y5vvfUWw4YNw8HBgZCQEI4dO8aUKVO4efMmn3zyiaFDNYgPP/yQkiVLkpKSgr+/\nPxs3bmTkyJHMmTOHdu3apWsbGxvL6NGjOXLkCB4eHgwdOhQ7Ozvu3r2Lj48PO3fu5PPPP6d3797p\n+mm1WqZOnYq3tzclSpSgbdu2lC1blsTERC5dusT69evZtGkTx44de5WHLnJBkmOgRYsW7Nixg/37\n99OmTZt063x8fGjUqBHHjx9/pTFFR0djY2ODSqXCwsLile67oAsODgbI88QY0PsyJoQQORUfH8/w\n4cMJCAhg4cKFvPfee+nWDxs2DD8/Py5cuGCQ+JKSktBqtQb9LGvUqBE1atTQvW7VqhUdOnRg+fLl\nesnxF198wZEjRxgxYgTjx49Pt27w4MF4enoybdo0ypYty1tvvaVbt3DhQry9vWnbti0zZ87UO79P\nnjw53Si2MF5ScwxUrVqVypUr4+Pjk265n58fN27coEuXLhn2O3z4MOPGjaNZs2a4u7tTt25dBg0a\nxMmTJ9O1a9q0KSdPnuTBgwdUrlxZ93PixAng37rT+/fv8+GHH1K/fn3q1KkDZF1zvGfPHvr160fd\nunXx8PCgZcuWTJ8+ncTERF0bRVH45Zdf6Ny5Mx4eHtSqVYt+/frlKNl/+PAhY8eOpU6dOtSuXZsR\nI0bg7++fafujR48yaNAg6tatS40aNWjXrh2//vprtveX3WOLjY3l+++/p3nz5lSvXp2GDRvyySef\n8ODBg3TbOnHihO73u2XLFt5//32qV69OkyZNWLFiha5d2nud9neQ9ntKq6vOrD44o9+RVqtl9erV\ntGvXjlq1alG7dm1atmzJp59+SlJSkq5dZtv8559/GDhwIHXq1MHd3Z1OnTrh7e2t1y6t/+PHj5kw\nYQL16tXDw8ODwYMHc+fOney+3UKIfMzb25s7d+4wcOBAvcQ4jbu7O3369Em3bP/+/fTs2ZOaNWtS\nq1Ytevbsyf79+/X6ZlYn7OPjk+6zDFITxMqVK3Pjxg1mzpxJo0aNcHd359y5c+n6Hj16lO7du+Ph\n4UHDhg2ZPn06MTExevuIioriu+++o0WLFlSvXp0333yTCRMmcP/+/Wy9N5lxc3PDwcGBu3fvplt+\n9epVtm3bhoeHB+PGjdPrV7hwYb7//nsA5syZo1seEhLCypUrKVGiBDNmzMhw4EOj0fDpp5++UNzi\n1ZCR46e6dOnCrFmzePz4MUWLFgVg8+bNFClShMaNG2fYZ+vWrURERNCxY0dcXFx4/Pgx3t7eeHp6\nsnbtWurWrQvAp59+yvfff09YWBhTpkzR9a9QoYLu/2NiYujbty+1a9dm3LhxhIaGZhnvvHnzWLp0\nKRUrVsTT0xMnJyf8/f3Zu3cvH374oe4f5sSJE9m5cyctW7akc+fOJCYmsn37dgYNGsTChQszrbdK\nExkZSZ8+fXj06BE9e/akQoUK/PPPP/Tv35/4+Hi99hs3buSLL76gZs2ajBgxAisrK44ePcqXX36J\nv78/kyZNynJ/2T22pKQkBg8ezJkzZ2jZsiUDBw7k3r17/Prrrxw5coQtW7bo1XVt2LCBJ0+e0LVr\nVzQaDdu2bWPOnDm4uLjQrl07ChcuzOzZs9m0aROnTp1i9uzZADg6Oj435v9asmQJP/zwA02aNKFn\nz56YmJgQEBDAgQMHSExMxMzMLNO+Bw4cYPTo0Tg6OjJw4EBsbGx0l/ECAgL0RjJiY2Pp27cvHh4e\njB8/noCAANauXcvIkSPZsWOHlOQI8Zrbs2cPAD169Mh2n/Xr1zNt2jTKly/PyJEjgdTPtFGjRjFt\n2rQcbSsjH3/8MZaWlgwaNAgAJycn3bpLly6xZ88eunXrRocOHThx4gReXl7cuHGDn3/+GbU6ddwu\nKiqKnj17EhgYSJcuXahUqRLBwcH88ssvdOvWjS1btlCiRIlcxRcREUFERARFihRJt3zv3r0AdOvW\nDZVKlWHfSpUqUbNmTc6ePcuDBw8oUaIEf/75JwkJCXTo0EGu9r4OlALs+PHjiqurq/LTTz8poaGh\nSrVq1ZQlS5YoiqIocXFxSp06dZRZs2YpiqIoNWvWVPr27Zuuf0xMjN42g4ODlfr16ytDhgxJt7xv\n375KkyZNMoyjb9++iqurqzJ37ly9dffv31dcXV2VH374Qbfs/Pnziqurq9KvXz8lPj4+XXutVqto\ntVpFURRl7969iqurq7Jhw4Z0bZKSkpROnTopTZo00bXNzPfff6+4uroqmzdvTrd8+vTpiqura7r3\n5PHjx0r16tWVCRMm6G3n66+/Vtzc3BR/f/8s95fdY9u4caPi6uqqfPvtt+naHDx4UHF1dVU+/vhj\n3bK033PDhg2VyMhI3fLY2FjljTfeULp3755uG5MmTVJcXV31Ysvsd5jR76hjx45K69atszzWjLaZ\nnJysNG7cWKlTp47y6NEj3fKEhASlR48eipubm3Lnzp10/V1dXZXly5en2+6KFSsUV1dX5dChQ8+N\nQQiRv9WvX1+pXbt2ttuHh4crNWvWVJo3b65ERUXplkdFRSnNmjVTatasqUREROiWu7q6KpMmTdLb\nzpYtWxRXV1fl+PHjumU//PCD7rMhKSlJr4+rq6vi6uqq7Nu3L93yr7/+WnF1dVV27NiRblmNGjWU\nK1eupGsbEBCg1KpVK8OY/istnqNHjyohISFKUFCQcurUKd2587+fIaNHj1ZcXV2VixcvZrndtHgP\nHDigKIqizJw5U3F1dVX27Nnz3JiE8ZOyiqccHBxo2rQpW7duBVK/PUZFRWVaUgFQqFAh3f/HxMQQ\nFhaGWq3Gw8MDPz+/HMcwePDgbLXbtm0bAB999JHeN1SVSqX7trtt2zasra1p3rw5oaGhup/IyEia\nNm3KgwcP9C4p/df+/ftxdHSkY8eO6ZYPHTpUr+2ePXtITEyka9eu6fYXGhpK06ZN0Wq1upvdXvTY\n9u3bh1qtZvjw4enaNG7cmCpVqvDHH3+g1WrTrevSpQu2tra611ZWVtSsWfO570Fu2NjY8PjxY06d\nOpWjfpcuXdKNkqRdwYDU2uQhQ4ag1Wr5448/0vVRq9W6WVfSvPnmm0DqTTpCiNdbdHQ01tbW2W5/\n5MgRYmNj6devHzY2NrrlNjY29OvXj9jY2Oeeq59nwIABmd7IXq5cOZo3b55u2bBhw4DUczuklgRu\n376devXq4ezsnO7zJO3cffjw4WzH4+npSYMGDXj77bfp3bs3586dY+jQoUyYMCFdu+joaIB0nxUZ\nSXu/o6Ki0vV79v0U+ZeUVTyjS5cuDBs2jFOnTrFlyxbc3d2pWLFipu39/f2ZN28ehw8fJjIyMt26\nzC7HZKZw4cJoNJpstb137x4qlQo3N7cs2926dYuYmJh0Nwz8V0hICOXKlct0/f3796lRo4bepXln\nZ2e9eG/dugWQ5YwcT548yTLm7B5bQEAAzs7O2NnZ6a2rWLEiV65cISwsLN0ls5IlS+q1tbe3Jzw8\nPMt95caECRMYNWoUffr0wdnZmfr169O4cWNatmyZ5U14afMpZ/R3V6lSJQC9WjtnZ2e9LxJpNxPm\nxbEJIYyLjY1NhvW6mUk7z6SdU56V2Xkmp8qWLZvpumdLCtOkfaak7Tc0NJTw8HAOHz5MgwYNMtxO\nWvlFdkydOpVy5coRFxenK+OIjIzUS+DTktu0pDczae93WhKd1i8nvwdhvCQ5fsbbb79N0aJFWbx4\nMSdOnODLL7/MtG1MTAx9+vQhLi6OAQMG4OrqirW1NWq1mmXLluV4dgsrK6sctX92FDUziqKku3kg\nIxmdHHNLURQAvv32W5ydnTNsU6pUqeduJzvHlht5VXubkpKit6xWrVrs27ePw4cPc+LECU6cOMGO\nHTtYsmQJv/zyy0udCSOr40r7nQghXl+VKlXin3/+4f79+9k6x74sGZ370lhaWr7QttPOXW+99VaG\nVypzyt3dXTdbRbNmzXB0dOT777+nSpUq9OrVS9euUqVK7N27l8uXL1OtWrVMt3fp0iUAXF1ddf0A\nLl++TIsWLV44XmFYkhw/w8TEhI4dO7Js2TIsLS1p27Ztpm2PHTtGUFAQM2bM0Cu9mD9/fp7GWbZs\nWQ4dOsTVq1dxd3fPtF2ZMmW4e/cuHh4eObrk9qxSpUpx7949UlJS0iVhQUFBeqPlaSMFDg4OWY5W\nZyW7x1aqVCn+/vtvIiMjMxzBtrGxwcHBIVcxZMbe3l53QnxWZiMs1tbWtGzZkpYtWwL/3gCzefNm\nhgwZkmGftNHtmzdv6q1LW/YqP/yEEMbvvffe459//sHb21uvTCAjaeeQGzdu6I3KZnSeyewKW25H\nl9OuMj4r7TMlbb9pV1Ojo6Nz/XmSlYEDB7J582bmz59Pu3btdCO/7733HosXL2bz5s107do1w4Ga\nmzdvcvbsWapVq6a7IbBx48ZYWFjg6+vLBx98INN05nNSc/wfPXv2ZPTo0Xz11VdZ1g6lJYr/HZk7\nfPgw58+f12tvbW1NRETESxnJS5uTce7cuemmNkuTto+OHTui1WqZO3duhtt5XokDpH7DfvLkCb/9\n9lu65c9OgZamdevWmJubs3DhwgxnsoiKisow3mdl99iaN2+OVqtl+fLl6db/9ddfXL58maZNm+bo\nklt2lC1blpiYmHT15GlTtv1XRrONpI1CREREZLqPatWqUbx4cXx8fHTzLUPqPKErV65EpVI9d4YR\nIUTB0q1bN8qVK8eqVasynIoN4OLFi6xfvx6Ahg0bUqhQIdatW6erlYXUutl169ZRqFAhGjZsqFte\ntmxZzp07R1xcnG5ZRESE3vSn2XXnzh29ONM+U9JqkdVqNe3atcPPz4/du3dnuJ2QkJBc7R/AzMyM\n4cOHEx4eztq1a3XL3dzcaNu2LefOnctwCtXw8HAmTpwIpN4bk6ZIkSIMHjyYBw8e8Nlnn2X4+RUd\nHc2MGTNyHbN4dWTk+D+KFy/OmDFjntuuTp06ODk58e233/LgwQNcXFy4cuUKvr6+uLq6cv369XTt\nPTw8OHjwINOmTaNWrVqYmJjw5ptv6k0jkx3u7u4MHTqUFStW0LlzZ1q3bo2TkxMBAQHs2bMHb29v\nNBoNrVq1onPnzqxbt45Lly7RpEkTHBwcePToEefOnePevXt6N3f915AhQ9ixYwf/+9//uHTpEhUr\nVuTkyZOcO3dOb2TWxcWFL7/8ks8//5w2bdrQvn17SpQoQWhoKNevX2f//v3s3Lkzw9rfnB5bp06d\n2Lp1KytWrODBgwfUrVsXf39/fvnlFxwdHbM1epJT3bt35+eff2bUqFH0798fMzMz9uzZk+GlxTZt\n2lCzZk3c3d1xdnYmODiYTZs2YWZmxvvvv5/pPkxMTPjf//7H6NGj6dq1K927d8fa2ppdu3Zx7tw5\nRowYkWUtnxCi4LGysmLZsmUMGzaMUaNG8fbbb/PWW29hb29PaGgoJ06c4PDhw7orVhqNho8//php\n06bRvXt3OnXqBKRO5Xbv3j2mTZuW7oa0Pn36MHHiRAYMGECHDh2IjIzE29ub4sWLp/sSn12urq5M\nnDiRbt26UaZMGU6cOMGePXuoX79+ugdxjR8/njNnzjBu3Dhat26Nh4cHZmZmBAYGcujQIapVq8as\nWbNy/b516NCBxYsXs3r1avr3768bEPvqq6948uQJixcv5ujRo7Ro0SLdE/LCwsKYOnVqui8QAGPG\njCE4OBhvb29Onz7N+++/T+nSpUlKSuLq1avs3r0bMzMzmes4H5DkOJc0Gg0//fQT3333HevWrSM5\nOZnq1auzYsUKNm/erJcce3p6cv/+ffbs2cOGDRvQarWsXbs2V8kxpM4h6ebmxrp16/jpp59QFAUX\nFxcaNWqUrtZr5syZvPHGG2zatIlly5aRlJSEk5MTVatWTfetNzN2dnasX7+eWbNm6UaP69evz9q1\nazO88a5Lly6ULVuWVatWsXHjRqKiorC3t6dcuXKMHTs23VyXL3JsZmZmrFy5kiVLlvD777+zb98+\nbG1tadWqFePGjaNYsWLZfCezr1SpUixevJi5c+eyYMEC7O3t6dChA126dKF169bp2g4aNIi//voL\nLy8voqKiKFKkCB4eHgwfPvy5Nxs2bdqU1atXs2TJElauXElSUhIVKlRg+vTpdOvW7aUflxAi/ytT\npgy//fYbGzduZM+ePSxdupTY2Fjs7OyoXr06s2bNSvckuLSbhVeuXMnixYuB1FHTxYsX680k0b59\ne4KCgli/fj0zZ86kVKlSjBw5ErVaneGV0uepVq0aU6ZMYd68eWzYsAEbGxv69u3L+PHj013xs7W1\n5ddff2XVqlXs3r2bP/74AxMTE1xcXKhTp84Lnw9NTU0ZNmwYX3zxBatXr2b06NFA6s11q1at4rff\nfuO3335j2bJlxMbGUqRIERo2bMigQYOoUqWK3vbUajXTp0+nTZs2bNiwAV9fX0JDQzEzM6NcuXL0\n7t1b75HTwjipFLljRwghhBBCCEBqjoUQQgghhNCR5FgIIYQQQoinJDkWQgghhBDiKUmOhRBCCCGE\neMqoZqtw9Nxg6BBEHru2uKuhQxCvQBHr3J9arGqNznGfuLOLcr0/IYQQ4llGlRwLIYTIX8LCYtBq\nczbpUZEiNoSERD+/YT4kx5Z/vc7HJ8eWnlqtwsEh8ycHS3IshDAuKqn2yk+0WiXHyXFav9eVHFv+\n9Tofnxxb9klyLIQwLiqVoSMQQghRgElyLIQwLjJyLIQQwoAkORZCGBcZORZCCGFAMkQjhDAuapOc\n/xQwQUFBzJkzh379+lGrVi0qV67MiRMnst3/1q1bDB48mFq1alG/fn0mTZpEaGhoHkYshBD5hyTH\nQgjjolLn/KeAuXPnDitWrODx48dUrlw5R30fPXpEnz59uH//PuPHj2fQoEEcPHiQwYMHk5SUlEcR\nCyFE/iFlFUII4yJlFc9VrVo1jh8/joODA/v372fUqFHZ7rt06VISEhLw8vKiaNGiALi7uzNw4EB8\nfX3p2lXmIhdCFGwFb8hFCGHcZOT4uWxsbHBwcMhV371799K0aVNdYgzw1ltvUbZsWXbt2vWyQhRC\niHxLRo6FEMZFRo7zzOPHjwkJCaF69ep669zd3Tly5IgBohJCGIubDyKIikk0dBjZpkpOpkLMOWzr\n1Xyp25XkWAhhXArgSPCrEhQUBICTk5PeOicnJ0JCQkhJScHEJPs3ORYpYpOrWJycbHPVLz+QY8u/\n8vL4UlK0HDh1n7iE5DzbR0biEpPZtP8GdjbmqLIYfIiJTSQm/tXG9iJs4yL5ZOccyvv7EX7LH/vy\npV7atiU5FkIYFxk5zjMJCQkAmJub662zsLAAID4+HmvrzB+r+l8hIdE5fjqVk5MtwcFROeqTX8ix\n5T9HLjzE/3E0VoXMiIvN+U2p+07dz4OoXj4TlYryxTVZtklO0fJmNRccbCxeUVS5Y3njKhXHjsX8\n8UMif1hCkq19jv421WpVll/sJTkWQhgXGTnOM2kJcGKi/mXTtMTZ0tLylcYkRHb5P47K9ajrkQuP\niInXT3yv3w/XjZYWsjRFUXL+GGIzUzV21ua8Vd3luW3VahWNPIpjZvpqz3NFnTVER8a90n3mFfOd\n29GMGobW1pYI399xaN3spX9pk+RYCGFcJDnOM87OzgAEBwfrrQsODqZIkSI5KqkQ4mW5ci+M0Mj4\nTNdv/fs2oZEJL7yfkk7pRwsdbC0wM1Xzcc9aeFRxeS1HxgGsLEyJNnQQL0qrpdD332L93UySatch\ncvUvaF2K5cmuJDkWQhgXtZRV5JWiRYtSuHBhLl68qLfOz8+PKlWqGCAqUdAkJKUQ+CSGLX/d4vr9\nCCD1cn52jOhQDdtC+mVB2VHWxRYrC0l78iNVdBS2o4ZjsWsH8T16E/XdfMjDq1zyVyKEMC4ycvzS\n+Pv7A1C6dGndsvfee49t27bx+PFj3XRux44d4+7duwwZMsQgcYrXU1B4HAmJKbrX/o+j2H70LkFh\n6S/vN/IojolaRf0qzjhoMk94imgsMFHL+aGgUd+5jd2AXpjcuE701zOJGzYyz+9NkeRYCGFc5Ia8\nbPnxxx+B1EdBA/j6+nL69Gk0Gg19+/YFwNPTE4ADBw7o+o0YMYLdu3fTv39/+vbtS2xsLCtXrsTN\nzY0OHTq82oMQr42AoGj8g6LY908AVhYmBD6JITKTm9tMTVS0e6ss5UvYUbWMQ5YzKIiCzeyvg2iG\nDgAgYoMPSe82eSX7leRYCGFcZOQ4WxYsWJDu9ZYtWwAoUaKELjnOSLFixVi3bh2zZs3i+++/x8zM\njMaNGzNlypQMZ7EQ4llJyVqOXXpEQlIKO4/dw8xEjUoFTyL+rRe2sjClpJM1GusUmtctibXlv6mG\ns0MhSjnnbvo/UYAoClbLFmP95eekuFYmYs2vaMuVf2W7l+RYCGFcZBQpW65du/bcNs+OGD+rUqVK\nrFy58mWHJF4zMfFJJCSmkJis5c+zD7jiH8b9x+lv60othyhKhRIK1csVpnxxDcWKZH8qQCH0xMdj\n+/FYLDf9SkKbdkQtWopi82rn15bkWAhhXGTkWIhXJi4hmftBqQlvSoqW3SfvY26qJjQqnjsPM565\noePb5WhcqwQmJiqsLc1eZbjiNad+GIhmYB/Mzpwm5pNPiZ3wCRigzlySYyGEcZGRYyHyjFZROH/j\nCXGJqXP7eu25TkJSil67Eo7WFNZY8FZ1FxztrLA0N6HV2xUIDcn3E4IJI2X6zwk0A/uiiokhYvUv\nJLZpa7hYDLZnIYTISB6PHCcmJrJgwQJ8fX2JjIzEzc2N8ePH06BBgxxtZ+jQoRw6dIj+/fvz2Wef\n5VG0Qrw4RVGIT0whJi6JjQducvp6+nmurSxMGNWpBpD6QIsKxe1QZzCloolMsyjyiMWv67CdOA5t\nseKEe/uSUqWqQeOR5FgIYVzyeOR48uTJ7N27l/79+1OmTBm2bt3K0KFD8fLyolatWtnaxp9//smp\nU6fyNE4hcis2PonQqH8fmPHz71f0SiQ+7OJOccdCANjbWGBuJg9/EQaQlIT1l59RaMVSEhs1IXLF\nzygOhQ0dlSTHQggjk4cjx35+fuzcuZMpU6bopjnr2LEjbdu2Zc6cOaxfv/6520hMTGTmzJkMHjyY\nhQsX5lmsQuTWzPVneBAco7e8e5OK2NmY80bVoqilfEkYmCokBM3QAZgfPkTs8FHEfPE1mBpHWmoc\nUQghRJo8/NDevXs3ZmZmdOvWTbfMwsKCrl27Mm/ePIKCgnSPWM7M2rVriY+Pl+RYGKUz14MJjYyn\nShkHmtQqoVtevriGwlk8YEOIV8nk0kXsBvRC/fgRkQuXktCjt6FDSkeSYyGEccnFyHFkZCSRkZF6\nyzUaDRqNRvf6ypUrlCtXDmvr9FNNubu7oygKV65cyTI5Dg4O5scff2Tq1KlYWVnlOE4h8kJ8YjLH\nLz3m7qNIDp1/CIBrKXvqumX9RU8IQzDf7otmzHC0GjvCfXeRXLuuoUPSI8mxEMK45CI5XrNmDYsW\nLdJbPnr0aMaMGaN7HRwcrHtk8rOcnJwACAoKynI/c+fOpVy5cvIkOWEUjlx4yJnrwZy98US3zMxU\nzeD3q1BPEmNhbLRaCs2egfXc2STVqUfk6vVoi7oYOqoMSXIshDAuuSirGDBgAJ06ddJb/uyoMUB8\nfDxmZvrzslpYWACQkJCgty6Nn58fv/32G15eXvK4W2FwP/9+hb/9UkeJizpYUaGEHR3fKYejnVzR\nEMZHFRWJ7ahhWOz+nbje/Yj+di48Pe8aI0mOhRDGJRcjx/8tn8iMpaUlSUlJesvTkmKLTE7WiqLw\nzTff8N5771G3rvFdAhQFx8YDN/j7/ENiE1LnKR7RoRr1q+hfDRHCWKhv38JuQC9Mbt4gauZ3xA8a\nZvTz2UtyLIQwLnl40nRycsqwdCI4OHXe18zqjfft24efnx/jx48nICAg3bro6GgCAgJwdHTE0lJu\neBIvX1KylqCwWG4FRrLn5H0AmtUuSZPaJSjuKI9qFsbL7MB+NMMHgYmaiE2/kfTOu4YOKVskORZC\nGJc8nMrNzc0NLy8vYmJi0t2Ud/78ed36jAQGBqLVahkwYIDeOh8fH3x8fFixYgWNGjXKm8BFgaMo\nCmt2X+VJRDyX74alW/dBx+pSUyyMm6JgtWQR1tP+R4pbVSLW/IK2TFlDR5VtkhwLIYxLHo4ct2rV\nilWrVuHt7a2b5zgxMREfH+LBTTcAACAASURBVB9q166tu1kvMDCQuLg4KlSoAEDTpk0pWbKk3vZG\njRpFkyZN6Nq1K9WqVcuzuMXr735QNDcCwnWvE5O0HDr/kCIaSyqU0GBlYco77sWxtjSlalnDPyRB\niEzFxWH70YdYbt5IQtsORP6wBGxsDB1VjkhyLIQwKnl5s5uHhwetWrVizpw5BAcHU7p0abZu3Upg\nYCAzZ87UtZs0aRInT57k2rVrAJQuXZrSpUtnuM1SpUrRvHnzPItZvP5uPohghtfpDNd1a1JBaopF\nvqEOfIDGszdm584SM/lzYsdPNPr64oxIciyEMCoqdd6eSGfPns38+fPx9fUlIiKCypUrs3z5curU\nqZOn+xUiM0FhsQD0aFqRBtX+ndpKrVZhY6U/u4oQxsj0xHHsBvWFuDgi1m4gsVUbQ4eUa5IcCyGM\nSl5Pk2ZhYcGkSZOYNGlSpm28vLyyta20kWUhXoZalRzRWJsbOgwhcsxy3RpsJk0gpWQpIn12kFI5\n4/s38gtJjoUQRkXmEBYFzamrwYYOQYjcSUrC5n+TsVq1gsTGTYlc/jOKvYOho3phkhwLIYyKJMei\nIPnt79ucu5n6hDspoRD5ierJEzRD+mN+9DCxo8YS8/mXYGJi6LBeCkmOhRBGRZJjUVBExSay7chd\nAGaPaEAhS0mORf5gcsEPO8/eqIMeE7l4OQndeho6pJcq7yYUFUKI3FDl4keIfCguMQWA5nVL4mgv\nj30W+YOFrw8O7d6DlBTCt+957RJjkORYCGFkVCpVjn+EyM/KFLU1dAhCPJ9WS6EZ09AM9SS5Wg3C\n9vxJcs3aho4qT0hZhRDCqEiyK15nCYkpHL/8iAu3Q0lO0Ro6HCGyRRUZge3IoVjs3U1c3wFEz5wD\nFhaGDivPSHIshDAqkhyL19WDJzHMWneamPhkAFwKF6KUs42MHAujZnLrBpr+vTC5c5uoWd8TP3BI\nvnywR05IciyEMCqSHIvX1Y9bLxATn4ypiYovBtanhKO1oUMSIkvmf+zFdvhgMDMlYvM2kt5629Ah\nvRKSHAshjIvkxuI1kZCUwjX/cC7eCeHSnVAehsRSq5Ijw9pVw8L89ZjySrymFAWrRQuwnv4FKVWr\nE7H2V7SlShs6qldGkmMhhFGRkWORXyUkpbD3xD12H72DCrj9MIrkFC1mpmpcS9nzRpWi1HVzlsRY\nGLfYWGwnjMbSZzPxHToTNX8xWBesqxySHAshjIokxyK/2vvPfbYeug2As4MVTWuXoHq5wriWssfc\nTBJiYfzUAffRePbB9MJ5oj/7grgPJ7z29cUZkeRYCGFUJDkW+dGVu6G6xHjGsDdxKVzIwBEJkTNm\nx4+iGdQP4uOJ9NpA4nutDR2SwUhyLIQwLpIbi3xAURQCQ2KJT0ydecLnaWLcqkFZSYxFvmO5ZhU2\nUz4mpUxZIn13kVLJ1dAhGZQkx0IIoyIjx8LYJSSlsOv4Pd2jn9O4lrJnVFcPgoOjDBOYEDmVmIjN\nZ5OwWrOShGYtiFq6EsXO3tBRGZwkx0IIoyLJsTB2xy890iXGXd4tTynn1HmKSzoVrJuWRP6mCg5G\nM7gf5sePEjtmPDGfTgUTqY0HSY6FEEZGkuPnS0xMZMGCBfj6+hIZGYmbmxvjx4+nQYMGz+179OhR\nlixZwvXr19FqtZQvX54BAwbQpk2bVxB5/hcencCa3dcA+HrIGzJXsciXTP3OoRnQG3XIEyKXriSh\nczdDh2RU1IYOQAghnqVSqXL8U9BMnjyZNWvW0L59ez777DPUajVDhw7l7NmzWfY7ePAggwYNIjk5\nmTFjxjB27FjUajXjx4/H29v7FUWfP6Votcz3Ps+ERUcAKONiK4mxyJcstm7Gvl1LAMJ37JXEOAMy\nciyEMC4FL9fNET8/P3bu3MmUKVPw9PQEoGPHjrRt25Y5c+awfv36TPuuX78eJycn1qxZg7m5OQDd\nu3enWbNm+Pr60q2bfEhm5uz1J/jdCgGgW5MKNK5ZwsARCZFDKSlYz5hGoYXzSHqjARGr1qE4ORk6\nKqMkI8dCCKMiI8dZ2717N2ZmZukSWQsLC7p27crp06cJCgrKtG90dDR2dna6xBjA3NwcOzs7LCws\n8jTu/EpRFPae9OfwhYcAfDmwHq3fKIOVhYwtiXwkPBxN3+4UWjiPuAGDCd+yXRLjLBTYf90VXWz5\nuEM13Ms44GJvhamJmgehsew/H8iiXVd5HBGva/tkdc8st/XNFj/mbb+cZZsWHsUY0LgiVUvZ42hr\nQWKyFv/gaDYevcvqgzdJSNLq2jZ0c8Z3ctMst9dm+n5O3nwCQFknG77tX4f6FR0JiUpg+b7rLN93\nXa/PjD61eauyE82+3EuKVsly+6+TtatWcO3qZa5duUzggwBcihXHZ+e+TNtfuuDHssULuHzRD1Qq\narjX5IMPx+NauUq29vfjD3M5d+Y0Aff9iYmOwqFwESpWqkzv/p7Urltfr33AfX9+WrqIf04cJzoq\nEueiLrRs3ZZ+g4bqJSx7d+1g1YqlBD9+hGvlKoz75FMqu6WPKyY6mj7dOtC1R2/6eg7OVszGpKAl\nuzl15coVypUrh/V/nljl7u6OoihcuXIFZ2fnDPvWr1+fZcuWMX/+fDp37gyAj48Pd+/eZcqUKXke\ne34TGZPIrQcRbDhwE5UKHGwtcLK3MnRYQuSIyY3rMLA35rdvEzV7HvH58HPhVSuwyXExh0IUtbNi\n5+kHBIbFkpKiUKWUHf0aV6DTG2VoPHU3T6ISAPhg2bEMtzGxY3XKF7Vlz9kHz91flZL2pGgV1h+6\nzePwOCzNTXjT1YlvetemhXtxus75U9f2emBEhvs0NzNhrmddQqISOXMn9fKeSgVrPnwbKzMTpnmf\nx62EHTP61CYwLJYdpwJ0fWuXL4xnkwq8/80fBSoxBli6aD4aOzsqu1UlKioyy7YX/c4zepgnjs5F\nGTJiNACbN/3KyMH9WfbzeipkY+7HSxf8qFjJlcbNWqCx1RAS8oQ9v29n9LCB/G/aTFq3ba9re/fO\nbYZ79iElJZnO3XtRvHhJLl44x88/LeXSRT/mLlqmSxYvXfDjq88n07RFK3r07sf237Yw8cMP+HXr\nznSJ0pKF87B3cKBn3wG5ebsMTpLjrAUHB1O0aFG95U5PR4GyGjkeMWIE/v7+LF26lCVLlgBQqFAh\nfvzxRxo2bJireIoUsclVPycn21z1y0tX74XyJDxO9/rbtad0/z9lQD0a1Ciere0Y47G9LK/zscFr\neHw7d0Lv3mBhgerAAWzfeYfX7AiBl/97K7DJ8d9XHvP3lcd6y49dC2bVqIb0erscC3ddBcD72D29\ndsUcrFjkZM3Z2yFcDoh47v5+2HlFb9lP+28QEpXA4GaVqFWuMGfvhAIQHJmQ4T47v1EaE7WaTUfv\nkJySmuBWKGpLtVL2dJh1gCNXUz8Uq5Swo22dkrrk2NRExbyB9Vn1x03dPgoS7227KVGyFAB9unUg\nLjY207bzvpuBqZkZS35ag5NzagLS7L1W9OrSjh/mfceCH1c8d3+LV6zWW9atVx+6tW+N188r0iXH\nSxbOIzo6iqWrvKjhUQuAjl27U7pMOZYums+e33fQ6v12ABz68w+KFS/BtJnfoVKpeKNBQ7q2a8ml\nC+ep/+ZbAFw4f5ZtWzezfPUvmJrm03/ekhtnKT4+HjMzM73laVcZEhISMu1rbm5O2bJladWqFS1a\ntCAlJYVNmzYxbtw4Vq9ejbu7e47jCQmJRpvDL9xOTrZGNxdwUrKWyYsO6w0elClqS7cmFSjnbJ2t\nmI3x2F6W1/nY4DU7PkXB6oe5WM+YRnIND8y2+xJs5QCvy/E9Ize/N7ValeUX+xx9ej558oQrV64Q\nFBREfHw8lpaWODs74+bmphu1yO/uP4kBwM7aPMt2vd8pj4lazbqnT0XKrYCn+7N/zv4A+r5bAYB1\nf/27T0vz1DkJw6L//UAMi0mk0DP1cGPaVMHWyowZW/xeKNb8Ki0xfp4A/3tcuXSRth066xJjACfn\nojRt3pKd27YS8iSYIo45/1svVMgaOzs7vZHrM6dOUqpMWV1inKZNu44sXTSfndu26pLjhPh4bGxt\ndSOrGo0dAHFxqSNdSUmJzPr6C7r16otb1Wo5jtFYyMhx1iwtLUlKStJbnpYUZ1U7/PXXX3PhwgU2\nb96MWp16y0nr1q1p27YtM2bMYMOGDXkTtBFTFIWDZx/g/ziaFK3Ce/VK8Y57sdSVKhUuha0wUcvt\nOSIfiYnBdvwoLH/zIb5zV6LmLsKpdNHXMjHOK9lKjs+fP8+cOXM4ffo0iqKgKOm/WatUKurUqcPH\nH39MzZo18yTQvGJhpsbawhQLMxMqF7djancPAPb7PcyyX6+3yxEdn8SW4/ojvFmxsTTF3FSNrZUZ\n9Ss5Meb9KoREJXD6dkiW/Uo7WvO2mzPHrgVz89G/f+A3H0YRGp3ARx2q8dXG81QuoaFpDRdmb70I\npI4sT2hXFc+FR4hNTMlRrAXN5cup71l1dw+9ddVquLPD14erVy7T8J13s7W98LAwtIqWkCfBbPPZ\nzN07t2nboXO6NkmJiVhaWur1TVt25dIFFEVBpVJR3b0m3hvWs2vHNmrWrsuvXj9jZmaGW5WqAHj9\n/BMJCQkMfVoOkl9Jcpw1JyenDEsngoODATKtN05MTGTz5s0MHz5clxgDmJmZ8c477/Drr7+SnJyc\nf6845NKZ68Gs25t6j4adtTnVyxemhFPuSkWEMDT1fX80A3pjeukC0f+bRtzosan1lyJHnnsWPHbs\nGEOHDqV48eKMGzeOGjVq4OzsjLm5OYmJiQQFBXH+/Hm2bt1Kv379WLFiBW+++eariP2l6NuoAt/2\nq6N7fS84mhHLjnH8enCmfd6pUpSyzjb88vdtouOTc7S/Hwa/Qft6/45knrr1hElrTxMZqz8S9Kze\njcqjVqtYd+hWuuXxSSmMXXWSxUPeoMOc0gD8ceEhy/ennuznDqzH76cf8MeFrJN9AU+eJhfPjhqn\nSVsWHKRfipOR2NgY2jR7W/fawtKSDp278eFHn6RrV658Re7cuaU3In361Mmn24klKjICjZ09zd5r\nxfGjf/P11NQbp8zNzRk3cQpFXYpx985t1q5awbfzFmFplb9vGJLkOGtubm54eXkRExOTrtb8/Pnz\nuvUZCQ8PJzk5mZQU/S/JycnJJCcn6w18vO5OXQ3il6fnys/716V8cY2BIxIi98yOHkYzuB8kJRO5\nfhOJzVsaOqR867nJ8fz586lRo0a6eTGfVaFCBRo0aMCgQYPo378/c+fOZdOmTXkSbF74/UwANx5G\nYm1pintpB1rWKkFhm6ynNOr3bnkA1ueipOI734usPngTR1sL3q5SlKql7HB4zv7UKhW9GpYjMjaR\nbf/c11u/68wD3Cdso1IxDeExidwJigagb6PyVClpx+DFR7A0M2Fqdw9a1SpBbEIyPx+4yco/buQ4\n/tdZQnxqeUJG9Zxpf/sJ8fF66zJiYWHJgiU/kZKczKOHD9mzawdxcbHEx8djZVVI165XvwF8+dkk\nJk0Yw6ixH+FSrASXL/oxf85MTE1NSU5OJj4+Ho1datL4+VczGPrBGIKDgihdpgwaO3sUReHb6V/S\npHlL3mjQkFs3rrPg+2+5c/smZcqWZ/zEKdm6kdBYSHKctVatWrFq1Sq8vb118xwnJibi4+ND7dq1\ndTfrBQYGEhcXR4UKqeVYRYoUQaPRsG/fPkaPHq37O4+JieHgwYO4urpm+Lf/OrviH0ZUbBLN6pSk\nbLHX8TYlUSAoCparV2Lz2SeklC1HpNcGUipUMnRU+dpzk+OrV6/y+eefZ5gYP8vc3JzOnTvzzTff\nvLTgXoWHYXE8DEtNinadecD2U/fZ98V7WJmbsCCDm+jsrc1pU6ck1wMjOHHjSY73dyUggiuk3sDn\nc8KfAY0rsPGjRrSbcUA3Ndt/Na3hQokihVh98CZxmZRGRMcnp7vZztnOki971OTzX8/yJCqB7/rX\noXH1YoxafpxiDlYsGFyfJ5Hx+GaQbBdUFpapI64Z1XMmJiY+baNfApERExMT6r3x76N823Xqwuih\nnowZPojV670xfZqEvNe6LREREaz4cSGjhnoCqcl5/0HDOHr4L65cuoi1dfpLvEVdilHUpZjute8W\nb+7duc3MOQuIiYnhww+G0PCddxk19iN8vDfw4QdD2OS7S2/qL2OlUktynBUPDw9atWrFnDlzCA4O\npnTp0mzdupXAwEBmzpypazdp0iROnjzJtWupjzo2MTFh0KBBzJ8/nx49etC+fXu0Wi2bN2/m0aNH\nTJo0yVCHZFBWFqb0aZF/vjwKkU5iIjZTPsbKazUJLVoSteQnlKf3o4jce+5dBhqNBn9//2xtzN/f\nH40mf1+WuhwQwQX/MAY1zfhbV9cGZbA0M8nVqHFGvI/eBcCzacVM2/RplDpSve6vW5m2+a8ZfWpz\n/m4oGw7fQaWCnm+XY8GOyxy7HozPCX92nArQbVekcnx6U2lGpRNpyzIqucgOExMT3mvTlts3b3Du\nzOl067r17MOO/YdYuW4jS1d5sWP/IQYPH8nDwEAcHZ2wtsm8/vFJcDA//jCXMRM+wd7BgSOHDhId\nFcmETz6lcpWqjJ84heioSI78/Weu4jYEeQjI882ePZt+/frh6+vL9OnTSU5OZvny5dSpUyfLfh98\n8AFz5szBxMSExYsXs2DBAmxsbFi0aBFt2rR5RdEbhz9OB3DmWublc0IYO1VQEPad22LltZqYcR8T\nuXaDJMYvyXNHjtu3b8/q1atxdnama9euWGVQzxgXF4e3tzdr1qyhf//+eRLoq2RpZoK9TcYj5X0a\nlScxOYWNR+6+lH2ZmaoxUatxyGS2CkdbC1rWLM4F/zDO3Q3L1jZb1izOex7FafT5bgCK2FhgZW7K\ng9B/pzB7EBqLexmHFz+A10jVqtWB1LmO23fqmm7dpQt+qFQq3c1vuZFWkhEZqT/1n7m5OVWe7h/g\nyuWLhIeF0q5jlyy3Offb6VSpXkM3PVzQ48do7Ox0dceWVlZo7OwIevQo13G/agUw180xCwsLJk2a\nlOVor5eXV4bL27VrR7t27fIqNKMXGhnPyStBHDgTQFKylqZ1Sho6JCFyzPTcGTSefVCHhRK5/GcS\nnvNZIXLmucnx2LFjefjwId988w2zZ8+mfPnyODk56W7ICw4O5vbt2yQlJdGqVSvGjh37KuJ+Yc52\nlgRF6NePvu3mTJWSdhy5qj+iULOsAzVKO7D91H3dA0L+y9RERVlnG+ISUtIlo5ntb9jTy3mnbmU8\nW0WPhmUxN83+SLWNpSmz+9XlO9+L3A1OrT0OjU4kISmFqiXtOHgxNUmqWtKOR89Mdi+gZOkyuFWt\nxoH9exg6cgxOTql3/QcHB3Fg/x7q1Hsj3U1z4WFhhIeH4ejohI1tar1iZGQEVlZWmJml/7ITFxfL\ndl8f1Go1VarVyDKOhIQEFsyZhbm5Ob36eWba7q+Df3D86BHWef+mW+bo5Ex4WBihIU8oXMSR0JAn\nhIeF4eiU8QwGxqggjgSLVyM6LomPfzyqe/2OezE6yxU0kc9YbN6I7YQxaB2dCNuxj5QaOZ+fXGTt\nucmxubk5c+fOxdPTk927d3P16lUeP36sm+fYycmJhg0b0qpVq1xNIG8o3/WvS1F7S/6+HERASAwW\nZiZ4lHWg0xuliY5PZuqGs3p9/i1vyDxRLeZQiOMz3+fI1SA6zDqgW/739NacuBGM390wHobHUdjG\nnMbVXHi3mguX7oezbO+1DLfXp1F54hKTdeUXz/NZV3dCYxL4cfe/29MqCj4n/PmofTVUKhUu9lY0\ndy/OmJUnsrXN/G7Xjm08ehQIpCa0yUlJ/PzTUgBcXIqneyjH+IlTGD1sIB8M7k+3Hr0B8N74C4pW\ny5gJE9Ntd/PGX1i1/Ec++3I677fvBMDZ06eY/c1XNG7WnJKlSlOokDUPHzxg9+/bCXr8iEHDRlKs\n+L9P2bp96ybTv/iUhu80xrloUUJDQti1w5cHAff59IvplC2X8Qd3THQ0c7/9hiEjRlG8xL8jX2+9\n04hC1tZM/mgsrd5vx57ft2NtY8Nb7zR6Ce/kqyG5sXjZkpK1nL0RzFLfSwB4VCjC8A7VsDAzMXBk\nQuRASgrWX39BoR9/ILFBQyJXeqE4Oho6qtdStie0dHd3z1fJ7/P4HL9Hj4Zl6f5WWYpoLFAUhYCQ\nWNb8eYtFv19NN+oLqaUWnd8sQ0BIDAcu5nxatOX7rtO4mguDmlXCwdqc+KQUbj6M5Gvv86zYdz3D\nOYjrVSyCa3E7Nh+7S8RzpnoDqFOhCAMaV6DN9P16T3masi61zvXD96sQm5DMN1v8XlppiLHb4evD\n2dP/pFu24seFANSqUy9dclzDoxaLlq9m+Y8/sPzHH3TzC3/z7VwquWY8RdazKlSsxNuNGnP21D/s\n3bWT+Ph47OzsqFK1OhM/nao3R7K9vT3ORV3YtnUzYaEh2NjY4lGrDlO/nknV6pn/e/tx4VwKFy5C\njz7py5g0GjvmLlzG3NnfsHj+HMqUK8+cBUt0DwzJD2TkWLxsf559wK9PZ+cpVqQQH3SsjrkkxiIf\nUYWHoRk+CPODfxA3aCjRX8+CAja7zKukUoxoYktHz4L3dKaC5trirs9vJPK9Ita5f5CE2+Q9Oe5z\ndZbM52koxv74aEVR+GjxEcKjE/nCsx5lXPJ2yrbX6hHE//E6HxsY7/GZXLuKpn9PTALuEz3re+Kz\nKLfLjLEe28tg8MdHCyFEXlPLVG7iJVq37zrh0alTMZYqKk++E/mL+Z5d2H4wBKysCPfZSfIb+ech\na/mZPDBeCGFUVKqc/wiREa2icPrpdG2Lxr2DWv5YRH6hKBSa9x2a/j1JqVCRsL1/SmL8CsnIsRDC\nqEjNsXgZrt8PZ9b6MwAU1lhQyFLqM0U+ER2NZuxILLb/RnyX7kTNXQgZTKMr8o4kx0IIoyK5sXgZ\n7j1OrUGsW9mJXs3lCXgif1Dfu4vdgN6YXL1M9BfTiRs5Rk6KBiDJsRDCqMjIsXhR8YnJBIWmzuPe\nv5UbNlYyaiyMn9nhQ2iG9IcULRG/bCapaXNDh1RgSXIshDAqkhyL3EpISuHIhYes23sdABO1CjMT\nubVGGDlFwXLlMmz+N4WUChWJXPsrKeUrGjqqAk2SYyGEUZHcWOTW2evBusS4tLMNIzpWx8Jc5jMW\nRiwhAZtJE7D6xYuElq2J+nEFiq3G0FEVeJIcCyGMiowci9xKStECvJL5jIV4UerHj9AM7IvZqZPE\nTPiE2E8+BbVc6TAGkhwLIYyK5MYit7YdvguAtZV8tAnjZnrmFBrPPqgjI4hYuZbEdh0NHZJ4hnxF\nEUIYFZVKleMfIU5eeUxIZDwA9jYWBo5GiMxZbPwF+w6twdycsB37JDE2QvL1WghhVCTXFblx60Ek\nAF8Nqo+p3IQnjFFyMtZf/Y9CyxaT+HYjIlesQSlSxNBRiQxIciyEMCoyEixyy8rChFLO8ohoYXxU\nYaFohg7E/NBBYocMJ+arGWAmUwwaK0mOhRBGRXJjkVMJSSnsO3Ufc1MZMRbGx+TqFez69UD9MJCo\n+YuJ793P0CGJ55DkWAhhVGTkWOTUqatBABR3tDZwJEKkZ/77DmxHDUOxtiZ8606S671h6JBENsjX\nbCGEUVGpcv4jCi5FUfA5dBuA3i3kMdHCSGi1FJozCzvP3qS4uhK+7y9JjPMRGTkWQhgVGTkW2XUj\nIJx5m84Tn5hC+eIaKpawM3RIQkB0NJoxI7DYuY347r2ImrMALC0NHZXIAUmOhRBGRXJjkV1rd18j\nPjGFkk42DH6/iqHDEQL13TvYDeiFybWrRH89k7hhI+Wklg9JciyEMCoyciyyIzImkQdPYgCYNri+\ngaMRAswO/Ylm6ABQFCI2+JDUuKmhQxK5JDXHQgijIjXHIjtStAoAfaTOWBiaomC1/EfsenRCW9SF\nsD1/SmKcz8nIsRDCqMjIscgJExP5exEGFB+P7cRxWG78hYTWbYlavAzFxtbQUYkXJMmxEMKoSHIs\nhMgP1I8eovHsjdmZ08R8PJnYjyeDWi7Ivw4kORZCGBW1WpJj8XxJKVpDhyAKMNNTJ9EM7Is6KoqI\nVetIbNve0CGJl0i+4gghjIrUHIvniYhJZPLSYwDyVDzxyln8ug77jm3AwpKw3/dLYvwakpFjIYRR\nyeuyisTERBYsWICvry+RkZG4ubkxfvx4GjRokGW/bdu2sXnzZm7dukVERATOzs688cYbjB49mhIl\nSuRpzCK9P04HAOBoZ0k9t6IGjkYUGMnJWH/5GYWWLyHxncZErvgZpXARQ0cl8oAkx0IIo5LXI8GT\nJ09m79699O/fnzJlyrB161aGDh2Kl5cXtWrVyrTf1atXKVq0KO+++y52dnYEBgayadMm/vzzT7Zt\n24aTk1PeBi50dh2/B8DEXrUwk5Fj8QqoQkPQDPXE/O+/iB0+kpgvpoOppFCvK/nNCiGMijoPs2M/\nPz927tzJlClT8PT0BKBjx460bduWOXPmsH79+kz7fvLJJ3rLmjVrRufOndm2bRuDBw/Oq7DFM45c\neEiKVqF+FWec7K0MHY4oAEwuX8Kufy/UjwKJ/GEJCT37GDokkcfkK7cQwqjkZc3x7t27MTMzo1u3\nbrplFhYWdO3aldOnTxMUFJSjWIsXLw5AZGRkjvqJ3IuMSQSg4zvlDRyJKAjMt/vi0KY5JMQT7rtL\nEuMCQkaOhRBGJTc1x5GRkRkmqBqNBo1Go3t95coVypUrh7W1dbp27u7uKIrClStXcHZ2znJf4eHh\npKSkEBgYyOLFiwGeW6/8suW2bjrN9u3bWbNmDTdv3sTc3BxXV1c++eQT3N3d8zjyl8fB1sLQIYjX\nmVZLoe9mYv39tyTVqUvkz+vRuhQzdFTiFZHkWAhhVHIzk9uaNWtYtGiR3vLRo0czZswY3evg4GCK\nFtW/gSutXjg7I8ctJ/AMWQAAIABJREFUW7YkPDwcAHt7e6ZOncqbb76Z86BfQG7rpgHmzZvHTz/9\nRPv27enRowexsbFcvXqV4ODgVxT9i4l4OnIsRF5RRUdhO3IYFrt3Et+zD1Gz54GlpaHDEq+QJMdC\nCKOSm5HjAQMG0KlTJ73lz44aA8THx2NmZqbXzsIidRQyISHhuftatGgRsbGx3Llzh23bthETE5Pj\neF/Ei9RNnzlzhmXLlrFw4UJatGjxiiJ+eZKStez95z6Qt7XpouBS376F3YBemNy8QfQ33xI3ZITM\nF1kASXIshDAqufkc+m/5RGYsLS1JSkrSW56WFKclyVmpV68eAO+++y7NmjWjXbt2FCpUiL59++Yw\n6tzJqm563rx5BAUFZVoasnbtWmrUqEGLFi3QarXExcXplZgYs4chqV9E6ro5yywV4uXbuxeH7j1A\nrSJi028kvfOuoSMSBiJnFyGEUVHl4r/scnJyyrB0Iq2k4Hn1xv9VqlQpqlWrxvbt23PU70Vkp246\nM8eOHaNGjRrMnTuXOnXqULt2bZo2bcq2bdvyOuyXYs9JfwDqVpZp88RLpChYLVkErVujLV6CsD1/\nSmJcwMnIsRDCqOTl06Pd3Nzw8vIiJiYmXXJ5/vx53fqcio+PJy4u7qXF+Dy5rZuOiIggPDycnTt3\nYmJiwscff4y9vT3r169n4sSJWFlZ5arUokgRmxz3SY3XNsd9LCzMsLex4P1GFXO1z1clN8eWX7x2\nxxYXB8OHg5cXdOmC6erVFLHJ3d+0sXvtfnfPeNnHJsmxEMKo5OUT8lq1asWqVavw9vbW1esmJibi\n4+ND7dq1dUlnYGAgcXFxVKhQQdc3NDSUwoULp9vexYsXuXr1Km3atMmzmP8rt3XTsbGxQOpsG5s2\nbcLDwwOAFi1a0KJFCxYvXpyr5DgkJBqtVslRHycnW4KDo3LUR6sonL76GAszkxz3fZVyc2z5xet2\nbOqHgWg8e2N29gwxkz7DesY0gkNiIO71OcY0r9vv7lm5OTa1WpXlF3tJjoUQRiUv733x8PCgVatW\nzJkzh+DgYEqXLs3WrVsJDAxk5syZunaTJk3i5MmTXLt2TbesSZMmtG7dGldXVwoVKsTNmzfZsmUL\n1tbWjBw5Mu+C/o/c1k2nLS9ZsqQuMQYwNzenZcuWrF27Vm9E3ZjsOn6PqNgkVNZyc5R4caYnT6AZ\n1BdVTAwRa34lsfX7WKul0lSkkuRYCGFU8noWgtmzZzN//nx8fX2JiIigcuXKLF++/P/t3XlcVNX/\nBvBnZpwZ1gFUMDfULNBU3DXNzF0qcws3VFCE3FNMRUVtNUv5KlZabrmQZqLgvmC5/Mo0k0w0kdwR\nURyRXYYBZn5/qJPINgwM9wLP29e8ijP3znwOy51nzpx7Ltq2bVvkfp6enjh16hR+/vlnaDQaODo6\nwt3dHZMmTUL9+vXNWvOzTJ03bW9vD4VCgZo1a+a7r2bNmtDr9UhPTxdlOD554S52nrgOAJg+pOKs\nxUziZLFlM2xm+0NXtx6Sd+xBbpOmQpdEIsNwTESiYu5Vk5RKJQICAhAQEFDoNiEhIfnaitq+PJk6\nb1oqlaJp06ZISEjId9+9e/cgk8lgZ2dnnqJLaf3+xycZzhnZBg1fKH5VEqICZWfDZuFcWK5fA+0b\n3ZG6ZgP0DtWL34+qHH6GQESiIpFISnyrStzd3ZGdnY3Q0FBDW2Hzpq9du5Zv37t37+LkyZOGtvT0\ndBw8eBCtW7eGhQgvdPAwVQMAeKWhA1zq2wtcDVVUksRE2A0dCMv1a/Bo4lSk/LiTwZgKxZFjIhKV\nKpZ1S6w086ZHjBiB0NBQTJ06FWPGjIFKpcLOnTuRlpaGGTNmCNGdYh384/HybR2a5l+hg8gYsosX\nYOc9AtL7CUj9ZjWyho4QuiQSOYZjIhIVXvmseKbOm7a0tMTmzZuxZMkS/PDDD9BoNGjWrBk2bNhQ\n7L5CydXpUU0mRdeWdYQuhSogxZ5wqN6fCJ2dPZL3HEJOa3H+npO4MBwTkagwGhfP1HnTwOMT+pYu\nXWqu0szCSikTugSqaHQ6WH35GayXByG7XQekbNgCfQHrgxMVhOGYiESlqs0hJqKyJUlLhe0kPygP\nH0TmSC+kf/E/wIhLwxM9xXBMRKJizivkEVHlJrt2BSpvT8iuXUXa4iBofPx4IgOVGMMxEYkKR46J\nyBTyo0eges8HqCZDyo49yH7tdaFLogqKS7kRkahIJCW/UeX0SJOD4+fuILeEl6emKkavh+U3K2Dn\nOQS6evWRFHGCwZhKhSPHRCQqHDmmpxKfrHHc8AVbgSsh0crMhK3/FFiEhULTfxDSVqwCRHiVR6pY\nGI6JSFQ455ie1611XaFLIBGS3omDytsT1S6cR8a8hXg07QN+lERlguGYiESFI8f01P2kTKFLIJGq\ndvoU7HxGARoNUjdvg7bvm0KXRJUIwzERiYqM4Zie2HfqJgDAxlIuaB0kLhabN8Bm7kzk1ndG6q4D\nyHVxFbokqmQYjolIVJiN6alqMglq17CCq7OD0KWQGGi1sJkfAMuN66Ht3hOpq7+H3p6/G1T2uFoF\nEYmKRCIp8Y0qH3VyJq7dSYWDLS/eQIBErYbdkAGw3LgejyZPQ8rWHQzGZDYcOSYiUWHWJQC4GpcC\nAGhQiytVVHXVLpyHytsT0gdqpH67DlnvDhW6JKrkGI6JSFSkTMf0jK6t6ghdAglIGb4DttMnQ+dQ\nHcl7DyOnZWuhS6IqgNMqiEhUeBEQIkJuLqw/+wiq8T7IadESSREnGIyp3Ihq5Dhu3XChSyAzc2g/\nRegSqBxknvvG5H05h5ioapOkpsB2wjgof45A5uixSF+8FFAohC6LqhBRhWMiIn6cRVR1ya5egcpr\nOGQ3byBtyXJoxowTuiSqghiOiUhUOHJMVDUpfj4M2/HjAKUCKTv3IrvTa0KXRFUUB2mISFSkkpLf\niKgC0+th+dUyqEYORW6Dhkg6fJzBmATFkWMiEhWGXaIq5NEj2E6fBItdYdAMHIy04FWAlZXQVVEV\nx3BMRKLCaRUEABsOXgbA34fKTBp3GypvT1S7GIX0+R8hc6o/l58hUWA4JiJR4cgxPaaHUi6Do52F\n0IWQGchPnYRq3GggS4vULduh7dVX6JKIDDjnmIhEhesc0yNNNnJy9XijVR2OHFdCFhvWwe7dd6Cz\ns0fy4WMMxiQ6HDkmIlHhFfIo4s/bAABrS7nAlVCZ0mphM282LDd/j6xefZD27Tro7eyFroooH4Zj\nIhIVfpxF2hwdAODtVxsIXAmVFcn9+7AbNxryP07h0bQPkDFnPiCTCV0WUYEYjolIVDhwTACgqCaF\nlBPQK4Vq589B5e0JadJDpK7+HlmDPIQuiahIDMdEJCqcVkFUeSh3boet/xToajoieV8Eclq0FLok\nomLxE0wiEhWekFe1XbuTgiu3k4Uug0orNxfWHy+AaqIvslu1QdLh4wzGVGFw5JiIRIWfpFdte07e\nxLX4VDSqrRK6FDKRJDkJqgnjoDj6MzLHjEP6Z18CCoXQZREZjeGYiESF0yqqNr1ejxfrqDDfq53Q\npZAJZP/GQOU1HLLbsUgLWgGN11ihSyIqMYZjIhIVZuOqTa/XQ68XugoyheLwQdhO9AUsLJC8cx9y\nXu0kdElEJuGcYyISFamk5DeqHLYfu4p/biZBp2M6rlD0elgFB0HlNRy5LzZG0pETDMZUoXHkmIhE\nRQKm3aro1r00HPojFgDQ/7WGwhZDxsvIgO20SbDYEw7N4CFIW/Y1YGUldFVEpcKRYyISFY4cF0+r\n1WLp0qXo0qUL3NzcMHToUJw6darEj+Pn5wdXV1csWrTIDFWWzPlrDwAAEwY0Q2sXR4GrIWNIY2/B\noV8fKPfuQvrCT5H27ToGY6oUGI6JSFQYjos3Z84cbNq0Cf3790dgYCCkUin8/Pxw7tw5ox/j+PHj\nOHv2rBmrNE07VyehSyAjyE/+Coc+b0B6OxYpP+5A5pRpPGGAKg2GYyISFYlEUuJbVRIVFYX9+/dj\n5syZmD17NoYNG4ZNmzahdu3aCAoKMuoxtFotFi9ejHHjxpm5WuPo9XocPhMrdBlkDL0eFutXw86j\nP3Q1aiL58FFk9+gtdFVEZYrhmIhEhSPHRTt06BDkcjmGDBliaFMqlfDw8EBkZCTu379f7GNs3rwZ\nGo1GNOE4PTMbmVm5ADj4KGpZWbCZMRW2c2dB27M3kg8dRW7jl4WuiqjMMRwTkajIpJIS36qS6Oho\nNGrUCNbW1nna3dzcoNfrER0dXeT+arUaq1atgr+/PywtLc1ZqtHuJj4CAIzs7VLlPgmoKCQJCUD3\n7rDcshkZ/jORunkb9La8UAtVTlytgohEpYpl3RJTq9WoVatWvnZHx8cnsRU3crxs2TI0atQIAwYM\nKJN6atSwMWk/R0dbw/+vO/A40DeqZ5+nvaKqDH3I488/gUGDgKQkYPt2WA8ZAuvi96qQKt3P7hns\nm/EYjolIVDhwWDSNRgO5XJ6vXalUAgCysrIK3TcqKgq7du1CSEhImY3QJiaml3hdYkdHW6jVaYav\ntVk5qGlngRdr2eRpr4ie71tFp9z+I2w/eB86p1qQ/f471HVeBCpR/55V2X52z2Lf8pJKJUW+see0\nCiISFSkkJb5VJRYWFsjOzs7X/jQUPw3Jz9Pr9Vi0aBH69OmDdu3Ed2lmhVwmdAn0rJwcWH8YCNWU\n8chu1wFJh48DLVsKXRVRueDIMRGJCkeOi+bo6Fjg1Am1Wg0AcHIqeCm0I0eOICoqCv7+/oiLi8tz\nX3p6OuLi4lCzZk1YWFiUfdFUoUiSHkL13lgoThxD5rj3kP7JYqCATyuIKiuGYyISFc45LlqTJk0Q\nEhKCjIyMPCflnT9/3nB/QeLj46HT6eDt7Z3vvrCwMISFhWHt2rXo2rWreQqnCkF2ORp2XsMhvROH\ntGVfQzMq/+8LUWXHcExEoiLl0HGR3N3d8f333yM0NBRjxowB8Hjd4rCwMLRp08Zwsl58fDwyMzPR\nuHFjAECPHj1Qr169fI83efJkdO/eHR4eHmjWrFm59YPER3FwP2wn+QFWVkgOP4CcDh2FLolIEAzH\nRCQqzMZFa9myJdzd3REUFAS1Wg1nZ2eEh4cjPj4eixcvNmwXEBCAM2fOICYmBgDg7OwMZ2fnAh+z\nfv366NWrV7nUTyKk08Fq2RJYL/kc2a1aI3XjVujq1BW6KiLBMBwTkahw5Lh4S5YsQXBwMHbv3o2U\nlBS4urpizZo1aNu2rdCllVhWdi7OxqhRu4aV0KVUTenpUE2dAOX+PdB4DEPa/74CRLL+NZFQGI6J\nSFSYjYunVCoREBCAgICAQrcJCQkx6rGejiwL5Xp8KgDAQsGXo/ImvXkDdt6ekMVEI/3jz5E5YTL/\nAInAcExEIsP1JasY/eM1kod2byxwIVWL/P+OQ+XnDej0SPlxJ7K79xS6JCLR4OsQEYmKRCIp8Y0q\nrp+OXgUA/hzLi14Py7Xfwm7YIOicaiHp8DEGY6LncOSYiESFEalq0WTnAgAa1a68l7YVjaws2Mz2\nh+WPPyDL/W2krVoDvQ2/70TPYzgmIlHhCXlVi0QiQYemTpBX4xXyzEl67y5UY0dCHnkWGR8E4NGs\nuYCUHx4TFYThmIhEhdGYqGxVi/wTqjEjIU1LQ8r6EGjfGSB0SUSixreNRCQqEknJb1Qxnb/6AAkP\nHwldRqWm3LYF9gPeBJRKJO0/wmBMZASOHBORqJj7xCytVosVK1Zg9+7dSE1NRZMmTeDv749OnToV\nuV9ERAQOHDiAqKgoJCYmonbt2ujevTsmTZoEW1vO2zRFQlImAKB7a15woszl5MD64/mwWr0K2tff\nQOqajdDXqCF0VUQVAsMxEYmKuT/OmjNnDiIiIuDl5YUGDRogPDwcfn5+CAkJQevWrQvdb8GCBXBy\ncsKAAQNQp04dxMTEICQkBL/++it27twJpVJp5sorr/pONkKXUKlIHiZC5TcWil+P49F7E5Hx0SKg\nGl/uiYzFvxYiEhVzjhxHRUVh//79mDt3LsaMGQMAGDhwIPr164egoCBs2bKl0H2/+uordOzYMU9b\n8+bNERAQgP3792Pw4MFmq5vIWLLoS7DzGg7p3XikrliFrBGjhC6JqMLhnGMiEhWJCTdjHTp0CHK5\nHEOGDDG0KZVKeHh4IDIyEvfv3y903+eDMQD06tULAHDt2rUSVEFkHop9e+DwZk9Ao0HyrgMMxkQm\n4sgxEYmKKSPHqampSE1NzdeuUqmgUqkMX0dHR6NRo0awtrbOs52bmxv0ej2io6Ph5ORk9PM+ePAA\nAODg4FDimonKjE4Hq6AvYB30BbLbtEXqxq3QvVBb6KqIKiyGYyISFVM+ztq0aRO++eabfO1TpkzB\n1KlTDV+r1WrUqlUr33aOjo4AUOTIcUHWrl0LmUyGPn36lLBiorIhSU+D7eTxUB7cB80wT6QtDQYs\nLIQui6hCYzgmIlExZeTY29sbgwYNytf+7KgxAGg0Gsjl8nzbPT2ZLisry+jn3Lt3L3bs2IHx48fD\n2dm5hBXToT9iEXqMl44uDemN67DzHgHZlX+R/uliZL43iWsbEpUBhmMiEhVTXtqfnz5RGAsLC2Rn\nZ+drfxqKjV1x4uzZswgMDES3bt0wbdq0khVLAID4BxlQKmQY3vNlWCr5UlRS8uNHoXpvDAAgZVsY\nst/oLmxBRJUIT8gjIlEx50VAHB0dC5w6oVarAcCo+caXL1/GxIkT4erqiuXLl0Mm42WPTWVlUQ1d\nW9YRuoyKRa+H5XffwG74YOheqI2kw8cZjInKGMMxEYmKFJIS34zVpEkT3LhxAxkZGXnaz58/b7i/\nKLGxsfD19UX16tWxevVqWFlZlbyDRKbSaGA7dQJsFs6D1v1tJB/4GbpGLwpdFVGlw3BMRKJizpFj\nd3d3ZGdnIzQ01NCm1WoRFhaGNm3aGE7Wi4+Pz7c8m1qtho+PDyQSCdavX4/q1auXSX+JjCG9Gw/7\nAe6w2P4jMmbPQ+r3IdDb8MqMRObAiV5EJCoSk2YdG6dly5Zwd3dHUFAQ1Go1nJ2dER4ejvj4eCxe\nvNiwXUBAAM6cOYOYmBhDm6+vL27fvg1fX19ERkYiMjLScJ+zs3ORV9cjKo1qf/4B1dhRkGRkIGXj\nVmjf6id0SUSVGsMxEYmKzMxn2y9ZsgTBwcHYvXs3UlJS4OrqijVr1qBt27ZF7nf58mUAwLp16/Ld\nN2jQIIZjMguLrSGwme0PXe06SA7djdymrwhdElGlx3BMRKJi7pWolEolAgICEBAQUOg2ISEh+dqe\nHUUmMrvsbFh/OA9W61ZD27U7UtdugN6BU3mIygPDMRGJCpdppapOkpgIlZ83FL/9Hx6Nn4yMDz8F\nqvHlmqi88K+NiETFnHOOSRz0ej3+ufkQer1e6FJER/bPRdh5j4A04R5Sv/4OWcM8hS6JqMphOCYi\nUZEyG1d6N++mIiktixf/eI5i7y6opk6ATmWH5N0HkdOmndAlEVVJXMqNiERFYsI/qliyc3QAAG93\nV4ErEQmdDlZffAq7cV7IadoMyUdOMBgTCYhv2wvwKCMDW7eE4OCB/Yi/EweFQoEGDRvhXY+h6D9w\nECRFTIpMTExE8LIgRF+6iIR7CdBoMlGr1gto2749xvmOh3ODBnm23x0ehoXz5xb4WMNGjMS8+QsN\nX+fk5GDVN19h755d0GRq0Om11zBn3oJ8661evBAF71Ge2LD5B7i1bFWK70TlZGkhR2RoIBrVq4nv\ntp2A/5f/rXk7bXQPvNW1BV5u4ITqdlZ4mPII/95MwKofj2PPsSijn6P+Cw6YPa4vundwRR0nOySl\nPsK56NtYvvlnnPwr7/q51pYKzHvvTQzs2Qp1a9kjKTUTEScv4eOVexGvTsmzbeum9RE0ywNurvUQ\ndy8Jn685iNDDkXje9uXvoZpMisHvf1fC747wOOe46rBQ8OqCkrRU2E7yg/LwQWR6jkb6l8sAIy9j\nTkTmwXD8HJ1Oh0kT/HD+73N4Z8BAjPAcBY0mEwcP7MfC+XNx/fo1+H8wq9D901JTEHvzBjp17oLa\nderAQmmBW7duYnf4Thw5fAghW7ej8Usv5dvP970JaPRi3isdNWzYKM/XP2zeiE0b1sN77DhUr14d\n369biw/nz8PXq/4LQDk5OfjkwwUYMmw4g3EhFk7sh5oONgXe165ZA9yKT8Th3/7Bg+R0VFdZY3Dv\n1vhp2Xv4eNU+fLH2ULGPX9vRDie3zkY1mQzrd/6Gq7Fq1Ha0g8+gzji8Zho8pq/God/+AQBYKOWI\nWDcdrZrUw5Z9Z/BH1A00rFsD44d2RfcOLnh99FIkJKYBAGyslNi5YgLi7ydj7vJwdG33MjYs8sb1\n22pEXoo1PP/gXq3Rrb0L2nosKoPvVvnjSDBVFbLrV6HyGgHZtatIW7wUGp/3+O6QSAQYjp9zIeo8\nzv0ViVGjvTFrzjxD+7DhnhjwzpvYGfpTkeG4YaMXsWnLtnztvfv0xcjhQ7Bt6w8IXPhRvvtf7dQZ\n7Tt0LLK2X34+grfefgfvT58BALCxtcXHC+cjKysLyicjDZs2fI+UlBRMnTbdmO5WOa2a1MMUz24I\nXLEbX34wON/9o+dsyNf29dZj+H3rbMzw7oUl6w9Dpyv6JKKR73SEo4Mthvivxr7jFwzt2w+dxT97\nPoLP4M6GcOz77mto84ozFny9B0HfRxi23X/iAn753h8fTn4Hkz7ZCgB4teWLqO1oh27e/0Ps3YdY\nv/Mk2jdvgHe6tzSEYzsbSwTN9sDHK/fh9r2kkn+DRIBzjiu/nFyd0CUITn70Z6jG+wAyKVK270L2\n628IXRIRPcE5x89JT08HADg6OeVplysUsLd3gIWlpUmPW7tOXQBAampqodtkZKQjW6st9P4sjQYq\nOzvD13Z2dtDpdMjKygIAxN66hTXfrcS8BR/C2rrgkdGqTCqVYOUCT0T8Ho1dv/xt9H65uTrE30+B\ntaUC8mrFfwyssrYAANy9n3dKRMKDVOTm6pCR+d/PuGt7FwBAyO5TebY9ff4GrsaqMaRvWygVj9/D\nWirlAICk1EcAHp/xn5yWCWtLhWG/z/0HIu5eElZtO2F0/8SGc44rtwfJmQj45jcAgExWBV+C9HpY\nrvwKdp4e0NWth6TDxxmMiUSmzI9MW7ZsQc+ePcv6YctNixZusFWpsPH7dYg4fBB34+Nx4/o1rFj+\nP0Rf+gcTJ0016nGys7ORlPQQavV9/BV5FnNmPR7t7dK1a4HbT5syEZ07tEX7Nm4YMqg/9u3dnW8b\nt1atcOjAfpz7KxI3b1zHxu/X48UXG0OlUgEAPv14Ibp26443unU3sfeV2/sje8C1US34f7G92G0d\nVFao6WAD10a1MPc9d/Tp3BQn/ryCLG1Osfv+fCoaALBi3jC83vZl1HG0Q9tXnLFp8VikP8rCipBf\nDNsq5Y+D7yNNdr7HeaTRwsZKieYv1QEAnIuOhTY7Bx9OehvOtR0w8p2OcHOpi9PnrwMAurR9CaPe\n6YhJn26t0EtkSSQlv1HFcenW4080nJ1s4FrfXuBqyllmJmwn+cHm4/nQvvUOkvZFQNegodBVEdFz\nynxaRWpqKuLj48v6YcuNys4OX33zLT5aGIhZM/6bmmBtbY3/BX+NHj17GfU4v5/8De9PnmD4ukaN\nmvhg1hy8039gnu0sLC3w1tv90L7jq6hevQbu3InDTz9uQeCc2Yi7fRsTJk0xbDtx8vu49M8/GDP6\n8bqXjo6OCFr+FQBgV/hOXI6Oxq69B0zue2XWoE4NzJ/4FhavOYjYuw/hXLvoK01F7VpomJecnZ2L\nXb/8jWmLiw/VAPB/Z69g2uc/YcHEtxGxbpqh/cqt+3jDOwgxNxIMbdHX76LPa6+gW3sX7D3+3wl/\nL9RUwbVhLQBAvRccEHkpFnEJyfhgyQ4snfkuJns+fgO0efdp7DxyDgp5NaycPwLBm3/BxSsV9+8P\nAMeBq4j3PdxQrQqNHEvvxEE1ZiTk588hY858PPKfxXd2RCJlVDj+888/jX7AuLg4k4sRC0srK7z0\nkgu6de+Blq3aICUlGT/9uBVzZ3+A4K9XoVPn14p9DLeWLbF63QZoNBpcv3YVhw4eQGpqCnJyclDt\nmSsd9XV/C33d38qz75ChwzFi6LtYu/pbvDNgIOrWrQcAqFGjBn74cTtu3rgBjSYTjV96GUqlEomJ\niVi2dAn8Z85CjZo18XPEYaxd8x0ePkxEu/YdMGfufNjZV7ERmud8HTgcN+ISseKHo0ZtP3zmWlgo\n5KjjZI/BvVvDQqmAjZUSD5LSjdr/QVI6/roUi2N/xOBK7H287OyE6d49Ef7VRPTxDUZcQjIAYE3o\nr/D16IIV84ZBqaiGMxduoH7t6lg8fZDhI2dLi/+mTazb8Rt2HI6ES8NaiL+fbHicuX7ukEol+HzN\nQTiorLB01rvo1t4F6qR0LF0fgbCfz5Xk2yUoKQMDVTLV/jgNu7EjAY0GKZu3QfvcMZ+IxMWocDx6\n9Ogily97ll6vN3pbMbrybwy8Rw7HzIC5GDpshKH9zbf64d2B/fDJhwuw79ARyGRFzz11cKiOVzt1\nBgB0694D/foPwJBB/fHw4UMs/OiTIvdVKBTwHuODBYFzcOrkSXgMHWa4TyqV4sXGjfNsv2TxIrzs\n4oJBgz0QFXUeM2dMQ8Dc+Wjewg2LF32CeXNmYeV3a0v6rag0hr/VHj1fdUXvccHIyTHuRKBnl1sL\n2XMamxaPwdENM9Dm3c+QnJZZ5L5jB3XGirnD8OqIL3Dp2l1D+5FT0Ti1NQCfTO0Pn/mbAQDXbz/A\noKnf4duFngj50sew7a5f/sZf0bEYP7Qr0jI0eR4/OS0TZy7cNHz9SuPa8Pfuif6TVyFLm4Oflvmh\nhp01hn+wDu2aN0DIl2Nx2/sh/rx4y6i+C63iHj3Kj1arxYoVK7B7926kpqaiSZMm8Pf3R6dOnYrc\nLyIiAgcOHECc24AtAAAdx0lEQVRUVBQSExNRu3ZtdO/eHZMmTYKtrW05VV+1WIRshM2cD5Bbrz5S\nw/cj17WJ0CURUTGMCsdWVlZo0qQJfHx8it320KFD2L9/f6kLE0rI5o3IyspCn77uedotLS3xetdu\n2Lb1B8TfuYP6zs4lelwnp1ro2KkzdoXtwJx586FQKIrcvk7dxyfwJScXveLAr/93AseO/ozQ8D2Q\nSCTYtXMHWrZqjREjRwEA3p8+A+N9x0Ktvg9HR6ciH6syUsir4csPBuPQb5dwLzEVL9avCQCo4/R4\nJF1lY4kX69dEYlIGUtILD70/7P0DQ93bYUDPVti061Sh2wHALJ8+iLmZkCcYA8A/V+MRczMBr7d9\nOU/7r5FX0HzAx2jy4guoYW+NW3cSEZeQjB+ehOVnp2E8TyKRYNVCT/x44E/839krqO1oh76vNcNb\nE77G2X9u4ew/tzCkb1t4DehUYcIx03Hx5syZg4iICHh5eaFBgwYIDw+Hn58fQkJC0Lp160L3W7Bg\nAZycnDBgwADUqVMHMTExCAkJwa+//oqdO3caVr2hMpCdDZv5AbDcsA7abj2QumYD9PYOQldFREYw\nKhw3b94cCQkJ6NWr+Pm2V65cKXVRQrqfcB/A4xUKnpebm5PnvyWVpdEgNzcX6enp+S7c8bzYW4+D\nTPUaNQrd5lFGBhZ98hHemzAJDZ6c1JGQcA8vvFDbsM0LL7wAALh3916VDMeWSjmcqtvira7N8VbX\n5vnu9+zXAZ79OmDusnAEP3OiXEGPAwDVVVbFPmcdJztcj3tQ4H3VZNJCz9C/fP2e4f8V8mp4o70L\nrsbex9XY+4U+14RhXdGgTg0MnLIKAFD3SeiPe2YZt7iEJNR7oeK8KHP1iaJFRUVh//79mDt3LsaM\nGQMAGDhwIPr164egoCBs2bKl0H2/+uordOyYd8nI5s2bIyAgAPv378fgwfmXNyxrp/+5V/xGFZzk\nwQOoxo2G4tRJPJo8DRnzPwKK+bSRiMTDqLMh3NzcEBsbi5SUlGK31ev1FfpM+cZPpizs2RWWpz01\nNRXHj/4ClcoO9Z0fX+Xu6UoW2dn/rTSQ+KDgUHTt6lX8cfo06td3zhOMCxoZTktLw4b1ayGXy/Ha\na68XWus3XwXDxtYG3mPHGdocnZxw9ep/b1Cu/PsvAMDJqeoFYwDI0GTBc9a6fLf3P3+8FvXhk//A\nc9Y67DtxAVYWijzLoj0llUowftjjVUaenc5QrZoULg1rof5zwTP6+j24NKiFDi0a5mnv6NYILzdw\nynPBjsJ8MvUd1HSwwZfrDhe6Tb1a9vhocj/MWrrDMNXj7pMr6jV/uY5hu2Yv1TG0VwRcraJohw4d\nglwux5AhQwxtSqUSHh4eiIyMxP37hb+Zej4YAzAMely7di3ffeZwOfbxPHlrS3m5PF+5+/tvOPR5\nA/K/ziJ15RpkfPgpgzFRBWPUyLG3tze6du0Kubz4g9mkSZMwadKkUhcmlJFe3ti7ZzdWLP8frlz5\nF61at0FqSgp27tgOtVqNefMXGuYbz58XgLN/nsGBiF8MJ82tX7sap079jq5d30CdunWh1wNXr/6L\nfXv2ICcnG3OfuRw0AHgMfAdt23XAyy4uqF69BuLvxGFX+E6o1Wp8MGsOaj0Z+X3ehago/LTtR2wI\n2ZLn5/J2v/4I37kDgXNno1nzFvh+3Rq079Cx0Mep7HJydAj/Of+axk9Xq7hx+4HhfjeXuohYNx3h\nv5zDlZv38TA1A3Uc7THUvS1cG72AkD2ncfLcfwGijqM9zocvwP+dvYK+fisM7YtWH8C2ID/s+3YK\n1u14fIW8l5wd4TfkdWizc/D56rwripzcMhv/d/YKrsbeh1JRDe90c0O3Dq5Yt+M3/LD3j0L7Fjx3\nGH776xp2RPxlaLtzPxkn/vwXQbM8UNvRDq2bOqPZS3Uw3Yjl68SiimXdEouOjkajRo1gbW2dp93N\nzQ16vR7R0dElejP84MkbegeH8vl0QSaVYHD3l6CUV77AqNy1E5g2CXCojuS9h5HTqo3QJRGRCYwK\nx46OjnB0dDR3LaJQp05dbNkWitXfrsQff5zC4YMHoFQq4dqkKT6YNQe9evcpcv+u3bojISEBEYcP\n4eHDROTm5sKpVi307usO77E+eOmlvPNN3d/qh7N/nsGp308iIyMdNjY2aN7CDR9/thivdSl41Pjx\nJaLnY+jw4XBza5nnvvYdOuKjTxfh+7VrcPzoL2jXvgPmf/hx6b4pVcSd+8n4cf8ZdG7TGP27t4St\nlQVS0jNxPiYOX6w9hG0Hzxr1OPuOX8DbE7+Bv3dPeA3oBDsbCySlPcLPp6KxeM1BRP17J8/2Z6Ju\n4O03WqCukz1ycnMR9e8deM/dgO2HIgt9jnd7t0bXdi8XeInoMfM24qt5w7Bg4ttITMrA+I9+wG+R\nV0v2zRAS03GR1Go1atWqla/96TG6qJHjgqxduxYymQx9+hR9bCtMjRqmXXDI0bESnQCYmwssWAAs\nXgx07gzZzp1wqKQDEpXq51aAytw/9s14Er2I5kBoTJvKSxWIQ/spxW9EFV7muW9M3vfsjcKvIlmY\ndo1UJj9fRdOrVy+89NJL+O677/K03759G7169cKCBQswatQoox5r7969mDlzJsaPH48ZM2aYVE9i\nYnqxl1R/lt+SYxjc/SW82b6+Sc8nNpLUFNhO9IXyyGFkjvKG5brVUKcWfqXTiszR0RZqdZrQZZhN\nZe4f+5aXVCop8o19mV8EhIioNKraHOKSsrCwyHOew1NPLyNv7IoTZ8+eRWBgILp164Zp06YVvwPl\nI7t6BSqv4ZDdvIG0L/4HzVhfWCqVACpnOCaqKhiOiUhUmI2L5ujoWODUCbVaDcC4k28vX76MiRMn\nwtXVFcuXLy923XbKT/FLBGzHjwPk1ZCyYw+yO3cRuiQiKiNV59qdRFQxSEy4VSFNmjTBjRs3kJGR\nkaf9/PnzhvuLEhsbC19fX1SvXh2rV6+GlVXxyxPSM/R6WH4dDJXnEOjqOyMp4gSDMVElw3BMRKIi\nMeFfVeLu7o7s7GyEhoYa2rRaLcLCwtCmTRvDyXrx8fH5lmdTq9Xw8fGBRCLB+vXri11vnZ7z6BFs\nJ46DzacLkdV/EJL2RUBXv2QXhCIi8eO0CiISFc45LlrLli3h7u6OoKAgqNVqODs7Izw8HPHx8Vi8\neLFhu4CAAJw5cwYxMTGGNl9fX9y+fRu+vr6IjIxEZOR/K6I4OzsXeXW9qk4adxsqb09UuxiF9MAP\nkfn+DP6yElVSDMdEJCqMG8VbsmQJgoODsXv3bqSkpMDV1RVr1qxB27Zti9zv8uXLAIB169blu2/Q\noEEMx4WQn/4dKp9RgCYLqSHboO3zptAlEZEZMRwTkbgwHRdLqVQiICAAAQEBhW4TEhKSr+3ZUWQy\njsXG9bCZNwu5DRoidfc25L7sInRJRGRmDMdEJCpVbQ4xiZRWC5t5s2G5+Xtk9eyNtO/WQ29nL3RV\nRFQOGI6JSFSkzMYkMIlaDTufUZD/cQqPpvojY95CgMvdEVUZDMdEJC4MxySgalF/Q+XtCWniA6R+\ntx5Zg4cIXRIRlTMu5UZEosKl3EgoyrBQ2PfrAwBI3hfBYExURTEcE5GoSCQlvxGVSm4urD/9EKoJ\n45DTsjWSIk4gx62V0FURkUA4rYKIRIVZl8qTJCUZthPGQfnLEWR6j0P6oi8BhULosohIQAzHRCQu\nTMdUTmRX/oVq9DDIYm8hbclyaMaME7okIhIBhmMiEhXOIabyoIg4CNuJfoBSgZSwfch+tbPQJRGR\nSHDOMRGJCucck1np9bAKDoJq9HDkNnoRSREnGIyJKA+OHBORqDDrktlkZMB2+mRY7A6DZrAH0pZ9\nA1hZCV0VEYkMwzERiQvTMZmBNPYW7Lw9Ibt0EekLPkHmlGn82IGICsRwTESiwjnHVNbkv/8G1bjR\nQHYOUrdsh7ZXX6FLIiIR45xjIhIVzjmmMqPXw+L7tbDz6A+dQ3UkHz7KYExExeLIMRGJCrMulYms\nLNjMnQnLHzYhq3dfpH27DnqVndBVEVEFwHBMROLCdEylJElIgJ3PKMj//AMZ02fiUUAgIJMJXRYR\nVRAMx0QkKpxzTKVR7e+/oPL2hDQ5CalrNiBr4LtCl0REFQznHBORqHDOMZlKGboN9v3dAZkMSfuO\nMBgTkUkYjolIVCQm3KiKy82F9UfzoZr8HrJbt0VSxAnktnATuioiqqA4rYKIxIVpl0pAkpwE1Xtj\noTh+FJk+fkj/9AtALhe6LCKqwBiOiUhUOOeYjCWLuQyV13DI4m4j7X9fQTN6jNAlEVElwHBMRKLC\nOcRkDMWhA7Cd6AtYWSE5bD9yOr4qdElEVElwzjERiQrnHFOR9HpYLVsCO6/hyH3pZSRFHGcwJqIy\nxZFjIhIVjhxTodLToXp/IpT7dkPz7lCkLfsasLQUuioiqmQYjolIZJiOKT/prZuw8xoBWUw00j/8\nDJmTpvKdFBGZBcMxEYkK8w49T/7rCaj8vIFcHVK27kB2j15Cl0RElRjnHBORqHDOMRno9bBY9x3s\nhg6ErqYjkg8fZTAmIrPjyDERiQpHjgkAkJUFm4AZsNwagqy+byJt1VrobVVCV0VEVQDDMRGJCtc5\nJmnCPajGjIQ88k9kzJiNR7PnAVJ+0ElE5YPhmIhEhSPHVVu1v85CNWYkpKkpSFm/Gdp3BgpdEhFV\nMXwrTkSiIpGU/EaVg/KnrbAf8CagUCBp3xEGYyISBEeOiUhUOK2iCsrJgfXHC2C1eiW0Xboide0m\n6GvUELoqIqqiOHJMROJi5uUqtFotli5dii5dusDNzQ1Dhw7FqVOnit0vKioKH330EQYPHozmzZvD\n1dW1ZE9chkztAwAkJCRg2rRpaNeuHdq0aYNJkybh9u3bZq64cJKkh7Ab/i6sVq/EI9/xSPkpnMGY\niATFcExEomLupdzmzJmDTZs2oX///ggMDIRUKoWfnx/OnTtX5H4nTpxAaGgoAKB+/folfNayZWof\nMjIy4OXlhcjISEyYMAHvv/8+Ll26BC8vL6SkpJRT9f+RRV+CQ59ukJ8+ibTglcj4fCkgl5d7HURE\nz2I4JiJRMeec46ioKOzfvx8zZ87E7NmzMWzYMGzatAm1a9dGUFBQkfuOGDECkZGRCAsLQ5cuXUrZ\nS9OVpg9bt27FrVu3sGbNGvj6+mLMmDFYv349EhISsHHjxvLpwBOK/Xvh8GZPIDMTyeH7ofEcXa7P\nT0RUGIZjIhIViQn/jHXo0CHI5XIMGTLE0KZUKuHh4YHIyEjcv3+/0H1r1qwJCwuLUvWtLJSmD4cP\nH0arVq3wyiuvGNoaN26MTp064eDBg2at+ymJXge3ratgN3YkclxdkXzkBHLadyyX5yYiMgZPyCMi\nUTFl9YnU1FSkpqbma1epVFCp/rtwRHR0NBo1agRra+s827m5uUGv1yM6OhpOTk4lL6AcmdoHnU6H\nmJgYDBs2LN99LVq0wMmTJ5GZmQlLS0uz1Q4Akw+vRKuLP0MzdATSglYAInjDQUT0LIZjIqrwNm3a\nhG+++SZf+5QpUzB16lTD12q1GrVq1cq3naOjIwAUOeoqFqb2ITk5GVqt1rDd8/vq9Xqo1Wo4OzuX\nqJ4aNWxKtH2tutVxZ/Bi1P0oABaVdB0+R0dboUswm8rcN6By9499Mx7DMRGJiil5ydvbG4MGDcrX\n/uyoMQBoNBrICzjhS6lUAgCysrJK/uTlzNQ+PG1XKBSF7qvRaEpcT2JiOnQ6vdHbvxCyDo6OtlCr\n00r8XBUB+1ZxVeb+sW95SaWSIt/YMxwTkaiYss7x89MnCmNhYYHs7Ox87U+D49OQKGam9uFpu1ar\nLXRfMcypJiISGsMxEYmKOT9pd3R0LHDagVqtBgDRzzcGTO+Dvb09FAqFYbvn95VIJAVOuSAiqmq4\nWgURiYo51zlu0qQJbty4gYyMjDzt58+fN9wvdqb2QSqVwsXFBRcvXsx3X1RUFBo0aGD2k/GIiCoC\nhmMiEhczpmN3d3dkZ2cbLuYBPJ5mEBYWhjZt2hhOdIuPj8e1a9fKojdlrjR96Nu3L/7++29cunTJ\n0Hb9+nWcPn0a7u7u5dMBIiKR47QKIhIVU+YcG6tly5Zwd3dHUFCQYWWG8PBwxMfHY/HixYbtAgIC\ncObMGcTExBja7ty5g927dwMALly4AABYtWoVgMejtT169DBb3WXVB09PT4SGhuK9997D2LFjIZPJ\nsHHjRjg6OmLMmDHlUj8RkdgxHBORqJh7da8lS5YgODgYu3fvRkpKClxdXbFmzRq0bdu2yP3i4uKw\nYsWKPG1Pvx40aFC5hWPA9D7Y2NggJCQEn3/+OVatWgWdToeOHTsiMDAQDg4O5VQ9EZG4SfR6vfFr\n8JiZJkfoCsjcHNpPEboEKgeZ5/KvOWysR9qSH5KsFJVzvdyKoKRLuQFcVqqiqsx9Ayp3/9i3vLiU\nGxFVLMy5REQkIIZjIhIVc845prInlZr28zJ1v4qAfau4KnP/2DfjtxfVtAoiIiIiIiFxKTciIiIi\noicYjomIiIiInmA4JiIiIiJ6guGYiIiIiOgJhmMiIiIioicYjomIiIiInmA4JiIiIiJ6guGYiIiI\niOgJhmMiIiIioicYjomIiIiInmA4JiIiIiJ6guG4nGm1WixduhRdunSBm5sbhg4dilOnTgldFpWx\n+/fvIygoCKNHj0br1q3h6uqKP/74Q+iyiMymNMe2hIQETJs2De3atUObNm0wadIk3L5928wVG8/U\nvkVERGD69Ono0aMHWrZsCXd3d3z55ZdIS0srh6qNU1avSX5+fnB1dcWiRYvMUKXpStu/vXv3wsPD\nA61atUKHDh0watQoREVFmbFi45Wmb7///jtGjx6Njh07on379hg2bBgOHDhg5oqNV9rX0GvXrmHc\nuHFo3bo1OnTogICAADx8+NDo/RmOy9mcOXOwadMm9O/fH4GBgZBKpfDz88O5c+eELo3K0I0bN7B2\n7VokJCTA1dVV6HKIzM7UY1tGRga8vLwQGRmJCRMm4P3338elS5fg5eWFlJSUcqq+aKb2bcGCBbh2\n7RoGDBiA+fPno0uXLggJCcGIESOQlZVVTtUXrSxek44fP46zZ8+asUrTlaZ/y5cvx5w5c/Dyyy8j\nMDAQkydPRv369aFWq8uh8uKZ2rdjx47Bx8cHOTk5mDp1KqZNmwapVAp/f3+EhoaWU/VFK81r6L17\n9zBy5Ejcvn0b/v7+8PHxwbFjxzBu3DhkZ2cb9yB6Kjfnz5/Xu7i46Dds2GBo02g0+l69euk9PT2F\nK4zKXFpamv7hw4d6vV6vP3LkiN7FxUV/+vRpgasiMo/SHNvWrFmjd3V11f/zzz+GtqtXr+qbNm2q\nDw4ONlfJRitN3wr6mw8PD9e7uLjod+7cWdalllhZvCZlZWXp+/Tpo//666/1Li4u+s8++8xM1ZZc\nafoXGRmpd3V11UdERJi5StOUpm/jxo3Td+nSRZ+VlWVoy8rK0nfp0kU/cuRIc5VcIqV5Df3www/1\nrVq10t+7d8/QdvLkSb2Li4s+NDTUqMfgyHE5OnToEORyOYYMGWJoUyqV8PDwQGRkJO7fvy9gdVSW\nbGxs4ODgIHQZROWiNMe2w4cPo1WrVnjllVcMbY0bN0anTp1w8OBBs9ZtjNL0rWPHjvnaevXqBeDx\nx75CK4vXpM2bN0Oj0WDcuHHmLNUkpenf5s2b0aJFC/Tu3Rs6nQ4ZGRnlUbLRStO39PR02NnZQaFQ\nGNoUCgXs7OygVCrNWrexSvMaGhERgR49eqBWrVqGts6dO6Nhw4ZGH1MYjstRdHQ0GjVqBGtr6zzt\nbm5u0Ov1iI6OFqgyIiLTmXps0+l0iImJQfPmzfPd16JFC9y8eROZmZlmqdlYZX3cfvDgAQCI4s1z\nafumVquxatUq+Pv7w9LS0pylmqQ0/Tt16hRatGiBZcuWoW3btmjTpg169OiBPXv2mLtso5Smbx06\ndMCVK1cQHByM2NhYxMbGIjg4GDdv3oSPj4+5SzerhIQEJCYmFnhMcXNzM/rvtVpZF0aFU6vVed7J\nPOXo6AgAHDkmogrJ1GNbcnIytFqtYbvn99Xr9VCr1XB2di7bgkugrI/ba9euhUwmQ58+fcqkvtIo\nbd+WLVuGRo0aYcCAAWapr7RM7V9KSgqSk5Oxf/9+yGQyzJw5E/b29tiyZQtmzZoFS0tL9O7d26y1\nF6c0P7sJEyYgNjYW3333Hb799lsAgJWVFVatWoXXXnvNPAWXk6f9LuyYkpiYiNzcXMhksiIfh+G4\nHGk0Gsjl8nztTz/GEMsJGkREJWHqse1p+7Mf7z6/r0ajKasyTVKWx+29e/dix44dGD9+vKCB/6nS\n9C0qKgq7du1CSEgIJBKJ2WosDVP79+jRIwCP37xt374dLVu2BAD07t0bvXv3xsqVKwUPx6X52SkU\nCjRs2BDu7u7o3bs3cnNzsX37dkyfPh0bN26Em5ub2eo2N2OPKc+PuD+P4bgcWVhYFHim5NMfpljm\n+hARlYSpx7an7VqtttB9LSwsyqpMk5TVcfvs2bMIDAxEt27dMG3atDKt0VSm9k2v12PRokXo06cP\n2rVrZ9YaS6O0v5f16tUzBGPgceDq27cvNm/ejIyMjGIDljmV5vfy008/xYULF7Bjxw5IpY9n1775\n5pvo168fPv/8c2zbts08RZeDsjqmcM5xOXJ0dCzwo46ny8I4OTmVd0lERKVm6rHN3t4eCoWiwKWx\n1Go1JBJJgR+PlqeyOG5fvnwZEydOhKurK5YvX17sR7rlxdS+HTlyBFFRURgxYgTi4uIMN+DxyV5x\ncXGCj/gDpf+9rFmzZr77atasCb1ej/T09LIttoRM7ZtWq8WOHTvQrVs3QzAGALlcjtdffx0XLlxA\nTk6OeYouB0/7XdgxpUaNGkb9/TEcl6MmTZrgxo0b+c56PX/+vOF+IqKKxtRjm1QqhYuLCy5evJjv\nvqioKDRo0EDwE71Ke9yOjY2Fr68vqlevjtWrV8PKyspstZaUqX2Lj4+HTqeDt7c3evbsabgBQFhY\nGHr27IkzZ86Yt3gjlOb3smnTpkhISMh337179yCTyWBnZ1f2BZeAqX1LTk5GTk4OcnNz892Xk5OD\nnJwc6PX6si+4nNSqVQvVq1cv9JjStGlTox6H4bgcubu7Izs7O88i21qtFmFhYWjTpk2Bk+uJiMTO\n2GNbfHx8viXM+vbti7///huXLl0ytF2/fh2nT5+Gu7t7+XSgCKXpm1qtho+PDyQSCdavX4/q1auX\na+3FMbVvPXr0wMqVK/PdAKB79+5YuXIlmjVrVr6dKUBpfnbu7u64e/cuTp48aWhLT0/HwYMH0bp1\na8Gn+5jatxo1akClUuHIkSN5pmVkZGTg2LFjcHFxKXAus1g9XW3jWX369MHRo0fzvLk5deoUbt68\nafQxRaKvyG8RKqBp06bhl19+gbe3N5ydnREeHo6LFy9i06ZNaNu2rdDlURlatWoVgMfrme7btw/v\nvvsu6tWrB5VKhVGjRglcHVHZMubYNnr0aJw5cwYxMTGG/dLT0zFo0CBkZmZi7NixkMlk2LhxI/R6\nPXbt2iWKJc9M7duAAQNw+fJl+Pr6wsXFJc9jOjs7o3Xr1uXaj4KY2reCuLq6wsvLC4GBgeVRulFM\n7V9mZiYGDx6MhIQEjBkzBiqVCjt37sSNGzdE83ptat++/fZbBAcHo1mzZujfvz90Oh127NiBa9eu\nYfny5XjrrbeE6lIexryG9ujRAwBw9OhRw353797FwIEDYW9vj1GjRuHRo0dYv349ateujdDQ0AJP\n1nsew3E5y8rKQnBwMPbu3YuUlBS4urpixowZ6Ny5s9ClURkr7JKXdevWzfOHTFQZGHNsKyxk3bt3\nD59//jlOnjwJnU6Hjh07IjAwEPXr1y/vbhTI1L4VddnbQYMG4YsvvjBr3cYozc/teWIMx6Xpn1qt\nxpIlS3DixAloNBo0a9YMM2bMQPv27cu7GwUqTd/27t2LzZs34+bNm9BqtXB1dYWfn5/gq3A8y5jX\n0ILCMQBcuXIFX3zxBSIjIyGXy9GtWzfMnTvX6E9vGI6JiIiIiJ7gnGMiIiIioicYjomIiIiInmA4\nJiIiIiJ6guGYiIiIiOgJhmMiIiIioicYjomIiIiInmA4JiIiIiJ6guGYiIiIiOgJhmMiIiIioif+\nH1OtLHSlq4PFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60t946KwdjJh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation and qualitative analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syd2rBrsSLro",
        "colab_type": "text"
      },
      "source": [
        "Preparing development sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9-lOx5RR-6B",
        "colab_type": "text"
      },
      "source": [
        "We will use our development set we let outside of the analysis to make an attempt of qualitative analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1GKZZKlSOZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We prepare again the development sample for analysis \n",
        "def dev_prepare_to_feed(df,max_length_value,batch_size_value):\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "  from transformers import CamembertTokenizer\n",
        "\n",
        "  texts = df.Texte.values\n",
        "  labels = df.sexe.values\n",
        "  tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=False,padding_side='right')\n",
        "  \n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  num_truncated_tokens =[]\n",
        "\n",
        "  for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # text\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_length_value,           # We choose for now a max length of 500.\n",
        "                          pad_to_max_length = True,    # Pad text to max (marche pas en pad left ?)\n",
        "                          return_attention_mask = True,   # Construct attention masks\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                          return_overflowing_tokens =True, # return overflowing token information\n",
        "                    )\n",
        "      \n",
        "      # Map tokens to their id in the dictionnary \n",
        "      # We add this to our list    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "      #num_truncated_tokens.append(encoded_dict['num_truncated_tokens'])\n",
        "      \n",
        "      # 3. Attention masks\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # We convert all this into tensors in order to be able to make it work on GPU \n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Original text and transformed tensor print \n",
        "  print(\"Let's check for the first text indexes, attention masks and labels\")\n",
        "  print(\" \")\n",
        "  print('Original: ', texts[0][0:100])\n",
        "  print('IDs:', input_ids[0][0:100])\n",
        "  print('Attention masks:', attention_masks[0][0:100])\n",
        "  print('labels',labels[0])\n",
        "\n",
        "  # Combine all above\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "  # We create data loaders for the train and validation dataset. \n",
        "  dev_dataloader = DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              batch_size = batch_size_value, # Trains with this batch size.\n",
        "              shuffle=False\n",
        "          )\n",
        "  return dev_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd8v_Gb55BPT",
        "colab_type": "code",
        "outputId": "eeda6772-9f46-4fa5-ad37-6781b5aa115f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "dev_dataloader=dev_prepare_to_feed(dev_balanced_split,max_length_value=500,batch_size_value=10)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:   J'ai plaisir à le souligner aujourd'hui devant vous tous, responsables des petites villes de France\n",
            "IDs: tensor([   5,  121,   11,   73,  593,   15,   16, 8415,  405,   11,  265,  466,\n",
            "          39,  117,    7, 2783,   20,  923, 1785,    8,  184,    9,    6,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n",
            "labels tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX0AAe6J9uyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "f6e89859-313e-4881-d926-d66edf21269a"
      },
      "source": [
        "dev_balanced_split"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>sexe</th>\n",
              "      <th>Texte</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9817</th>\n",
              "      <td>7405</td>\n",
              "      <td>131006</td>\n",
              "      <td>0</td>\n",
              "      <td>J'ai plaisir à le souligner aujourd'hui devan...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9818</th>\n",
              "      <td>3196</td>\n",
              "      <td>186921</td>\n",
              "      <td>1</td>\n",
              "      <td>J'ai demandé à la mission « musique » de cond...</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9819</th>\n",
              "      <td>2956</td>\n",
              "      <td>150006</td>\n",
              "      <td>1</td>\n",
              "      <td>Le deuxième objectif que Michel Duffour et mo...</td>\n",
              "      <td>481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9820</th>\n",
              "      <td>4754</td>\n",
              "      <td>204874</td>\n",
              "      <td>1</td>\n",
              "      <td>Vous le savez, le Président de la République ...</td>\n",
              "      <td>492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9821</th>\n",
              "      <td>9831</td>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>Dans l'attente des conclusions du groupe de t...</td>\n",
              "      <td>486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10116</th>\n",
              "      <td>3655</td>\n",
              "      <td>199244</td>\n",
              "      <td>1</td>\n",
              "      <td>Le débat que nous aurons demain sur les liens...</td>\n",
              "      <td>489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10117</th>\n",
              "      <td>1661</td>\n",
              "      <td>142593</td>\n",
              "      <td>1</td>\n",
              "      <td>Nous sommes conscients que la condamnation de...</td>\n",
              "      <td>444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10118</th>\n",
              "      <td>2139</td>\n",
              "      <td>175426</td>\n",
              "      <td>1</td>\n",
              "      <td>. . Ce n'est pas facile : la parité, la diver...</td>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10119</th>\n",
              "      <td>7491</td>\n",
              "      <td>163460</td>\n",
              "      <td>0</td>\n",
              "      <td>Laïque, Démocratique et Sociale. Oui, tous un...</td>\n",
              "      <td>228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10120</th>\n",
              "      <td>9056</td>\n",
              "      <td>136731</td>\n",
              "      <td>0</td>\n",
              "      <td>J'ai d'ores et déjà demandé au PUCA d'inscrir...</td>\n",
              "      <td>491</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>304 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  ...  Length\n",
              "9817    7405  ...      16\n",
              "9818    3196  ...     421\n",
              "9819    2956  ...     481\n",
              "9820    4754  ...     492\n",
              "9821    9831  ...     486\n",
              "...      ...  ...     ...\n",
              "10116   3655  ...     489\n",
              "10117   1661  ...     444\n",
              "10118   2139  ...     495\n",
              "10119   7491  ...     228\n",
              "10120   9056  ...     491\n",
              "\n",
              "[304 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXVKl1flXbkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_pred,total_labels,total_logits=evaluation_loop(gender_model,dev_dataloader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvgZXct7YSpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the score for label 1 \n",
        "one_score = [el[1] for el in total_logits]\n",
        "max_score = np.max(total_logits,axis=1)\n",
        "# Put everything inside a dataframe\n",
        "results_dev=pd.DataFrame([total_labels,total_pred,one_score,max_score]).transpose()\n",
        "results_dev.columns=['returned_labels','model_pred','one_score','max_score']\n",
        "results_dev['WF']=pd.DataFrame([results_dev['model_pred']==results_dev['returned_labels']]).transpose()\n",
        "# Merge back with the text\n",
        "frames = [dev_balanced_split[['Texte','sexe','index_df']].reset_index(), results_dev]\n",
        "result = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBhJ624rzOWG",
        "colab_type": "code",
        "outputId": "031baa79-bac4-46aa-f844-08011e724dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9817</td>\n",
              "      <td>J'ai plaisir à le souligner aujourd'hui devan...</td>\n",
              "      <td>0</td>\n",
              "      <td>131006</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.445413</td>\n",
              "      <td>0.445413</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9818</td>\n",
              "      <td>J'ai demandé à la mission « musique » de cond...</td>\n",
              "      <td>1</td>\n",
              "      <td>186921</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.487588</td>\n",
              "      <td>2.487588</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9819</td>\n",
              "      <td>Le deuxième objectif que Michel Duffour et mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>150006</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.389463</td>\n",
              "      <td>1.389463</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9820</td>\n",
              "      <td>Vous le savez, le Président de la République ...</td>\n",
              "      <td>1</td>\n",
              "      <td>204874</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.933072</td>\n",
              "      <td>1.933072</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9821</td>\n",
              "      <td>Dans l'attente des conclusions du groupe de t...</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.283976</td>\n",
              "      <td>2.283976</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>10116</td>\n",
              "      <td>Le débat que nous aurons demain sur les liens...</td>\n",
              "      <td>1</td>\n",
              "      <td>199244</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.160049</td>\n",
              "      <td>2.160049</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>10117</td>\n",
              "      <td>Nous sommes conscients que la condamnation de...</td>\n",
              "      <td>1</td>\n",
              "      <td>142593</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.154672</td>\n",
              "      <td>2.154672</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>10118</td>\n",
              "      <td>. . Ce n'est pas facile : la parité, la diver...</td>\n",
              "      <td>1</td>\n",
              "      <td>175426</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.490168</td>\n",
              "      <td>2.490168</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>10119</td>\n",
              "      <td>Laïque, Démocratique et Sociale. Oui, tous un...</td>\n",
              "      <td>0</td>\n",
              "      <td>163460</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.327210</td>\n",
              "      <td>2.504230</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>10120</td>\n",
              "      <td>J'ai d'ores et déjà demandé au PUCA d'inscrir...</td>\n",
              "      <td>0</td>\n",
              "      <td>136731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.920517</td>\n",
              "      <td>0.920517</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>304 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index                                              Texte  ...  max_score     WF\n",
              "0     9817   J'ai plaisir à le souligner aujourd'hui devan...  ...   0.445413  False\n",
              "1     9818   J'ai demandé à la mission « musique » de cond...  ...   2.487588   True\n",
              "2     9819   Le deuxième objectif que Michel Duffour et mo...  ...   1.389463   True\n",
              "3     9820   Vous le savez, le Président de la République ...  ...   1.933072   True\n",
              "4     9821   Dans l'attente des conclusions du groupe de t...  ...   2.283976  False\n",
              "..     ...                                                ...  ...        ...    ...\n",
              "299  10116   Le débat que nous aurons demain sur les liens...  ...   2.160049   True\n",
              "300  10117   Nous sommes conscients que la condamnation de...  ...   2.154672   True\n",
              "301  10118   . . Ce n'est pas facile : la parité, la diver...  ...   2.490168   True\n",
              "302  10119   Laïque, Démocratique et Sociale. Oui, tous un...  ...   2.504230   True\n",
              "303  10120   J'ai d'ores et déjà demandé au PUCA d'inscrir...  ...   0.920517  False\n",
              "\n",
              "[304 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLF9aS31dH7M",
        "colab_type": "code",
        "outputId": "f02cb924-e70b-4f60-ca7e-a26d19c1027e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Which texts failed ? \n",
        "print('{0:.2f} percent of the development texts were not well classified by our model'.format(result[result.WF==False].WF.count()*100/len(result)))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.76 percent of the development texts were not well classified by our model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHTuqPbu1nti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We merge this dataframe to the information we had at the beginning\n",
        "merged_results=result[['index','index_df','returned_labels','model_pred','one_score','max_score','WF']].merge(df,how='left',left_on='index_df',right_on='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtVSBuEp02VL",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to take 3 texts well classified and 3 other wrongly classified. We will try to take the ones the model as really sure about in the good or bad side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy9_53i23Y2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_texts_true=merged_results[merged_results.WF==1].nlargest(3,'max_score')\n",
        "top_texts_false=merged_results[merged_results.WF==0].nlargest(3,'max_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR8fnne6KHQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "10d136f0-de9f-4e39-aabc-9a819c16fe7a"
      },
      "source": [
        "top_texts_true"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>Titre</th>\n",
              "      <th>Type</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Prenom</th>\n",
              "      <th>Nom</th>\n",
              "      <th>Fonction</th>\n",
              "      <th>Date</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Texte</th>\n",
              "      <th>Lien</th>\n",
              "      <th>PRENOM</th>\n",
              "      <th>preusuel</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>9945</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.415498</td>\n",
              "      <td>2.580664</td>\n",
              "      <td>True</td>\n",
              "      <td>14889</td>\n",
              "      <td>176000</td>\n",
              "      <td>Déclaration de M. Bernard Kouchner, ministre d...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Bernard</td>\n",
              "      <td>Kouchner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2009-07-16T12:00:00Z</td>\n",
              "      <td>Culture - Médias,Politique culturelle</td>\n",
              "      <td>Madame la Ministre,Monsieur le Directeur génér...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/176000-de...</td>\n",
              "      <td>BERNARD</td>\n",
              "      <td>BERNARD</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>10076</td>\n",
              "      <td>162162</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.403018</td>\n",
              "      <td>2.578912</td>\n",
              "      <td>True</td>\n",
              "      <td>12107</td>\n",
              "      <td>162162</td>\n",
              "      <td>Déclaration de M. Nicolas Sarkozy, ministre de...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Nicolas</td>\n",
              "      <td>Sarkozy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2006-06-09T12:00:00Z</td>\n",
              "      <td>Sécurité,Délinquance,Ordre public</td>\n",
              "      <td>Mesdames, Messieurs,Je vous ai réunis aujourd'...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/162162-de...</td>\n",
              "      <td>NICOLAS</td>\n",
              "      <td>NICOLAS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>9969</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.398858</td>\n",
              "      <td>2.573263</td>\n",
              "      <td>True</td>\n",
              "      <td>28704</td>\n",
              "      <td>179250</td>\n",
              "      <td>Déclaration de M. Brice Hortefeux, ministre de...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Brice</td>\n",
              "      <td>Hortefeux</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2010-06-12T12:00:00Z</td>\n",
              "      <td>Sécurité,Police</td>\n",
              "      <td>Monsieur le préfet,Monsieur le député,Monsieur...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/179250-de...</td>\n",
              "      <td>BRICE</td>\n",
              "      <td>BRICE</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  index_df  returned_labels  ...   PRENOM  preusuel  sexe\n",
              "128   9945    176000              0.0  ...  BERNARD   BERNARD     0\n",
              "259  10076    162162              0.0  ...  NICOLAS   NICOLAS     0\n",
              "152   9969    179250              0.0  ...    BRICE     BRICE     0\n",
              "\n",
              "[3 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4MGbB9j3koA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "3c87353b-efd4-46e2-c009-5e833fcbddbe"
      },
      "source": [
        "top_texts_false"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>Titre</th>\n",
              "      <th>Type</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Prenom</th>\n",
              "      <th>Nom</th>\n",
              "      <th>Fonction</th>\n",
              "      <th>Date</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Texte</th>\n",
              "      <th>Lien</th>\n",
              "      <th>PRENOM</th>\n",
              "      <th>preusuel</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>9868</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.252967</td>\n",
              "      <td>2.399928</td>\n",
              "      <td>False</td>\n",
              "      <td>29371</td>\n",
              "      <td>173030</td>\n",
              "      <td>Déclaration de Mme Rama Yade, secrétaire d'Eta...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>International</td>\n",
              "      <td>Rama</td>\n",
              "      <td>Yade</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2008-11-12T12:00:00Z</td>\n",
              "      <td>Relations internationales,Relations bilatérale...</td>\n",
              "      <td>Monsieur le Député, après douze jours d'inquié...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/173030-de...</td>\n",
              "      <td>RAMA</td>\n",
              "      <td>RAMA</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9821</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.283976</td>\n",
              "      <td>2.283976</td>\n",
              "      <td>False</td>\n",
              "      <td>15395</td>\n",
              "      <td>207653</td>\n",
              "      <td>Déclaration de M. Bernard Kouchner, ministre d...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Bernard</td>\n",
              "      <td>Kouchner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2001-12-06T12:00:00Z</td>\n",
              "      <td>Santé - Protection sociale,Etablissement sanit...</td>\n",
              "      <td>Mesdames, Messieurs,Au printemps dernier, en p...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/207653-de...</td>\n",
              "      <td>BERNARD</td>\n",
              "      <td>BERNARD</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>9999</td>\n",
              "      <td>206934</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.090272</td>\n",
              "      <td>2.090272</td>\n",
              "      <td>False</td>\n",
              "      <td>256</td>\n",
              "      <td>206934</td>\n",
              "      <td>Déclaration de M. Edouard Philippe, Premier mi...</td>\n",
              "      <td>déclaration</td>\n",
              "      <td>Société</td>\n",
              "      <td>Edouard</td>\n",
              "      <td>Philippe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2018-10-15T12:00:00Z</td>\n",
              "      <td>Santé - Protection sociale,Politique sociale</td>\n",
              "      <td>Madame la Présidente de l'UNCCAS, chère Joëlle...</td>\n",
              "      <td>https://www.vie-publique.fr/discours/206934-de...</td>\n",
              "      <td>EDOUARD</td>\n",
              "      <td>EDOUARD</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  index_df  returned_labels  ...   PRENOM  preusuel  sexe\n",
              "51    9868    173030              1.0  ...     RAMA      RAMA     1\n",
              "4     9821    207653              0.0  ...  BERNARD   BERNARD     0\n",
              "182   9999    206934              0.0  ...  EDOUARD   EDOUARD     0\n",
              "\n",
              "[3 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oi-Hk4Q4J36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjJlGCVmhlsW",
        "colab_type": "text"
      },
      "source": [
        "We want to dive a bit into the model and see how it makes a choice and why it fails on thos 38 sentences. Let's take one of them. We will redo point 4 of TD4 to see the score reached by each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQWuNom8MMcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_document_to_limit(MAX_TOKENS,df):\n",
        "  lst= []\n",
        "  for index,row in df.iterrows():\n",
        "    identifiant = row.Id\n",
        "    label = row.sexe\n",
        "    phrase = []\n",
        "    for token in row.Texte.split(' '):\n",
        "      if len(phrase) < MAX_TOKENS:\n",
        "        phrase.append(token)\n",
        "      else:\n",
        "        lst += [(identifiant,label,' '.join(phrase),len(phrase))]\n",
        "        phrase = []\n",
        "    if len(phrase)>1:\n",
        "      lst += [(identifiant,label,' '.join(phrase),len(phrase))]\n",
        "  return pd.DataFrame(lst,columns=['index_df','sexe','Texte','Length'])\n",
        "def split_document_to_limit_phrases(MAX_TOKENS,df):\n",
        "  lst= []\n",
        "  for index,row in df.iterrows():\n",
        "    identifiant = row.Id\n",
        "    label = row.sexe\n",
        "    phrase = ''\n",
        "    for phrases in sent_detector_mano(row.Texte):\n",
        "      if len(phrase.split(' ')) + len(phrases.split(' ')) < MAX_TOKENS:\n",
        "        phrase+= \" \" + phrases\n",
        "      else:\n",
        "        lst += [(identifiant,label,phrase,len(phrase.split(' ')))]\n",
        "        phrase = ''\n",
        "    lst += [(identifiant,label,phrase,len(phrase.split(' ')))]\n",
        "    phrase = ''\n",
        "  return pd.DataFrame(lst,columns=['index_df','sexe','Texte','Length'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XKOwjLLh7u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_to_analyse = pd.concat([top_texts_false,top_texts_true]).reset_index(drop=True)\n",
        "sentence_to_analyse=split_document_to_limit_phrases(50,sentence_to_analyse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3MVBt9Ym275",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7a21428-84a0-4acb-a2af-deaab65f5030"
      },
      "source": [
        "max(sentence_to_analyse.Length)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhu2c2MSkOmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "764c037a-5ecb-4798-a733-ad5ae386def8"
      },
      "source": [
        "dev_dataloader=dev_prepare_to_feed(sentence_to_analyse,max_length_value=55,batch_size_value=1)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:   Monsieur le Député, après douze jours d'inquiétude, nos sept compatriotes et un ressortissant tunis\n",
            "IDs: tensor([    5,  2445,    16, 27891,     7,   182,  5972,   274,    18,    11,\n",
            "        16035,     7,   166,  2085, 16681,    10,    14,    23,  4008,  2914,\n",
            "        17773,     7,  3665,     8,    13,   426, 21530,     7,    56, 10540,\n",
            "           10,    44,   823,    25,  9834,   182,   190,   101, 10508,    10,\n",
            "            9,   121,    11,  1009,    15,    17,    11,  4220,     8, 28658,\n",
            "           24,    19,  3468,    42,     6])\n",
            "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1])\n",
            "labels tensor(1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzsoooGrlpdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_pred,total_labels,total_logits =evaluation_loop(gender_model,dev_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX2fdfkm89C_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the score for label 1 \n",
        "one_score = [el[1] for el in total_logits]\n",
        "max_score = np.max(total_logits,axis=1)\n",
        "# Put everything inside a dataframe\n",
        "results_dev=pd.DataFrame([total_labels,total_pred,one_score,max_score]).transpose()\n",
        "results_dev.columns=['returned_labels','model_pred','one_score','max_score']\n",
        "results_dev['WF']=pd.DataFrame([results_dev['model_pred']==results_dev['returned_labels']]).transpose()\n",
        "# Merge back with the text\n",
        "frames = [sentence_to_analyse[['Texte','sexe','index_df']].reset_index(), results_dev]\n",
        "result = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTK50qJZngQW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "outputId": "d8315d61-9e2d-4a2e-8829-b2565fea1dfa"
      },
      "source": [
        "result"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Monsieur le Député, après douze jours d'inqui...</td>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.061862</td>\n",
              "      <td>1.105137</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>C'est aussi, vous l'avez souligné Monsieur le...</td>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.534817</td>\n",
              "      <td>0.521251</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>L'objectif était alors de les réconforter et ...</td>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.292707</td>\n",
              "      <td>1.359186</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Je salue également les autorités nigérianes q...</td>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.711271</td>\n",
              "      <td>1.846850</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Que ce soit dans le golfe de Guinée, dans les...</td>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.268483</td>\n",
              "      <td>1.347814</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>266</td>\n",
              "      <td>(3) Je n'oublie pas non plus que, durant ces ...</td>\n",
              "      <td>0</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.940289</td>\n",
              "      <td>0.940289</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>267</td>\n",
              "      <td>J'étais, hier, à Chamonix pour rendre hommage...</td>\n",
              "      <td>0</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.797531</td>\n",
              "      <td>1.923270</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>268</td>\n",
              "      <td>Mesdames et Messieurs,Cette journée est une n...</td>\n",
              "      <td>0</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.578052</td>\n",
              "      <td>0.578052</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>269</td>\n",
              "      <td>En incarnant la quintessence même des valeurs...</td>\n",
              "      <td>0</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.544272</td>\n",
              "      <td>1.646882</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>270</td>\n",
              "      <td>Le RAID fête son quart de siècle et peut envi...</td>\n",
              "      <td>0</td>\n",
              "      <td>179250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.693378</td>\n",
              "      <td>1.818718</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>271 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index                                              Texte  ...  max_score     WF\n",
              "0        0   Monsieur le Député, après douze jours d'inqui...  ...   1.105137  False\n",
              "1        1   C'est aussi, vous l'avez souligné Monsieur le...  ...   0.521251  False\n",
              "2        2   L'objectif était alors de les réconforter et ...  ...   1.359186  False\n",
              "3        3   Je salue également les autorités nigérianes q...  ...   1.846850  False\n",
              "4        4   Que ce soit dans le golfe de Guinée, dans les...  ...   1.347814  False\n",
              "..     ...                                                ...  ...        ...    ...\n",
              "266    266   (3) Je n'oublie pas non plus que, durant ces ...  ...   0.940289  False\n",
              "267    267   J'étais, hier, à Chamonix pour rendre hommage...  ...   1.923270   True\n",
              "268    268   Mesdames et Messieurs,Cette journée est une n...  ...   0.578052  False\n",
              "269    269   En incarnant la quintessence même des valeurs...  ...   1.646882   True\n",
              "270    270   Le RAID fête son quart de siècle et peut envi...  ...   1.818718   True\n",
              "\n",
              "[271 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4-NN0erngfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_results=result[['index','index_df','Texte','returned_labels','model_pred','one_score','max_score','WF']].merge(df[['Id','sexe']],how='left',left_on='index_df',right_on='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKNslucdok9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "261fc530-39ae-4127-c4d9-9de1339295da"
      },
      "source": [
        "#Which texts failed ? \n",
        "print('{0:.2f} percent of the development texts were not well classified by our model'.format(result[result.WF==False].WF.count()*100/len(result)))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41.33 percent of the development texts were not well classified by our model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5MI09GF4yuR",
        "colab_type": "code",
        "outputId": "13ea02f6-dd5f-49a2-ca7e-f2875134cd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "merged_results"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>Texte</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "      <th>Id</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>173030</td>\n",
              "      <td>Monsieur le Député, après douze jours d'inqui...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.061862</td>\n",
              "      <td>1.105137</td>\n",
              "      <td>False</td>\n",
              "      <td>173030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>173030</td>\n",
              "      <td>C'est aussi, vous l'avez souligné Monsieur le...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.534817</td>\n",
              "      <td>0.521251</td>\n",
              "      <td>False</td>\n",
              "      <td>173030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>173030</td>\n",
              "      <td>L'objectif était alors de les réconforter et ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.292707</td>\n",
              "      <td>1.359186</td>\n",
              "      <td>False</td>\n",
              "      <td>173030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>173030</td>\n",
              "      <td>Je salue également les autorités nigérianes q...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.711271</td>\n",
              "      <td>1.846850</td>\n",
              "      <td>False</td>\n",
              "      <td>173030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>173030</td>\n",
              "      <td>Que ce soit dans le golfe de Guinée, dans les...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.268483</td>\n",
              "      <td>1.347814</td>\n",
              "      <td>False</td>\n",
              "      <td>173030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>266</td>\n",
              "      <td>179250</td>\n",
              "      <td>(3) Je n'oublie pas non plus que, durant ces ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.940289</td>\n",
              "      <td>0.940289</td>\n",
              "      <td>False</td>\n",
              "      <td>179250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>267</td>\n",
              "      <td>179250</td>\n",
              "      <td>J'étais, hier, à Chamonix pour rendre hommage...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.797531</td>\n",
              "      <td>1.923270</td>\n",
              "      <td>True</td>\n",
              "      <td>179250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>268</td>\n",
              "      <td>179250</td>\n",
              "      <td>Mesdames et Messieurs,Cette journée est une n...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.578052</td>\n",
              "      <td>0.578052</td>\n",
              "      <td>False</td>\n",
              "      <td>179250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>269</td>\n",
              "      <td>179250</td>\n",
              "      <td>En incarnant la quintessence même des valeurs...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.544272</td>\n",
              "      <td>1.646882</td>\n",
              "      <td>True</td>\n",
              "      <td>179250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>270</td>\n",
              "      <td>179250</td>\n",
              "      <td>Le RAID fête son quart de siècle et peut envi...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.693378</td>\n",
              "      <td>1.818718</td>\n",
              "      <td>True</td>\n",
              "      <td>179250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>271 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     index  index_df  ...      Id  sexe\n",
              "0        0    173030  ...  173030     1\n",
              "1        1    173030  ...  173030     1\n",
              "2        2    173030  ...  173030     1\n",
              "3        3    173030  ...  173030     1\n",
              "4        4    173030  ...  173030     1\n",
              "..     ...       ...  ...     ...   ...\n",
              "266    266    179250  ...  179250     0\n",
              "267    267    179250  ...  179250     0\n",
              "268    268    179250  ...  179250     0\n",
              "269    269    179250  ...  179250     0\n",
              "270    270    179250  ...  179250     0\n",
              "\n",
              "[271 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4z2lKZyoYf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_sentence_true=merged_results[merged_results.WF==1].nlargest(1,'max_score')\n",
        "top_sentence_false=merged_results[merged_results.WF==0].nlargest(1,'max_score')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpMkyg35qUnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "850c51ae-8790-40b7-ede8-81fa5f06ade0"
      },
      "source": [
        "top_sentence_false"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>Texte</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "      <th>Id</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>207653</td>\n",
              "      <td>Je souhaite par ailleurs que vous soyez dorén...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.46695</td>\n",
              "      <td>2.46695</td>\n",
              "      <td>False</td>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  index_df  ...      Id  sexe\n",
              "32     32    207653  ...  207653     0\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDqBRQHAqTog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "a74537d4-9d3f-4796-a5bf-42714f340d2d"
      },
      "source": [
        "top_sentence_true"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>index_df</th>\n",
              "      <th>Texte</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "      <th>Id</th>\n",
              "      <th>sexe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>176000</td>\n",
              "      <td>Ce sujet, qui est celui de la reconstruction ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.40031</td>\n",
              "      <td>2.571011</td>\n",
              "      <td>True</td>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  index_df  ...      Id  sexe\n",
              "93     93    176000  ...  176000     0\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOaXe4-K1GjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WASb-HY_wewg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_to_analyse = pd.concat([top_sentence_false,top_sentence_true]).reset_index(drop=True)\n",
        "words_to_analyse=split_document_to_limit(1,words_to_analyse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBYBn5Ekay5F",
        "colab_type": "code",
        "outputId": "42276014-7f07-4597-b386-355c38b390ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "words_to_analyse"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index_df</th>\n",
              "      <th>sexe</th>\n",
              "      <th>Texte</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>souhaite</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>ailleurs</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>vous</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>dorénavant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>sous</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>des</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>sages-femmes,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>chef</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>service</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>du</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>et</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>la</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>profession</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>assurée</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>la</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>des</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>médicales</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>207653</td>\n",
              "      <td>0</td>\n",
              "      <td>l'établissement</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>sujet,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>est</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>reconstruction</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>de</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>gouvernance</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>ces</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>est</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>sujet</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>pour</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>crédibilité</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>notre</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>Mesdames</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>Messieurs,</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>le</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>je</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>suis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>dans</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>rénovation</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>176000</td>\n",
              "      <td>0</td>\n",
              "      <td>notre</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index_df  sexe            Texte  Length\n",
              "0     207653     0                        1\n",
              "1     207653     0         souhaite       1\n",
              "2     207653     0         ailleurs       1\n",
              "3     207653     0             vous       1\n",
              "4     207653     0       dorénavant       1\n",
              "5     207653     0             sous       1\n",
              "6     207653     0              des       1\n",
              "7     207653     0    sages-femmes,       1\n",
              "8     207653     0             chef       1\n",
              "9     207653     0          service       1\n",
              "10    207653     0               du       1\n",
              "11    207653     0               de       1\n",
              "12    207653     0               et       1\n",
              "13    207653     0               la       1\n",
              "14    207653     0               de       1\n",
              "15    207653     0       profession       1\n",
              "16    207653     0          assurée       1\n",
              "17    207653     0               la       1\n",
              "18    207653     0              des       1\n",
              "19    207653     0        médicales       1\n",
              "20    207653     0  l'établissement       1\n",
              "21    176000     0                        1\n",
              "22    176000     0           sujet,       1\n",
              "23    176000     0              est       1\n",
              "24    176000     0               de       1\n",
              "25    176000     0   reconstruction       1\n",
              "26    176000     0               de       1\n",
              "27    176000     0      gouvernance       1\n",
              "28    176000     0              ces       1\n",
              "29    176000     0              est       1\n",
              "30    176000     0            sujet       1\n",
              "31    176000     0             pour       1\n",
              "32    176000     0      crédibilité       1\n",
              "33    176000     0            notre       1\n",
              "34    176000     0         Mesdames       1\n",
              "35    176000     0       Messieurs,       1\n",
              "36    176000     0               le       1\n",
              "37    176000     0               je       1\n",
              "38    176000     0             suis       1\n",
              "39    176000     0             dans       1\n",
              "40    176000     0       rénovation       1\n",
              "41    176000     0            notre       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Psom5Gizsjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9a162255-985e-4023-de33-397874835622"
      },
      "source": [
        "dev_dataloader=dev_prepare_to_feed(words_to_analyse,max_length_value=3,batch_size_value=1)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's check for the first text indexes, attention masks and labels\n",
            " \n",
            "Original:  \n",
            "IDs: tensor([5, 6, 1])\n",
            "Attention masks: tensor([1, 1, 0])\n",
            "labels tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWOre462zer-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_pred,total_labels,total_logits =evaluation_loop(gender_model,dev_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVUVyNnnz5kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the score for label 1 \n",
        "one_score = [el[1] for el in total_logits]\n",
        "max_score = np.max(total_logits,axis=1)\n",
        "# Put everything inside a dataframe\n",
        "results_dev=pd.DataFrame([total_labels,total_pred,one_score,max_score]).transpose()\n",
        "results_dev.columns=['returned_labels','model_pred','one_score','max_score']\n",
        "results_dev['WF']=pd.DataFrame([results_dev['model_pred']==results_dev['returned_labels']]).transpose()\n",
        "# Merge back with the text\n",
        "frames = [words_to_analyse[['Texte','sexe','index_df']].reset_index(), results_dev]\n",
        "result = pd.concat(frames,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY0huGw0z9pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "816f17bd-f46b-47b3-d9b9-9a5430f50c1d"
      },
      "source": [
        "result"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Texte</th>\n",
              "      <th>sexe</th>\n",
              "      <th>index_df</th>\n",
              "      <th>returned_labels</th>\n",
              "      <th>model_pred</th>\n",
              "      <th>one_score</th>\n",
              "      <th>max_score</th>\n",
              "      <th>WF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022262</td>\n",
              "      <td>-0.022262</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>souhaite</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022137</td>\n",
              "      <td>-0.022137</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ailleurs</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021943</td>\n",
              "      <td>-0.021943</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>vous</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021103</td>\n",
              "      <td>-0.021103</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>dorénavant</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022120</td>\n",
              "      <td>-0.022120</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>sous</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022071</td>\n",
              "      <td>-0.022071</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>des</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022110</td>\n",
              "      <td>-0.022110</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>sages-femmes,</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022272</td>\n",
              "      <td>-0.022272</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>chef</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021611</td>\n",
              "      <td>-0.021611</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>service</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>du</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022017</td>\n",
              "      <td>-0.022017</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>de</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>et</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022257</td>\n",
              "      <td>-0.022257</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>la</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021553</td>\n",
              "      <td>-0.021553</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>de</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>profession</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022091</td>\n",
              "      <td>-0.022091</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>assurée</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022211</td>\n",
              "      <td>-0.022211</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>la</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021553</td>\n",
              "      <td>-0.021553</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>des</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022110</td>\n",
              "      <td>-0.022110</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>médicales</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021816</td>\n",
              "      <td>-0.021816</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>l'établissement</td>\n",
              "      <td>0</td>\n",
              "      <td>207653</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022146</td>\n",
              "      <td>-0.022146</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022262</td>\n",
              "      <td>-0.022262</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>sujet,</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>est</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>de</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>reconstruction</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022210</td>\n",
              "      <td>-0.022210</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>de</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>gouvernance</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021977</td>\n",
              "      <td>-0.021977</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>ces</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022007</td>\n",
              "      <td>-0.022007</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>est</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>sujet</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>pour</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021750</td>\n",
              "      <td>-0.021750</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>crédibilité</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022082</td>\n",
              "      <td>-0.022082</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>notre</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>Mesdames</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021963</td>\n",
              "      <td>-0.021963</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>Messieurs,</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022156</td>\n",
              "      <td>-0.022156</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>le</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021482</td>\n",
              "      <td>-0.021482</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>je</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022094</td>\n",
              "      <td>-0.022094</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>suis</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.021706</td>\n",
              "      <td>-0.021706</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>dans</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022218</td>\n",
              "      <td>-0.022218</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>40</td>\n",
              "      <td>rénovation</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022238</td>\n",
              "      <td>-0.022238</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>41</td>\n",
              "      <td>notre</td>\n",
              "      <td>0</td>\n",
              "      <td>176000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index            Texte  sexe  ...  one_score  max_score     WF\n",
              "0       0                      0  ...  -0.022262  -0.022262  False\n",
              "1       1         souhaite     0  ...  -0.022137  -0.022137  False\n",
              "2       2         ailleurs     0  ...  -0.021943  -0.021943  False\n",
              "3       3             vous     0  ...  -0.021103  -0.021103  False\n",
              "4       4       dorénavant     0  ...  -0.022120  -0.022120  False\n",
              "5       5             sous     0  ...  -0.022071  -0.022071  False\n",
              "6       6              des     0  ...  -0.022110  -0.022110  False\n",
              "7       7    sages-femmes,     0  ...  -0.022272  -0.022272  False\n",
              "8       8             chef     0  ...  -0.021611  -0.021611  False\n",
              "9       9          service     0  ...  -0.022001  -0.022001  False\n",
              "10     10               du     0  ...  -0.022017  -0.022017  False\n",
              "11     11               de     0  ...  -0.020978  -0.020978  False\n",
              "12     12               et     0  ...  -0.022257  -0.022257  False\n",
              "13     13               la     0  ...  -0.021553  -0.021553  False\n",
              "14     14               de     0  ...  -0.020978  -0.020978  False\n",
              "15     15       profession     0  ...  -0.022091  -0.022091  False\n",
              "16     16          assurée     0  ...  -0.022211  -0.022211  False\n",
              "17     17               la     0  ...  -0.021553  -0.021553  False\n",
              "18     18              des     0  ...  -0.022110  -0.022110  False\n",
              "19     19        médicales     0  ...  -0.021816  -0.021816  False\n",
              "20     20  l'établissement     0  ...  -0.022146  -0.022146  False\n",
              "21     21                      0  ...  -0.022262  -0.022262  False\n",
              "22     22           sujet,     0  ...  -0.021496  -0.021496  False\n",
              "23     23              est     0  ...  -0.022119  -0.022119  False\n",
              "24     24               de     0  ...  -0.020978  -0.020978  False\n",
              "25     25   reconstruction     0  ...  -0.022210  -0.022210  False\n",
              "26     26               de     0  ...  -0.020978  -0.020978  False\n",
              "27     27      gouvernance     0  ...  -0.021977  -0.021977  False\n",
              "28     28              ces     0  ...  -0.022007  -0.022007  False\n",
              "29     29              est     0  ...  -0.022119  -0.022119  False\n",
              "30     30            sujet     0  ...  -0.021496  -0.021496  False\n",
              "31     31             pour     0  ...  -0.021750  -0.021750  False\n",
              "32     32      crédibilité     0  ...  -0.022082  -0.022082  False\n",
              "33     33            notre     0  ...  -0.022191  -0.022191  False\n",
              "34     34         Mesdames     0  ...  -0.021963  -0.021963  False\n",
              "35     35       Messieurs,     0  ...  -0.022156  -0.022156  False\n",
              "36     36               le     0  ...  -0.021482  -0.021482  False\n",
              "37     37               je     0  ...  -0.022094  -0.022094  False\n",
              "38     38             suis     0  ...  -0.021706  -0.021706  False\n",
              "39     39             dans     0  ...  -0.022218  -0.022218  False\n",
              "40     40       rénovation     0  ...  -0.022238  -0.022238  False\n",
              "41     41            notre     0  ...  -0.022191  -0.022191  False\n",
              "\n",
              "[42 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFVzNZ63jWkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = [i for i in result[result.index_df==176000].one_score]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVYqMaNr0nyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot=pd.DataFrame(scores).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dbId7RmuG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot.columns=list(result[result.index_df==176000].Texte)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG46EmH31oFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "5986dddb-18c4-465a-ee75-ad80733fbda3"
      },
      "source": [
        "df_plot"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>sujet,</th>\n",
              "      <th>est</th>\n",
              "      <th>de</th>\n",
              "      <th>reconstruction</th>\n",
              "      <th>de</th>\n",
              "      <th>gouvernance</th>\n",
              "      <th>ces</th>\n",
              "      <th>est</th>\n",
              "      <th>sujet</th>\n",
              "      <th>pour</th>\n",
              "      <th>crédibilité</th>\n",
              "      <th>notre</th>\n",
              "      <th>Mesdames</th>\n",
              "      <th>Messieurs,</th>\n",
              "      <th>le</th>\n",
              "      <th>je</th>\n",
              "      <th>suis</th>\n",
              "      <th>dans</th>\n",
              "      <th>rénovation</th>\n",
              "      <th>notre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.022262</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.02221</td>\n",
              "      <td>-0.020978</td>\n",
              "      <td>-0.021977</td>\n",
              "      <td>-0.022007</td>\n",
              "      <td>-0.022119</td>\n",
              "      <td>-0.021496</td>\n",
              "      <td>-0.02175</td>\n",
              "      <td>-0.022082</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>-0.021963</td>\n",
              "      <td>-0.022156</td>\n",
              "      <td>-0.021482</td>\n",
              "      <td>-0.022094</td>\n",
              "      <td>-0.021706</td>\n",
              "      <td>-0.022218</td>\n",
              "      <td>-0.022238</td>\n",
              "      <td>-0.022191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               sujet,       est  ...      dans  rénovation     notre\n",
              "0 -0.022262 -0.021496 -0.022119  ... -0.022218   -0.022238 -0.022191\n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilmXHJJcloMK",
        "colab_type": "code",
        "outputId": "5b827dd2-4d11-40f5-91b7-d5d46a994643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " \n",
        "# Default heatmap: just a visualization of this square matrix\n",
        "fig, ax = plt.subplots(figsize=(20,4)) \n",
        "sns.heatmap(df_plot)\n",
        "plt.show()\n"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEQAAAFrCAYAAADYR1xnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIZz71hoZfAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VXzb-ayoB-o",
        "colab_type": "text"
      },
      "source": [
        "Faire des annotations pour la fin du doc et nettoyer mettre en forme\n",
        "Faire des fonctions pour la fin \n",
        "Amélioer le graphique . Est ce que c'est intelligent par mot ? \n",
        "Remplacer score max par la moyenne des logits à chercher à la fin du train \n",
        "Analyser un peu et mettre au propre sur la feuille\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlwRs8Jgn8DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}