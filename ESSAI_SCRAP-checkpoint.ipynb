{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import time, re\n",
    "import requests\n",
    "from itertools import repeat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essai sur les premières pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []\n",
    "unique_id = []\n",
    "speech_type = []\n",
    "for i in range(20):\n",
    "    link = 'https://www.vie-publique.fr/discours' +'?page='+ str(i)\n",
    "    page_resp=requests.get(link, timeout=5)\n",
    "    content= bs(page_resp.content,'html.parser')\n",
    "    for link in content.find_all(\"a\", class_='link-multiple'):\n",
    "        if link.find('span').text.startswith('Déclaration'):\n",
    "            link_list.append('https://www.vie-publique.fr'+ link.get('href'))\n",
    "            unique_id.append(link.get('href').split('/')[2].split('-')[0])\n",
    "            speech_type.append('Déclaration')\n",
    "            time.sleep(1)\n",
    "    link = 'https://www.vie-publique.fr/discours' +'?page='+ str(i)\n",
    "    page_resp=requests.get(link, timeout=5)\n",
    "    content= bs(page_resp.content,'html.parser')\n",
    "    for link in content.find_all(\"a\", class_='link-multiple'):\n",
    "        if link.find('span').text.startswith('Interview'):\n",
    "            link_list.append('https://www.vie-publique.fr'+ link.get('href'))\n",
    "            unique_id.append(link.get('href').split('/')[2].split('-')[0])\n",
    "            speech_type.append('Interview')\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS=pd.DataFrame({'unique_id':unique_id,'speech_type':speech_type,'link':link_list})\n",
    "LINKS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles infos extra prendre ? \n",
    "- Intervenant (PB quand plusieurs si non journaliste ? ) \n",
    "- Fonction (OK mais possibles erreurs à checker ! ) \n",
    "- Titre \n",
    "- date # mettre as date \n",
    "- sujet principal (about) # pas encore fait\n",
    "- Thématique \n",
    "- publications dans d'autres médias ? # pas encore fait\n",
    "- Interview : mettre radio ou journal considéré\n",
    "- discours ( à checker)\n",
    "\n",
    "Ex : pb avec https://www.vie-publique.fr/discours/272865-ministere-de-leurope-et-des-affaires-etrangeres-20012020-ormuz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "prenom=[]\n",
    "nom=[]\n",
    "titre=[]\n",
    "date=[]\n",
    "theme=[]\n",
    "mots_cle=[]\n",
    "discours=[]\n",
    "merge_id=[]\n",
    "fonct=[]\n",
    "\n",
    "d = webdriver.Chrome()\n",
    "\n",
    "for j in range(len(LINKS.link)):\n",
    "    try: \n",
    "        d.get(link_list[j])\n",
    "        num=unique_id[j]\n",
    "        prenom.append(d.find_element_by_class_name(\"line-intervenant\").text.split(' - ')[0].split(' ')[0].upper())\n",
    "        nom.append(d.find_element_by_class_name(\"line-intervenant\").text.split(' - ')[0].split(' ')[1].upper())\n",
    "        titre.append(d.find_element_by_xpath(\"//h1[contains(@class, 'h')]\").text)\n",
    "        date.append(d.find_element_by_tag_name('time').get_attribute((\"datetime\"))) # set as date \n",
    "        mots_cle.append(d.find_element_by_class_name('tagsBox').text.replace('MOTS-CLÉS :\\n','').replace('\\n',' '))\n",
    "        theme.append(d.find_element_by_class_name('thematicBox').text.replace('Thématique(s) :\\n',''))\n",
    "    except KeyError:\n",
    "        theme.append(None)\n",
    "        mots_cle.append(None)\n",
    "    except NoSuchElementException:\n",
    "        theme.append(None)\n",
    "        mots_cle.append(None)\n",
    "        \n",
    "    try:\n",
    "        fonct.append(d.find_element_by_class_name(\"line-intervenant\").text.split(' - ')[1])\n",
    "        \n",
    "    except KeyError:\n",
    "        fonct.append(None)        \n",
    "    try:\n",
    "        para=d.find_element_by_xpath(\"//span[contains(@class, 'texte-integral')]\").find_element_by_xpath(\"//span[contains(@class, 'texte-integral')]\").text.split('\\n')\n",
    "        para = [x for x in para if x != '']\n",
    "        if LINKS.speech_type[j]=='Déclaration':\n",
    "            for a in para:\n",
    "                if not a.startswith('Source'):  \n",
    "                    discours.append(a)\n",
    "                    merge_id.append(num)\n",
    "        if LINKS.speech_type[j]=='Interview':\n",
    "            for i in range(len(para)-1):\n",
    "                if para[i].startswith(d.find_element_by_class_name(\"line-intervenant\").text.split(' - ')[0].upper()):\n",
    "                    discours.append(para[i+1])\n",
    "                    merge_id.append(num)\n",
    "    except KeyError:\n",
    "        discours.append(None)\n",
    "        merge_id.append(num)\n",
    "    time.sleep(1)\n",
    "    #print(j)\n",
    "d.close()\n",
    "print(j)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titre),len(fonct),len(date),len(merge_id),len(theme),len(prenom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA =pd.DataFrame({'titre':titre,\n",
    "                    'prenom':prenom,\n",
    "                    'nom' : nom,\n",
    "                    'date':date,\n",
    "                   'theme':theme,\n",
    "                   'date':date,\n",
    "                   'fonct':fonct,\n",
    "                   'unique_id':unique_id})\n",
    "DISCOURS = pd.DataFrame({'merge_id':merge_id,\n",
    "                         'discours':discours})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mots_cle=[mot.replace(' - ','-') for mot in mots_cle if mot is not None]\n",
    "mots_cle=[mot.split(' ') for mot in mots_cle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTS_CLE=pd.DataFrame(data=mots_cle)\n",
    "DATA=pd.concat([DATA,MOTS_CLE],axis=1)\n",
    "# Renommer en dynamique. Soucis avec les 'DE'... a regarder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=pd.merge(DATA,DISCOURS,left_on='unique_id',right_on='merge_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge label prénoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prenoms=pd.read_csv(r'C:\\Users\\morga\\Documents\\ENSAE - MS\\Semestre 2\\NLP\\nat2018.csv',sep=';')\n",
    "prenoms=prenoms.groupby(['preusuel','sexe'],as_index=False).sum()\n",
    "prenoms=prenoms.sort_values(['nombre'],ascending=True)\n",
    "prenoms=prenoms.drop_duplicates(subset=['preusuel'],keep='last') # On prend le sexe qui apparait le plus \n",
    "prenoms=prenoms[['preusuel','sexe']]\n",
    "DATA=DATA.merge(prenoms, left_on='prenom',right_on='preusuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistiques\n",
    "DATA.discours.apply(len).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA[DATA.discours.apply(len)>8000] # Ici problème avec pas d'espace ... comment faire ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistiques\n",
    "DATA.sexe.hist(bins=80) # 1 = Homme, 2 = Femme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import ToktokTokenizer, word_tokenize\n",
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print(tokenizer.tokenize(DATA.discours[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words with separator = \" \"\n",
    "arr = DATA.discours.apply(lambda x: x.split(' ')).array\n",
    "\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "arr = reduce(add, arr) # Concatenates all lists contained in arr\n",
    "# Equivalent to :\n",
    "# arr2 = []\n",
    "# for a in arr:\n",
    "#   arr2 = arr2 + a\n",
    "# arr = arr2\n",
    "len(set(arr)) \n",
    "# the set object transforms a list to the set of unique elements in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import ToktokTokenizer\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "arr = DATA.discours.apply(lambda x: tokenizer.tokenize(x)).array\n",
    "arr = reduce(add, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arr = DATA.discours.apply(lambda x: tokenizer.tokenize(x)).array\n",
    "arr = reduce(add, arr)\n",
    "c = Counter(arr)\n",
    "print(c)\n",
    "d = pd.DataFrame(c, index=['occurrences']).transpose().reset_index()\n",
    "d.columns=['word', 'occurences']\n",
    "nb_total = d.occurences.sum()\n",
    "d['freq'] = d.occurences.apply(lambda x: x/nb_total)\n",
    "d = d.sort_values('freq', ascending=False)\n",
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.xscale('log')  # Using log scale\n",
    "plt.yscale('log')  # Using log scale\n",
    "plt.xlabel('log(rank)')\n",
    "plt.ylabel('log(frequency)')\n",
    "plt.title(\"Tokenizer : {0}\".format(word_tokenize))\n",
    "x = list(range(d.shape[0]))\n",
    "plt.plot(x, d.freq)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize as wt\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.fr import French\n",
    "nltk.download('stopwords')\n",
    "french_stopwords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "nlp = French()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('french'))\n",
    "to_be_removed = list(en_stop) + list(punctuation)\n",
    "\n",
    "tok = tokenizer\n",
    "# Tokenizing + removing stopwords\n",
    "text_data = list(DATA.discours.apply(lambda x: list(filter(lambda a: a.lower() not in to_be_removed,tok.tokenize(x)))).array)\n",
    "def remove_url(tokens):\n",
    "  tokens = filter(lambda x: \"http\" not in x, tokens)\n",
    "  return list(tokens)\n",
    "remove_url(text_data)\n",
    "print(text_data[0])\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "ldamodel = LdaModel(corpus, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
